"""
ë¶€ë™ì‚° ë°ì´í„° ë¶„ì„ ë° í†µê³„ ë¦¬í¬íŒ… ë„êµ¬
"""

import logging
import pandas as pd
import numpy as np
import xml.etree.ElementTree as ET
from pathlib import Path
import json
from typing import Any, Optional, Dict, Annotated
from pydantic import Field
from datetime import datetime
import os
import re
import unicodedata
import glob
import time
from mcp_kr_realestate.apis.ecos_api import get_key_statistic_list

from mcp_kr_realestate.server import mcp
from mcp_kr_realestate.utils.ctx_helper import with_context
from mcp.types import TextContent
from mcp_kr_realestate.apis.reb_api import get_reb_stat_list, get_reb_stat_items, get_reb_stat_data, get_reb_stat_list_all, get_reb_stat_items_all, get_reb_stat_data_all, cache_stat_list_full, cache_stat_list
from mcp_kr_realestate.apis.ecos_api import (
    get_statistic_table_list,
    get_statistic_word,
    get_statistic_item_list,
    get_statistic_search,
    get_key_statistic_list,
)
from mcp_kr_realestate.utils.data_processor import get_cache_dir

logger = logging.getLogger("mcp-kr-realestate")

# --- Helper Functions ---
def default_serializer(o):
    """JSON ì§ë ¬í™”ë¥¼ ìœ„í•œ ê¸°ë³¸ serializer. numpy íƒ€ì…ì„ python íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    if isinstance(o, (np.integer, np.floating)):
        return o.item()
    if isinstance(o, np.ndarray):
        return o.tolist()
    if isinstance(o, pd.Timestamp):
        return o.isoformat()
    raise TypeError(f"Object of type {o.__class__.__name__} is not JSON serializable")

def to_numeric(series: pd.Series) -> pd.Series:
    """Helper function to convert series to numeric, handling commas."""
    return pd.to_numeric(series.astype(str).str.replace(',', ''), errors='coerce')

def as_value_unit(value, unit_str, precision=0):
    if pd.isna(value):
        return None
    if unit_str == "ë§Œì›":
        v = float(value) * 10000
        unit_str = "ì›"
    elif unit_str == "ë§Œì›/í‰":
        v = float(value) * 10000
        unit_str = "ì›/í‰"
    else:
        if isinstance(value, (np.integer, int)):
            v = int(value)
        else:
            v = float(value)
    # ì†Œìˆ˜ì  ì´í•˜ ë²„ë¦¼ (ì •ìˆ˜ ë³€í™˜)
    v = int(v)
    return {"value": v, "unit": unit_str}

def clean_deal_for_display(series):
    deal_dict = series.where(pd.notna(series), None).to_dict()
    # Format date
    try:
        deal_dict['dealDate'] = f"{deal_dict.get('dealYear')}-{str(deal_dict.get('dealMonth','')).zfill(2)}-{str(deal_dict.get('dealDay','')).zfill(2)}"
    except (TypeError, ValueError):
        deal_dict['dealDate'] = None
    # Remove intermediate columns
    cols_to_remove = ['ê±°ë˜ê¸ˆì•¡_num', 'ì „ìš©ë©´ì _num', 'ê±´ì¶•ë…„ë„_num', 'í‰ë‹¹ê°€_ë§Œì›', 'ê±´ë¬¼ì—°ë ¹', 'ê±´ë¬¼ì—°ë ¹ëŒ€', 'ê±´ë¬¼ê·œëª¨', 'dealYear', 'dealMonth', 'dealDay']
    for col in cols_to_remove:
        deal_dict.pop(col, None)
    # ìˆ«ì í•„ë“œ value/unit êµ¬ì¡°ë¡œ ë³€í™˜
    for k in list(deal_dict.keys()):
        if k in ['dealAmount', 'dealAmountNum', 'ë³´ì¦ê¸ˆ', 'deposit', 'ë³´ì¦ê¸ˆì•¡', 'ì›”ì„¸', 'monthlyRent', 'ì›”ì„¸ì•¡', 'rentFee', 'rentFeeNum']:
            try:
                # Remove commas if it's a string, then convert to float
                if isinstance(deal_dict[k], str):
                    v = float(deal_dict[k].replace(',', '')) if deal_dict[k] is not None else None
                else:
                    v = float(deal_dict[k]) if deal_dict[k] is not None else None
            except Exception:
                continue
            deal_dict[k] = as_value_unit(v, "ë§Œì›")
        elif k in ['í‰ë‹¹ê°€', 'í‰ë‹¹ê°€_ë§Œì›']:
            try:
                v = float(deal_dict[k]) if deal_dict[k] is not None else None
            except Exception:
                continue
            deal_dict[k] = as_value_unit(v, "ë§Œì›/í‰")
        elif k in ['areaNum', 'ì „ìš©ë©´ì ', 'excluUseAr', 'ê³„ì•½ë©´ì ', 'totalFloorAr', 'ì—°ë©´ì ', 'YUA', 'plottageAr', 'landAr', 'dealArea', 'ê³„ì•½ë©´ì _num', 'ì „ìš©ë©´ì _num', 'ì—°ë©´ì _num', 'í† ì§€ë©´ì _num']:
            try:
                v = float(deal_dict[k]) if deal_dict[k] is not None else None
            except Exception:
                continue
            deal_dict[k] = as_value_unit(v, "ã¡")
        elif k in ['ê±´ì¶•ë…„ë„', 'buildYear', 'buildYearNum']:
            try:
                v = float(deal_dict[k]) if deal_dict[k] is not None else None
            except Exception:
                continue
            deal_dict[k] = as_value_unit(v, "ë…„")
        elif k in ['floor', 'floorNum']:
            try:
                v = float(deal_dict[k]) if deal_dict[k] is not None else None
            except Exception:
                continue
            deal_dict[k] = as_value_unit(v, "ì¸µ")
    return deal_dict

def get_col_from_df(df, *col_names):
    """DataFrameê³¼ ì—¬ëŸ¬ ì»¬ëŸ¼ ì´ë¦„ì„ ë°›ì•„, ì¡´ì¬í•˜ëŠ” ì²« ë²ˆì§¸ ì»¬ëŸ¼ì„ Seriesë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    for col in col_names:
        if col in df.columns:
            return df[col]
    # ì–´ë–¤ ì»¬ëŸ¼ë„ ì°¾ì§€ ëª»í•œ ê²½ìš°, Noneìœ¼ë¡œ ì±„ì›Œì§„ Seriesë¥¼ ë°˜í™˜í•˜ì—¬ ì´í›„ ì—°ì‚°ì—ì„œ ì˜¤ë¥˜ê°€ ë‚˜ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤.
    return pd.Series([None] * len(df), index=df.index, dtype=object)

def get_summary_cache_path(p: Path, property_type: Optional[str] = None, trade_type: Optional[str] = None) -> Path:
    """
    property_type: 'commercial', 'land', 'industrial', 'apartment', 'officetel', 'row_house', 'single_detached', ...
    trade_type: 'trade', 'rent', None
    """
    # Set cache directory to the correct path relative to project root
    cache_dir = Path(get_cache_dir())
    cache_dir.mkdir(parents=True, exist_ok=True)
    # ìƒì—…/í† ì§€/ì°½ê³ (ì‚°ì—…ìš©)ëŠ” ë§¤ë§¤/ì „ì›”ì„¸ êµ¬ë¶„ ì—†ì´ í•˜ë‚˜ì˜ summary
    if property_type in {"commercial", "land", "industrial"}:
        return cache_dir / f"{p.stem}_summary.json"
    # ê·¸ ì™¸ëŠ” ë§¤ë§¤/ì „ì›”ì„¸ë³„ë¡œ ë¶„ë¦¬
    if trade_type:
        return cache_dir / f"{p.stem}_{trade_type}_summary.json"
    return cache_dir / f"{p.stem}_summary.json"

def analyze_commercial_property_data(df: pd.DataFrame) -> Dict[str, Any]:
    """DataFrameì„ ë°›ì•„ ìƒì—…ì—…ë¬´ìš© ë¶€ë™ì‚° í†µê³„ë¥¼ ë¶„ì„í•˜ê³  ì˜ë¬¸ keyì™€ ë‹¨ìœ„ê°€ í¬í•¨ëœ ê°’ìœ¼ë¡œ JSONì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    
    if df.empty:
        return {"error": "No data to analyze."}

    # --- ë°ì´í„° ì „ì²˜ë¦¬ ---
    df['ê±°ë˜ê¸ˆì•¡_num'] = to_numeric(get_col_from_df(df, 'ê±°ë˜ê¸ˆì•¡', 'dealAmount'))
    df['ì „ìš©ë©´ì _num'] = to_numeric(get_col_from_df(df, 'ì „ìš©ë©´ì ', 'area', 'excluUseAr', 'buildingAr'))
    df['ê±´ì¶•ë…„ë„_num'] = to_numeric(get_col_from_df(df, 'ê±´ì¶•ë…„ë„', 'buildYear'))
    
    df.dropna(subset=['ê±°ë˜ê¸ˆì•¡_num', 'ì „ìš©ë©´ì _num'], inplace=True)
    df = df[df['ì „ìš©ë©´ì _num'] > 0].copy()
    if df.empty:
        return {"error": "No valid transaction data after cleaning."}

    df['í‰ë‹¹ê°€_ë§Œì›'] = (df['ê±°ë˜ê¸ˆì•¡_num'] / df['ì „ìš©ë©´ì _num']) * 3.305785
    
    current_year = datetime.now().year
    df['ê±´ë¬¼ì—°ë ¹'] = current_year - df['ê±´ì¶•ë…„ë„_num']

    # --- Formatting helpers ---
    def as_value_unit_m(v): return as_value_unit(v, "ë§Œì›")
    def as_value_unit_per_pyeong(v): return as_value_unit(v, "ë§Œì›/í‰")

    # --- 1. ì¢…í•© í†µê³„ (Overall Statistics) ---
    total_count = len(df)
    total_value = df['ê±°ë˜ê¸ˆì•¡_num'].sum()
    property_type_col = get_col_from_df(df, 'ì£¼ìš©ë„', 'ìœ í˜•', 'buildingUse')
    type_distribution = property_type_col.value_counts().to_dict() if property_type_col.notna().any() else {}

    overall_stats = {
        "totalTransactionCount": total_count,
        "totalTransactionValue": as_value_unit_m(total_value),
        "transactionDistributionByPropertyType": type_distribution
    }

    # --- 2. ê°€ê²© ìˆ˜ì¤€ í†µê³„ (Price Level Statistics) ---
    price_stats_raw = df['ê±°ë˜ê¸ˆì•¡_num'].agg(['mean', 'median', 'max', 'min'])
    price_stats = {
        "overallAveragePrice": as_value_unit_m(price_stats_raw['mean']),
        "overallMedianPrice": as_value_unit_m(price_stats_raw['median']),
        "overallHighestPrice": as_value_unit_m(price_stats_raw['max']),
        "overallLowestPrice": as_value_unit_m(price_stats_raw['min']),
        "representativeDeals": {
            "highestPriceDeal": clean_deal_for_display(df.loc[df['ê±°ë˜ê¸ˆì•¡_num'].idxmax()]),
            "lowestPriceDeal": clean_deal_for_display(df.loc[df['ê±°ë˜ê¸ˆì•¡_num'].idxmin()]),
            "dealClosestToAverage": clean_deal_for_display(df.loc[(df['ê±°ë˜ê¸ˆì•¡_num'] - price_stats_raw['mean']).abs().idxmin()]),
            "dealClosestToMedian": clean_deal_for_display(df.loc[(df['ê±°ë˜ê¸ˆì•¡_num'] - price_stats_raw['median']).abs().idxmin()])
        }
    }

    if property_type_col.notna().any():
        price_by_type_raw = df.groupby(property_type_col)['ê±°ë˜ê¸ˆì•¡_num'].agg(['mean', 'median', 'max', 'min'])
        price_stats["priceStatisticsByPropertyType"] = {
            prop_type: {
                "averagePrice": as_value_unit_m(stats['mean']),
                "medianPrice": as_value_unit_m(stats['median']),
                "highestPrice": as_value_unit_m(stats['max']),
                "lowestPrice": as_value_unit_m(stats['min']),
            } for prop_type, stats in price_by_type_raw.to_dict('index').items()
        }

    # --- 3. ë‹¨ìœ„ ë©´ì ë‹¹ ê°€ê²© í†µê³„ (Price per Area Statistics) ---
    price_per_area_stats_raw = df['í‰ë‹¹ê°€_ë§Œì›'].agg(['mean', 'median'])
    price_per_area_stats = {
        "overallAveragePricePerPyeong": as_value_unit_per_pyeong(price_per_area_stats_raw['mean']),
        "overallMedianPricePerPyeong": as_value_unit_per_pyeong(price_per_area_stats_raw['median']),
    }
    if property_type_col.notna().any():
        price_per_area_by_type_raw = df.groupby(property_type_col)['í‰ë‹¹ê°€_ë§Œì›'].agg(['mean', 'median'])
        price_per_area_stats["pricePerPyeongStatisticsByPropertyType"] = {
            prop_type: {
                "averagePricePerPyeong": as_value_unit_per_pyeong(stats['mean']),
                "medianPricePerPyeong": as_value_unit_per_pyeong(stats['median']),
            } for prop_type, stats in price_per_area_by_type_raw.to_dict('index').items()
        }

    # --- 4. ì…ì§€ë³„ í†µê³„ (Location-based Statistics) ---
    location_col = get_col_from_df(df, 'ë²•ì •ë™', 'umdNm', 'dong')
    location_stats = {}
    if location_col.notna().any():
        location_summary_raw = df.groupby(location_col).agg(
            Count=('ê±°ë˜ê¸ˆì•¡_num', 'size'),
            Mean_Price=('ê±°ë˜ê¸ˆì•¡_num', 'mean'),
            Max_Price=('ê±°ë˜ê¸ˆì•¡_num', 'max'),
            Min_Price=('ê±°ë˜ê¸ˆì•¡_num', 'min'),
            Mean_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'mean'),
            Median_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'median')
        )
        
        for dong, stats in location_summary_raw.to_dict('index').items():
            location_stats[dong] = {
                "transactionCount": int(stats['Count']),
                "averagePrice": as_value_unit_m(stats['Mean_Price']),
                "highestPrice": as_value_unit_m(stats['Max_Price']),
                "lowestPrice": as_value_unit_m(stats['Min_Price']),
                "averagePricePerPyeong": as_value_unit_per_pyeong(stats['Mean_PPA']),
                "medianPricePerPyeong": as_value_unit_per_pyeong(stats['Median_PPA']),
            }

    # --- 5. ê±´ë¬¼ íŠ¹ì„±ë³„ í†µê³„ (Building Characteristics Statistics) ---
    age_bins = [0, 6, 11, 21, np.inf]
    age_labels = ['5 years or newer', '6-10 years', '11-20 years', 'over 20 years']
    df['ê±´ë¬¼ì—°ë ¹ëŒ€'] = pd.cut(df['ê±´ë¬¼ì—°ë ¹'], bins=age_bins, labels=age_labels, right=False)
    age_stats = {}
    if not df['ê±´ë¬¼ì—°ë ¹ëŒ€'].isnull().all():
        age_summary_raw = df.groupby('ê±´ë¬¼ì—°ë ¹ëŒ€', observed=True).agg(
            Count=('ê±°ë˜ê¸ˆì•¡_num', 'size'),
            Mean_Price=('ê±°ë˜ê¸ˆì•¡_num', 'mean'),
            Mean_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'mean')
        )
        age_stats = {
            age_group: {
                "transactionCount": int(stats['Count']),
                "averagePrice": as_value_unit_m(stats['Mean_Price']),
                "averagePricePerPyeong": as_value_unit_per_pyeong(stats['Mean_PPA']),
            } for age_group, stats in age_summary_raw.to_dict('index').items()
        }

    area_bins = [0, 100, 300, 1000, np.inf]
    area_labels = ['small (<100mÂ²)', 'medium (100-300mÂ²)', 'large (300-1000mÂ²)', 'extra_large (>1000mÂ²)']
    df['ê±´ë¬¼ê·œëª¨'] = pd.cut(df['ì „ìš©ë©´ì _num'], bins=area_bins, labels=area_labels, right=False)
    scale_stats = {}
    if not df['ê±´ë¬¼ê·œëª¨'].isnull().all():
        scale_summary_raw = df.groupby('ê±´ë¬¼ê·œëª¨', observed=True).agg(
            Count=('ê±°ë˜ê¸ˆì•¡_num', 'size'),
            Mean_Price=('ê±°ë˜ê¸ˆì•¡_num', 'mean'),
            Mean_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'mean')
        )
        scale_stats = {
            scale_group: {
                "transactionCount": int(stats['Count']),
                "averagePrice": as_value_unit_m(stats['Mean_Price']),
                "averagePricePerPyeong": as_value_unit_per_pyeong(stats['Mean_PPA']),
            } for scale_group, stats in scale_summary_raw.to_dict('index').items()
        }
    
    building_stats = {
        "statisticsByBuildingAge": age_stats, 
        "statisticsByBuildingScale_exclusiveArea": scale_stats
    }

    return {
        "overallStatistics": overall_stats,
        "priceLevelStatistics": price_stats,
        "pricePerAreaStatistics": price_per_area_stats,
        "locationStatistics_byDong": location_stats,
        "buildingCharacteristicsStatistics": building_stats,
        "notes": "Price per pyeong and building scale are calculated based on the 'exclusive use area'. Data for 'land share' or 'road access conditions' is not provided by the API and is not included in this analysis."
    }

@mcp.tool(
    name="analyze_commercial_property_trade",
    description="""ìƒì—…ì—…ë¬´ìš©(ì˜¤í”¼ìŠ¤, ìƒê°€) ë¶€ë™ì‚° ë§¤ë§¤ ì‹¤ê±°ë˜ ë°ì´í„° íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ì›”ê°„ ë¦¬í¬íŠ¸ í˜•ì‹ì˜ í•µì‹¬ í†µê³„ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.
ì´ ë„êµ¬ëŠ” `get_commercial_property_trade_data`ë¥¼ í†µí•´ ì–»ì€ ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì…ë ¥ë°›ì•„ ì‘ë™í•©ë‹ˆë‹¤.
ì¢…í•© í†µê³„, ê°€ê²© ìˆ˜ì¤€, í‰ë‹¹ê°€, ì…ì§€, ê±´ë¬¼ íŠ¹ì„±ë³„ ë“± ë‹¤ê°ì ì¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì£¼ìš” í†µê³„ ì§€í‘œë“¤ì„ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ì°¨íŠ¸ë‚˜ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.

Arguments:
- file_path (str, required): `get_commercial_property_trade_data` ë„êµ¬ë¡œ ìƒì„±ëœ `raw.data.json` ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œ.

Returns:
- í†µê³„ ë¶„ì„ ê²°ê³¼ê°€ ë‹´ê¸´ ìƒì„¸í•œ JSON ë¬¸ìì—´.
""",
    tags={"í†µê³„", "ë¶„ì„", "ë¦¬í¬íŠ¸", "ìƒì—…ì—…ë¬´ìš©", "ì‹¤ê±°ë˜ê°€"}
)
def analyze_commercial_property_trade(
    file_path: Annotated[str, Field(description="ë¶„ì„í•  raw.data.json íŒŒì¼ ê²½ë¡œ")],
    ctx: Optional[Any] = None
) -> TextContent:
    """
    ìƒì—…ì—…ë¬´ìš© ë¶€ë™ì‚° ê±°ë˜ ë°ì´í„° XML íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ í†µê³„ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    def call(context):
        try:
            p = Path(file_path)
            if not p.exists():
                return json.dumps({"error": f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}"}, ensure_ascii=False)

            cache_path = get_summary_cache_path(p, property_type="commercial")

            # ìºì‹œ í™•ì¸ ë° ì¬ì‚¬ìš© ë¡œì§
            if cache_path.exists():
                source_mtime = p.stat().st_mtime
                cache_mtime = cache_path.stat().st_mtime
                if cache_mtime > source_mtime:
                    logger.info(f"âœ… ìœ íš¨í•œ ìºì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {cache_path}")
                    with open(cache_path, 'r', encoding='utf-8') as f:
                        return f.read()
            
            logger.info(f"ğŸ”„ ìºì‹œê°€ ì—†ê±°ë‚˜ ì˜¤ë˜ë˜ì–´ ìƒˆë¡œìš´ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤: {file_path}")
            df = pd.read_json(p, lines=True)
            
            summary_data = analyze_commercial_property_data(df)
            
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=4, default=default_serializer)

            summary_data["summary_cached_path"] = str(cache_path)
            
            return json.dumps(summary_data, ensure_ascii=False, indent=4, default=default_serializer)

        except Exception as e:
            logger.error(f"ìƒì—…ì—…ë¬´ìš© ë¶€ë™ì‚° ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return json.dumps({"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}, ensure_ascii=False)
            
    result = with_context(ctx, "analyze_commercial_property_trade", call)
    return TextContent(type="text", text=result)

# --- ì•„íŒŒíŠ¸ ë§¤ë§¤ ë¶„ì„ ---

def analyze_apartment_trade_data(df: pd.DataFrame) -> Dict[str, Any]:
    """DataFrameì„ ë°›ì•„ ì•„íŒŒíŠ¸ ë§¤ë§¤ í†µê³„ë¥¼ ë¶„ì„í•˜ê³  ì˜ë¬¸ keyì™€ ë‹¨ìœ„ê°€ í¬í•¨ëœ ê°’ìœ¼ë¡œ JSONì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if df.empty:
        return {"error": "No data to analyze."}

    # --- ë°ì´í„° ì „ì²˜ë¦¬ ---
    df['ê±°ë˜ê¸ˆì•¡_num'] = to_numeric(get_col_from_df(df, 'ê±°ë˜ê¸ˆì•¡', 'dealAmount'))
    df['ì „ìš©ë©´ì _num'] = to_numeric(get_col_from_df(df, 'ì „ìš©ë©´ì ', 'area', 'excluUseAr'))
    df['ê±´ì¶•ë…„ë„_num'] = to_numeric(get_col_from_df(df, 'ê±´ì¶•ë…„ë„', 'buildYear'))
    df['ê³„ì•½ë…„ì›”_num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ë…„ì›”'))
    df['ê³„ì•½ì¼_num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ì¼'))

    df.dropna(subset=['ê±°ë˜ê¸ˆì•¡_num', 'ì „ìš©ë©´ì _num'], inplace=True)
    df = df[df['ì „ìš©ë©´ì _num'] > 0].copy()
    if df.empty:
        return {"error": "No valid transaction data after cleaning."}

    df['í‰ë‹¹ê°€_ë§Œì›'] = (df['ê±°ë˜ê¸ˆì•¡_num'] / df['ì „ìš©ë©´ì _num']) * 3.305785
    current_year = datetime.now().year
    df['ê±´ë¬¼ì—°ë ¹'] = current_year - df['ê±´ì¶•ë…„ë„_num']

    def as_value_unit_m(v): return as_value_unit(v, "ë§Œì›")
    def as_value_unit_per_pyeong(v): return as_value_unit(v, "ë§Œì›/í‰")

    # --- 1. ì¢…í•© í†µê³„ ---
    total_count = len(df)
    total_value = df['ê±°ë˜ê¸ˆì•¡_num'].sum()
    overall_stats = {
        "totalTransactionCount": total_count,
        "totalTransactionValue": as_value_unit_m(total_value),
    }

    # --- 2. ê°€ê²© ìˆ˜ì¤€ í†µê³„ ---
    price_stats_raw = df['ê±°ë˜ê¸ˆì•¡_num'].agg(['mean', 'median', 'max', 'min'])
    price_stats = {
        "overallAveragePrice": as_value_unit_m(price_stats_raw['mean']),
        "overallMedianPrice": as_value_unit_m(price_stats_raw['median']),
        "overallHighestPrice": as_value_unit_m(price_stats_raw['max']),
        "overallLowestPrice": as_value_unit_m(price_stats_raw['min']),
        "representativeDeals": {
            "highestPriceDeal": clean_deal_for_display(df.loc[df['ê±°ë˜ê¸ˆì•¡_num'].idxmax()]),
            "lowestPriceDeal": clean_deal_for_display(df.loc[df['ê±°ë˜ê¸ˆì•¡_num'].idxmin()]),
            "dealClosestToAverage": clean_deal_for_display(df.loc[(df['ê±°ë˜ê¸ˆì•¡_num'] - price_stats_raw['mean']).abs().idxmin()]),
            "dealClosestToMedian": clean_deal_for_display(df.loc[(df['ê±°ë˜ê¸ˆì•¡_num'] - price_stats_raw['median']).abs().idxmin()])
        }
    }

    # --- 3. ë‹¨ìœ„ ë©´ì ë‹¹ ê°€ê²© í†µê³„ ---
    price_per_area_stats = {
        "overallAveragePricePerPyeong": as_value_unit_per_pyeong(df['í‰ë‹¹ê°€_ë§Œì›'].mean()),
        "overallMedianPricePerPyeong": as_value_unit_per_pyeong(df['í‰ë‹¹ê°€_ë§Œì›'].median()),
    }

    # --- 4. ë‹¨ì§€ë³„/ì…ì§€ë³„ í†µê³„ ---
    complex_col = get_col_from_df(df, 'ì•„íŒŒíŠ¸', 'ë‹¨ì§€ëª…', 'aptName')
    location_col = get_col_from_df(df, 'ë²•ì •ë™', 'umdNm', 'dong')
    
    def get_grouped_stats(group_col):
        stats = {}
        if group_col.notna().any():
            summary_raw = df.groupby(group_col).agg(
                Count=('ê±°ë˜ê¸ˆì•¡_num', 'size'),
                Mean_Price=('ê±°ë˜ê¸ˆì•¡_num', 'mean'),
                Median_Price=('ê±°ë˜ê¸ˆì•¡_num', 'median'),
                Mean_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'mean'),
                Median_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'median')
            )
            for name, data in summary_raw.to_dict('index').items():
                stats[name] = {
                    "transactionCount": int(data['Count']),
                    "averagePrice": as_value_unit_m(data['Mean_Price']),
                    "medianPrice": as_value_unit_m(data['Median_Price']),
                    "averagePricePerPyeong": as_value_unit_per_pyeong(data['Mean_PPA']),
                    "medianPricePerPyeong": as_value_unit_per_pyeong(data['Median_PPA']),
                }
        return stats

    complex_stats = get_grouped_stats(complex_col)
    location_stats = get_grouped_stats(location_col)

    return {
        "overallStatistics": overall_stats,
        "priceLevelStatistics": price_stats,
        "pricePerAreaStatistics": price_per_area_stats,
        "statisticsByApartmentComplex": complex_stats,
        "statisticsByDong": location_stats,
        "notes": "PPA (Price Per Pyeong) is calculated based on the 'exclusive use area'."
    }

@mcp.tool(
    name="analyze_apartment_trade",
    description="""ì•„íŒŒíŠ¸ ë§¤ë§¤ ì‹¤ê±°ë˜ ë°ì´í„° íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ì›”ê°„ ë¦¬í¬íŠ¸ í˜•ì‹ì˜ í•µì‹¬ í†µê³„ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.
ì´ ë„êµ¬ëŠ” `get_apt_trade_data`ë¥¼ í†µí•´ ì–»ì€ ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì…ë ¥ë°›ì•„ ì‘ë™í•©ë‹ˆë‹¤.
ì¢…í•© í†µê³„, ê°€ê²© ìˆ˜ì¤€, í‰ë‹¹ê°€, ë‹¨ì§€ë³„, ë™ë³„ ë“± ë‹¤ê°ì ì¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì£¼ìš” í†µê³„ ì§€í‘œë“¤ì„ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ì°¨íŠ¸ë‚˜ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.

Arguments:
- file_path (str, required): `get_apt_trade_data` ë„êµ¬ë¡œ ìƒì„±ëœ `raw.data.json` ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œ.

Returns:
- í†µê³„ ë¶„ì„ ê²°ê³¼ê°€ ë‹´ê¸´ ìƒì„¸í•œ JSON ë¬¸ìì—´.
""",
    tags={"ì•„íŒŒíŠ¸", "í†µê³„", "ë¶„ì„", "ë¦¬í¬íŠ¸", "ë§¤ë§¤", "ì‹¤ê±°ë˜ê°€"}
)
def analyze_apartment_trade(
    file_path: Annotated[str, Field(description="ë¶„ì„í•  raw.data.json íŒŒì¼ ê²½ë¡œ")],
    ctx: Optional[Any] = None
) -> TextContent:
    """
    ì•„íŒŒíŠ¸ ë§¤ë§¤ ê±°ë˜ ë°ì´í„° XML íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ í†µê³„ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    def call(context):
        try:
            p = Path(file_path)
            if not p.exists():
                return json.dumps({"error": f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}"}, ensure_ascii=False)

            cache_path = get_summary_cache_path(p, property_type="apartment", trade_type="trade")

            if cache_path.exists():
                if cache_path.stat().st_mtime > p.stat().st_mtime:
                    logger.info(f"âœ… ìœ íš¨í•œ ì•„íŒŒíŠ¸ ë§¤ë§¤ ìºì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {cache_path}")
                    with open(cache_path, 'r', encoding='utf-8') as f:
                        return f.read()
            
            logger.info(f"ğŸ”„ ìƒˆë¡œìš´ ì•„íŒŒíŠ¸ ë§¤ë§¤ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤: {file_path}")
            df = pd.read_json(p, lines=True)
            
            summary_data = analyze_apartment_trade_data(df)
            
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=4, default=default_serializer)

            summary_data["summary_cached_path"] = str(cache_path)
            return json.dumps(summary_data, ensure_ascii=False, indent=4, default=default_serializer)

        except Exception as e:
            logger.error(f"ì•„íŒŒíŠ¸ ë§¤ë§¤ ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return json.dumps({"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}, ensure_ascii=False)
            
    result = with_context(ctx, "analyze_apartment_trade", call)
    return TextContent(type="text", text=result)

# --- ì•„íŒŒíŠ¸ ì „ì›”ì„¸ ë¶„ì„ ---

def analyze_apartment_rent_data(df: pd.DataFrame) -> Dict[str, Any]:
    """DataFrameì„ ë°›ì•„ ì•„íŒŒíŠ¸ ì „ì›”ì„¸ í†µê³„ë¥¼ ë¶„ì„í•˜ê³  ì „ì„¸/ì›”ì„¸ë¥¼ êµ¬ë¶„í•˜ì—¬ JSONìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if df.empty:
        return {"error": "No data to analyze."}

    # --- ë°ì´í„° ì „ì²˜ë¦¬ ---
    df['ë³´ì¦ê¸ˆ_num'] = to_numeric(get_col_from_df(df, 'ë³´ì¦ê¸ˆì•¡', 'ë³´ì¦ê¸ˆ', 'deposit', 'depositNum'))
    df['ì›”ì„¸_num'] = to_numeric(get_col_from_df(df, 'ì›”ì„¸ì•¡', 'ì›”ì„¸', 'monthlyRent', 'rentFeeNum'))
    df['ì „ìš©ë©´ì _num'] = to_numeric(get_col_from_df(df, 'ì „ìš©ë©´ì ', 'area', 'excluUseAr', 'areaNum'))
    df['ê±´ì¶•ë…„ë„_num'] = to_numeric(get_col_from_df(df, 'ê±´ì¶•ë…„ë„', 'buildYear'))
    df['ê³„ì•½ë…„ì›”_num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ë…„ì›”'))
    df['ê³„ì•½ì¼_num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ì¼'))

    df.dropna(subset=['ë³´ì¦ê¸ˆ_num', 'ì›”ì„¸_num', 'ì „ìš©ë©´ì _num'], inplace=True)
    df = df[df['ì „ìš©ë©´ì _num'] > 0].copy()
    if df.empty:
        return {"error": "No valid transaction data after cleaning."}

    df_jeonse = df[df['ì›”ì„¸_num'] == 0].copy()
    df_wolse = df[df['ì›”ì„¸_num'] > 0].copy()

    def as_value_unit_m(v): return as_value_unit(v, "ë§Œì›")
    
    def analyze_rent_type(df_rent_type, rent_type_name):
        if df_rent_type.empty:
            return { "totalTransactionCount": 0 }

        stats = { "totalTransactionCount": len(df_rent_type) }
        
        price_stats_raw = df_rent_type['ë³´ì¦ê¸ˆ_num'].agg(['mean', 'median', 'max', 'min'])
        stats['depositPriceStatistics'] = {
            "averageDeposit": as_value_unit_m(price_stats_raw['mean']),
            "medianDeposit": as_value_unit_m(price_stats_raw['median']),
            "highestDeposit": as_value_unit_m(price_stats_raw['max']),
            "lowestDeposit": as_value_unit_m(price_stats_raw['min']),
            "representativeDeals": {
                "highestDepositDeal": clean_deal_for_display(df_rent_type.loc[df_rent_type['ë³´ì¦ê¸ˆ_num'].idxmax()]),
                "lowestDepositDeal": clean_deal_for_display(df_rent_type.loc[df_rent_type['ë³´ì¦ê¸ˆ_num'].idxmin()]),
            }
        }
        if rent_type_name == 'wolse': # ì›”ì„¸ í†µê³„ ì¶”ê°€
            wolse_stats_raw = df_rent_type['ì›”ì„¸_num'].agg(['mean', 'median', 'max', 'min'])
            stats['monthlyRentStatistics'] = {
                "averageMonthlyRent": as_value_unit_m(wolse_stats_raw['mean']),
                "medianMonthlyRent": as_value_unit_m(wolse_stats_raw['median']),
            }

        complex_col = get_col_from_df(df_rent_type, 'ì•„íŒŒíŠ¸', 'ë‹¨ì§€ëª…', 'aptName')
        if complex_col.notna().any():
            stats['statisticsByApartmentComplex'] = df_rent_type.groupby(complex_col).agg(
                transactionCount=('ë³´ì¦ê¸ˆ_num', 'size'),
                averageDeposit=('ë³´ì¦ê¸ˆ_num', 'mean')
            ).apply(lambda x: x.astype(int) if x.name == 'transactionCount' else as_value_unit_m(x)).to_dict('index')

        return stats

    jeonse_analysis = analyze_rent_type(df_jeonse, 'jeonse')
    wolse_analysis = analyze_rent_type(df_wolse, 'wolse')
    
    return {
        "transactionTypeDistribution": {
            "jeonse_count": len(df_jeonse),
            "wolse_count": len(df_wolse),
        },
        "jeonseAnalysis": jeonse_analysis,
        "wolseAnalysis": wolse_analysis,
        "notes": "Statistics are separated by transaction type (Jeonse vs. Wolse)."
    }

@mcp.tool(
    name="analyze_apartment_rent",
    description="""ì•„íŒŒíŠ¸ ì „ì›”ì„¸ ì‹¤ê±°ë˜ ë°ì´í„° íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ì›”ê°„ ë¦¬í¬íŠ¸ í˜•ì‹ì˜ í•µì‹¬ í†µê³„ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.\nì´ ë„êµ¬ëŠ” `get_apt_rent_data`ë¥¼ í†µí•´ ì–»ì€ ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì…ë ¥ë°›ì•„ ì‘ë™í•˜ë©°, 'ì „ì„¸'ì™€ 'ì›”ì„¸'ë¥¼ ìë™ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ê°ê°ì— ëŒ€í•œ ìƒì„¸ í†µê³„ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\në¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì£¼ìš” í†µê³„ ì§€í‘œë“¤ì„ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ì°¨íŠ¸ë‚˜ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n\n- **ê±°ë˜ ìœ í˜• ë¶„í¬**: ì „ì²´ ê±°ë˜ ì¤‘ ì „ì„¸ì™€ ì›”ì„¸ì˜ ë¹„ì¤‘ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n- **ì „ì„¸ ë¶„ì„**: ë³´ì¦ê¸ˆì— ëŒ€í•œ í‰ê· /ì¤‘ìœ„/ìµœê³ /ìµœì € ê°€ê²© ë° ì£¼ìš” ê±°ë˜ ì‚¬ë¡€ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ë‹¨ì§€ë³„ í†µê³„ë„ í¬í•¨ë©ë‹ˆë‹¤.\n- **ì›”ì„¸ ë¶„ì„**: ë³´ì¦ê¸ˆ ë° ì›”ì„¸ ê°ê°ì— ëŒ€í•œ í‰ê· /ì¤‘ìœ„ ê°€ê²© í†µê³„ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ë‹¨ì§€ë³„ í†µê³„ë„ í¬í•¨ë©ë‹ˆë‹¤.\n\nArguments:\n- file_path (str, required): `get_apt_rent_data` ë„êµ¬ë¡œ ìƒì„±ëœ `raw.data.json` ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œ.\n\nReturns:\n- ì „ì„¸ì™€ ì›”ì„¸ë¡œ êµ¬ë¶„ëœ í†µê³„ ë¶„ì„ ê²°ê³¼ê°€ ë‹´ê¸´ ìƒì„¸í•œ JSON ë¬¸ìì—´.""",
    tags={"ì•„íŒŒíŠ¸", "í†µê³„", "ë¶„ì„", "ë¦¬í¬íŠ¸", "ì „ì„¸", "ì›”ì„¸", "ì „ì›”ì„¸", "ì‹¤ê±°ë˜ê°€"}
)
def analyze_apartment_rent(
    file_path: Annotated[str, Field(description="ë¶„ì„í•  raw.data.json íŒŒì¼ ê²½ë¡œ")],
    ctx: Optional[Any] = None
) -> TextContent:
    def call(context):
        try:
            p = Path(file_path)
            if not p.exists():
                return json.dumps({"error": f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}"}, ensure_ascii=False)
            cache_path = get_summary_cache_path(p, property_type="apartment", trade_type="rent")
            if cache_path.exists():
                if cache_path.stat().st_mtime > p.stat().st_mtime:
                    logger.info(f"âœ… ìœ íš¨í•œ ì•„íŒŒíŠ¸ ì „ì›”ì„¸ ìºì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {cache_path}")
                    with open(cache_path, 'r', encoding='utf-8') as f:
                        return f.read()
            logger.info(f"ğŸ”„ ìƒˆë¡œìš´ ì•„íŒŒíŠ¸ ì „ì›”ì„¸ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤: {file_path}")
            df = pd.read_json(p, lines=True)
            summary_data = analyze_apartment_rent_data(df)
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=4, default=default_serializer)
            summary_data["summary_cached_path"] = str(cache_path)
            return json.dumps(summary_data, ensure_ascii=False, indent=4, default=default_serializer)
        except Exception as e:
            logger.error(f"ì•„íŒŒíŠ¸ ì „ì›”ì„¸ ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return json.dumps({"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}, ensure_ascii=False)
    result = with_context(ctx, "analyze_apartment_rent", call)
    return TextContent(type="text", text=result)

# --- ì˜¤í”¼ìŠ¤í…” ë§¤ë§¤ ë¶„ì„ ---

def analyze_officetel_trade_data(df: pd.DataFrame) -> Dict[str, Any]:
    """DataFrameì„ ë°›ì•„ ì˜¤í”¼ìŠ¤í…” ë§¤ë§¤ í†µê³„ë¥¼ ë¶„ì„í•˜ê³  ì˜ë¬¸ keyì™€ ë‹¨ìœ„ê°€ í¬í•¨ëœ ê°’ìœ¼ë¡œ JSONì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if df.empty:
        return {"error": "No data to analyze."}

    # --- ë°ì´í„° ì „ì²˜ë¦¬ ---
    df['ê±°ë˜ê¸ˆì•¡_num'] = to_numeric(get_col_from_df(df, 'ê±°ë˜ê¸ˆì•¡', 'dealAmount', 'dealAmountNum'))
    df['ì „ìš©ë©´ì _num'] = to_numeric(get_col_from_df(df, 'ì „ìš©ë©´ì ', 'area', 'excluUseAr', 'areaNum'))
    df['ê±´ì¶•ë…„ë„_num'] = to_numeric(get_col_from_df(df, 'ê±´ì¶•ë…„ë„', 'buildYear', 'buildYearNum'))
    df['ê³„ì•½ë…„ì›”_num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ë…„ì›”'))
    df['ê³„ì•½ì¼_num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ì¼'))

    df.dropna(subset=['ê±°ë˜ê¸ˆì•¡_num', 'ì „ìš©ë©´ì _num'], inplace=True)
    df = df[df['ì „ìš©ë©´ì _num'] > 0].copy()
    if df.empty:
        return {"error": "No valid transaction data after cleaning."}

    df['í‰ë‹¹ê°€_ë§Œì›'] = (df['ê±°ë˜ê¸ˆì•¡_num'] / df['ì „ìš©ë©´ì _num']) * 3.305785
    current_year = datetime.now().year
    df['ê±´ë¬¼ì—°ë ¹'] = current_year - df['ê±´ì¶•ë…„ë„_num']

    def as_value_unit_m(v): return as_value_unit(v, "ë§Œì›")
    def as_value_unit_per_pyeong(v): return as_value_unit(v, "ë§Œì›/í‰")

    # --- 1. ì¢…í•© í†µê³„ ---
    total_count = len(df)
    total_value = df['ê±°ë˜ê¸ˆì•¡_num'].sum()
    overall_stats = {
        "totalTransactionCount": total_count,
        "totalTransactionValue": as_value_unit_m(total_value),
    }

    # --- 2. ê°€ê²© ìˆ˜ì¤€ í†µê³„ ---
    price_stats_raw = df['ê±°ë˜ê¸ˆì•¡_num'].agg(['mean', 'median', 'max', 'min'])
    price_stats = {
        "overallAveragePrice": as_value_unit_m(price_stats_raw['mean']),
        "overallMedianPrice": as_value_unit_m(price_stats_raw['median']),
        "overallHighestPrice": as_value_unit_m(price_stats_raw['max']),
        "overallLowestPrice": as_value_unit_m(price_stats_raw['min']),
        "representativeDeals": {
            "highestPriceDeal": clean_deal_for_display(df.loc[df['ê±°ë˜ê¸ˆì•¡_num'].idxmax()]),
            "lowestPriceDeal": clean_deal_for_display(df.loc[df['ê±°ë˜ê¸ˆì•¡_num'].idxmin()]),
            "dealClosestToAverage": clean_deal_for_display(df.loc[(df['ê±°ë˜ê¸ˆì•¡_num'] - price_stats_raw['mean']).abs().idxmin()]),
            "dealClosestToMedian": clean_deal_for_display(df.loc[(df['ê±°ë˜ê¸ˆì•¡_num'] - price_stats_raw['median']).abs().idxmin()])
        }
    }

    # --- 3. ë‹¨ìœ„ ë©´ì ë‹¹ ê°€ê²© í†µê³„ ---
    price_per_area_stats = {
        "overallAveragePricePerPyeong": as_value_unit_per_pyeong(df['í‰ë‹¹ê°€_ë§Œì›'].mean()),
        "overallMedianPricePerPyeong": as_value_unit_per_pyeong(df['í‰ë‹¹ê°€_ë§Œì›'].median()),
    }

    # --- 4. ë‹¨ì§€ë³„/ì…ì§€ë³„ í†µê³„ ---
    complex_col = get_col_from_df(df, 'ì˜¤í”¼ìŠ¤í…”', 'ì˜¤í”¼ìŠ¤í…”ëª…', 'officetelName')
    location_col = get_col_from_df(df, 'ë²•ì •ë™', 'umdNm', 'dong')
    def get_grouped_stats(group_col):
        stats = {}
        if group_col.notna().any():
            summary_raw = df.groupby(group_col).agg(
                Count=('ê±°ë˜ê¸ˆì•¡_num', 'size'),
                Mean_Price=('ê±°ë˜ê¸ˆì•¡_num', 'mean'),
                Median_Price=('ê±°ë˜ê¸ˆì•¡_num', 'median'),
                Mean_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'mean'),
                Median_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'median')
            )
            for name, data in summary_raw.to_dict('index').items():
                stats[name] = {
                    "transactionCount": int(data['Count']),
                    "averagePrice": as_value_unit_m(data['Mean_Price']),
                    "medianPrice": as_value_unit_m(data['Median_Price']),
                    "averagePricePerPyeong": as_value_unit_per_pyeong(data['Mean_PPA']),
                    "medianPricePerPyeong": as_value_unit_per_pyeong(data['Median_PPA']),
                }
        return stats

    complex_stats = get_grouped_stats(complex_col)
    location_stats = get_grouped_stats(location_col)

    return {
        "overallStatistics": overall_stats,
        "priceLevelStatistics": price_stats,
        "pricePerAreaStatistics": price_per_area_stats,
        "statisticsByOfficetelComplex": complex_stats,
        "statisticsByDong": location_stats,
        "notes": "PPA (Price Per Pyeong) is calculated based on the 'exclusive use area'."
    }

@mcp.tool(
    name="analyze_officetel_trade",
    description="""ì˜¤í”¼ìŠ¤í…” ë§¤ë§¤ ì‹¤ê±°ë˜ ë°ì´í„° íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ì›”ê°„ ë¦¬í¬íŠ¸ í˜•ì‹ì˜ í•µì‹¬ í†µê³„ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.\nì´ ë„êµ¬ëŠ” `get_officetel_trade_data`ë¥¼ í†µí•´ ì–»ì€ ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì…ë ¥ë°›ì•„ ì‘ë™í•©ë‹ˆë‹¤.\nì¢…í•© í†µê³„, ê°€ê²© ìˆ˜ì¤€, í‰ë‹¹ê°€, ë‹¨ì§€ë³„, ë™ë³„ ë“± ë‹¤ê°ì ì¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\në¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì£¼ìš” í†µê³„ ì§€í‘œë“¤ì„ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ì°¨íŠ¸ë‚˜ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n\nArguments:\n- file_path (str, required): `get_officetel_trade_data` ë„êµ¬ë¡œ ìƒì„±ëœ `raw.data.json` ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œ.\n\nReturns:\n- í†µê³„ ë¶„ì„ ê²°ê³¼ê°€ ë‹´ê¸´ ìƒì„¸í•œ JSON ë¬¸ìì—´.""",
    tags={"ì˜¤í”¼ìŠ¤í…”", "í†µê³„", "ë¶„ì„", "ë¦¬í¬íŠ¸", "ë§¤ë§¤", "ì‹¤ê±°ë˜ê°€"}
)
def analyze_officetel_trade(
    file_path: Annotated[str, Field(description="ë¶„ì„í•  raw.data.json íŒŒì¼ ê²½ë¡œ")],
    ctx: Optional[Any] = None
) -> TextContent:
    def call(context):
        try:
            p = Path(file_path)
            if not p.exists():
                return json.dumps({"error": f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}"}, ensure_ascii=False)
            cache_path = get_summary_cache_path(p, property_type="officetel", trade_type="trade")
            if cache_path.exists():
                if cache_path.stat().st_mtime > p.stat().st_mtime:
                    logger.info(f"âœ… ìœ íš¨í•œ ì˜¤í”¼ìŠ¤í…” ë§¤ë§¤ ìºì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {cache_path}")
                    with open(cache_path, 'r', encoding='utf-8') as f:
                        return f.read()
            logger.info(f"ğŸ”„ ìƒˆë¡œìš´ ì˜¤í”¼ìŠ¤í…” ë§¤ë§¤ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤: {file_path}")
            df = pd.read_json(p, lines=True)
            summary_data = analyze_officetel_trade_data(df)
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=4, default=default_serializer)
            summary_data["summary_cached_path"] = str(cache_path)
            return json.dumps(summary_data, ensure_ascii=False, indent=4, default=default_serializer)
        except Exception as e:
            logger.error(f"ì˜¤í”¼ìŠ¤í…” ë§¤ë§¤ ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return json.dumps({"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}, ensure_ascii=False)
    result = with_context(ctx, "analyze_officetel_trade", call)
    return TextContent(type="text", text=result)

# --- ì˜¤í”¼ìŠ¤í…” ì „ì›”ì„¸ ë¶„ì„ ---
def analyze_officetel_rent_data(df: pd.DataFrame) -> Dict[str, Any]:
    """DataFrameì„ ë°›ì•„ ì˜¤í”¼ìŠ¤í…” ì „ì›”ì„¸ í†µê³„ë¥¼ ë¶„ì„í•˜ê³  ì „ì„¸/ì›”ì„¸ë¥¼ êµ¬ë¶„í•˜ì—¬ JSONìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if df.empty:
        return {"error": "No data to analyze."}
    df['ë³´ì¦ê¸ˆ_num'] = to_numeric(get_col_from_df(df, 'ë³´ì¦ê¸ˆì•¡', 'ë³´ì¦ê¸ˆ', 'deposit', 'depositNum'))
    df['ì›”ì„¸_num'] = to_numeric(get_col_from_df(df, 'ì›”ì„¸ì•¡', 'ì›”ì„¸', 'monthlyRent', 'rentFeeNum'))
    df['ì „ìš©ë©´ì _num'] = to_numeric(get_col_from_df(df, 'ì „ìš©ë©´ì ', 'area', 'excluUseAr', 'areaNum'))
    df['ê±´ì¶•ë…„ë„_num'] = to_numeric(get_col_from_df(df, 'ê±´ì¶•ë…„ë„', 'buildYear', 'buildYearNum'))
    df['ê³„ì•½ë…„ì›”_num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ë…„ì›”'))
    df['ê³„ì•½ì¼_num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ì¼'))
    df.dropna(subset=['ë³´ì¦ê¸ˆ_num', 'ì›”ì„¸_num', 'ì „ìš©ë©´ì _num'], inplace=True)
    df = df[df['ì „ìš©ë©´ì _num'] > 0].copy()
    if df.empty:
        return {"error": "No valid transaction data after cleaning."}
    df_jeonse = df[df['ì›”ì„¸_num'] == 0].copy()
    df_wolse = df[df['ì›”ì„¸_num'] > 0].copy()
    def as_value_unit_m(v): return as_value_unit(v, "ë§Œì›")
    def analyze_rent_type(df_rent_type, rent_type_name):
        if df_rent_type.empty:
            return { "totalTransactionCount": 0 }
        stats = { "totalTransactionCount": len(df_rent_type) }
        price_stats_raw = df_rent_type['ë³´ì¦ê¸ˆ_num'].agg(['mean', 'median', 'max', 'min'])
        stats['depositPriceStatistics'] = {
            "averageDeposit": as_value_unit_m(price_stats_raw['mean']),
            "medianDeposit": as_value_unit_m(price_stats_raw['median']),
            "highestDeposit": as_value_unit_m(price_stats_raw['max']),
            "lowestDeposit": as_value_unit_m(price_stats_raw['min']),
            "representativeDeals": {
                "highestDepositDeal": clean_deal_for_display(df_rent_type.loc[df_rent_type['ë³´ì¦ê¸ˆ_num'].idxmax()]),
                "lowestDepositDeal": clean_deal_for_display(df_rent_type.loc[df_rent_type['ë³´ì¦ê¸ˆ_num'].idxmin()]),
            }
        }
        if rent_type_name == 'wolse':
            wolse_stats_raw = df_rent_type['ì›”ì„¸_num'].agg(['mean', 'median', 'max', 'min'])
            stats['monthlyRentStatistics'] = {
                "averageMonthlyRent": as_value_unit_m(wolse_stats_raw['mean']),
                "medianMonthlyRent": as_value_unit_m(wolse_stats_raw['median']),
            }
        building_col = get_col_from_df(df_rent_type, 'ì˜¤í”¼ìŠ¤í…”', 'ì˜¤í”¼ìŠ¤í…”ëª…', 'officetelName')
        if building_col.notna().any():
            stats['statisticsByOfficetelComplex'] = df_rent_type.groupby(building_col).agg(
                transactionCount=('ë³´ì¦ê¸ˆ_num', 'size'),
                averageDeposit=('ë³´ì¦ê¸ˆ_num', 'mean')
            ).apply(lambda x: x.astype(int) if x.name == 'transactionCount' else as_value_unit_m(x)).to_dict('index')
        return stats
    jeonse_analysis = analyze_rent_type(df_jeonse, 'jeonse')
    wolse_analysis = analyze_rent_type(df_wolse, 'wolse')
    return {
        "transactionTypeDistribution": {
            "jeonse_count": len(df_jeonse),
            "wolse_count": len(df_wolse),
        },
        "jeonseAnalysis": jeonse_analysis,
        "wolseAnalysis": wolse_analysis,
        "notes": "Statistics are separated by transaction type (Jeonse vs. Wolse). ê³„ì•½ë©´ì  ê¸°ì¤€ìœ¼ë¡œ ì§‘ê³„í•©ë‹ˆë‹¤."
    }

@mcp.tool(
    name="analyze_officetel_rent",
    description="""ì˜¤í”¼ìŠ¤í…” ì „ì›”ì„¸ ì‹¤ê±°ë˜ ë°ì´í„° íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ì›”ê°„ ë¦¬í¬íŠ¸ í˜•ì‹ì˜ í•µì‹¬ í†µê³„ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.\nì´ ë„êµ¬ëŠ” `get_officetel_rent_data`ë¥¼ í†µí•´ ì–»ì€ ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì…ë ¥ë°›ì•„ ì‘ë™í•˜ë©°, 'ì „ì„¸'ì™€ 'ì›”ì„¸'ë¥¼ ìë™ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ê°ê°ì— ëŒ€í•œ ìƒì„¸ í†µê³„ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\në¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì£¼ìš” í†µê³„ ì§€í‘œë“¤ì„ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ì°¨íŠ¸ë‚˜ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n\n- **ê±°ë˜ ìœ í˜• ë¶„í¬**: ì „ì²´ ê±°ë˜ ì¤‘ ì „ì„¸ì™€ ì›”ì„¸ì˜ ë¹„ì¤‘ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n- **ì „ì„¸ ë¶„ì„**: ë³´ì¦ê¸ˆì— ëŒ€í•œ í‰ê· /ì¤‘ìœ„/ìµœê³ /ìµœì € ê°€ê²© ë° ì£¼ìš” ê±°ë˜ ì‚¬ë¡€ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ê±´ë¬¼ëª…ë³„ í†µê³„ë„ í¬í•¨ë©ë‹ˆë‹¤.\n- **ì›”ì„¸ ë¶„ì„**: ë³´ì¦ê¸ˆ ë° ì›”ì„¸ ê°ê°ì— ëŒ€í•œ í‰ê· /ì¤‘ìœ„ ê°€ê²© í†µê³„ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ê±´ë¬¼ëª…ë³„ í†µê³„ë„ í¬í•¨ë©ë‹ˆë‹¤.\n\nArguments:\n- file_path (str, required): `get_officetel_rent_data` ë„êµ¬ë¡œ ìƒì„±ëœ `raw.data.json` ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œ.\n\nReturns:\n- ì „ì„¸ì™€ ì›”ì„¸ë¡œ êµ¬ë¶„ëœ í†µê³„ ë¶„ì„ ê²°ê³¼ê°€ ë‹´ê¸´ ìƒì„¸í•œ JSON ë¬¸ìì—´.""",
    tags={"ì˜¤í”¼ìŠ¤í…”", "í†µê³„", "ë¶„ì„", "ë¦¬í¬íŠ¸", "ì „ì„¸", "ì›”ì„¸", "ì „ì›”ì„¸", "ì‹¤ê±°ë˜ê°€"}
)
def analyze_officetel_rent(
    file_path: Annotated[str, Field(description="ë¶„ì„í•  raw.data.json íŒŒì¼ ê²½ë¡œ")],
    ctx: Optional[Any] = None
) -> TextContent:
    def call(context):
        try:
            p = Path(file_path)
            if not p.exists():
                return json.dumps({"error": f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}"}, ensure_ascii=False)
            cache_path = get_summary_cache_path(p, property_type="officetel", trade_type="rent")
            if cache_path.exists():
                if cache_path.stat().st_mtime > p.stat().st_mtime:
                    logger.info(f"âœ… ìœ íš¨í•œ ì˜¤í”¼ìŠ¤í…” ì „ì›”ì„¸ ìºì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {cache_path}")
                    with open(cache_path, 'r', encoding='utf-8') as f:
                        return f.read()
            logger.info(f"ğŸ”„ ìƒˆë¡œìš´ ì˜¤í”¼ìŠ¤í…” ì „ì›”ì„¸ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤: {file_path}")
            df = pd.read_json(p, lines=True)
            summary_data = analyze_officetel_rent_data(df)
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=4, default=default_serializer)
            summary_data["summary_cached_path"] = str(cache_path)
            return json.dumps(summary_data, ensure_ascii=False, indent=4, default=default_serializer)
        except Exception as e:
            logger.error(f"ì˜¤í”¼ìŠ¤í…” ì „ì›”ì„¸ ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return json.dumps({"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}, ensure_ascii=False)
    result = with_context(ctx, "analyze_officetel_rent", call)
    return TextContent(type="text", text=result)

def analyze_single_detached_trade_data(df: pd.DataFrame) -> Dict[str, Any]:
    """DataFrameì„ ë°›ì•„ ë‹¨ë…/ë‹¤ê°€êµ¬ ë§¤ë§¤ í†µê³„ë¥¼ ë¶„ì„í•˜ê³  ì˜ë¬¸ keyì™€ ë‹¨ìœ„ê°€ í¬í•¨ëœ ê°’ìœ¼ë¡œ JSONì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if df.empty:
        return {"error": "No data to analyze."}
    # --- ë°ì´í„° ì „ì²˜ë¦¬ ---
    df['ê±°ë˜ê¸ˆì•¡_num'] = to_numeric(get_col_from_df(df, 'ê±°ë˜ê¸ˆì•¡', 'dealAmount', 'dealAmountNum'))
    # Fix: Use totalFloorAr if areaNum/YUA are null
    df['ì—°ë©´ì _num'] = to_numeric(get_col_from_df(df, 'ì—°ë©´ì ', 'YUA', 'totalFloorAr', 'areaNum'))
    df['ê±´ì¶•ë…„ë„_num'] = to_numeric(get_col_from_df(df, 'ê±´ì¶•ë…„ë„', 'buildYear', 'buildYearNum'))
    df['ê³„ì•½ë…„ì›”_num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ë…„ì›”'))
    df['ê³„ì•½ì¼_num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ì¼'))
    df.dropna(subset=['ê±°ë˜ê¸ˆì•¡_num', 'ì—°ë©´ì _num'], inplace=True)
    df = df[df['ì—°ë©´ì _num'] > 0].copy()
    if df.empty:
        return {"error": "No valid transaction data after cleaning."}
    df['í‰ë‹¹ê°€_ë§Œì›'] = (df['ê±°ë˜ê¸ˆì•¡_num'] / df['ì—°ë©´ì _num']) * 3.305785
    current_year = datetime.now().year
    df['ê±´ë¬¼ì—°ë ¹'] = current_year - df['ê±´ì¶•ë…„ë„_num']
    def as_value_unit_m(v): return as_value_unit(v, "ë§Œì›")
    def as_value_unit_per_pyeong(v): return as_value_unit(v, "ë§Œì›/í‰")
    # --- 1. ì¢…í•© í†µê³„ ---
    total_count = len(df)
    total_value = df['ê±°ë˜ê¸ˆì•¡_num'].sum()
    overall_stats = {
        "totalTransactionCount": total_count,
        "totalTransactionValue": as_value_unit_m(total_value),
    }
    # --- 2. ê°€ê²© ìˆ˜ì¤€ í†µê³„ ---
    price_stats_raw = df['ê±°ë˜ê¸ˆì•¡_num'].agg(['mean', 'median', 'max', 'min'])
    price_stats = {
        "overallAveragePrice": as_value_unit_m(price_stats_raw['mean']),
        "overallMedianPrice": as_value_unit_m(price_stats_raw['median']),
        "overallHighestPrice": as_value_unit_m(price_stats_raw['max']),
        "overallLowestPrice": as_value_unit_m(price_stats_raw['min']),
        "representativeDeals": {
            "highestPriceDeal": clean_deal_for_display(df.loc[df['ê±°ë˜ê¸ˆì•¡_num'].idxmax()]),
            "lowestPriceDeal": clean_deal_for_display(df.loc[df['ê±°ë˜ê¸ˆì•¡_num'].idxmin()]),
            "dealClosestToAverage": clean_deal_for_display(df.loc[(df['ê±°ë˜ê¸ˆì•¡_num'] - price_stats_raw['mean']).abs().idxmin()]),
            "dealClosestToMedian": clean_deal_for_display(df.loc[(df['ê±°ë˜ê¸ˆì•¡_num'] - price_stats_raw['median']).abs().idxmin()])
        }
    }
    # --- 3. ë‹¨ìœ„ ë©´ì ë‹¹ ê°€ê²© í†µê³„ ---
    price_per_area_stats = {
        "overallAveragePricePerPyeong": as_value_unit_per_pyeong(df['í‰ë‹¹ê°€_ë§Œì›'].mean()),
        "overallMedianPricePerPyeong": as_value_unit_per_pyeong(df['í‰ë‹¹ê°€_ë§Œì›'].median()),
    }
    # --- 4. ê±´ë¬¼ëª…/ë™ë³„ í†µê³„ ---
    building_col = get_col_from_df(df, 'ê±´ë¬¼ëª…', 'buildingName')
    location_col = get_col_from_df(df, 'ë²•ì •ë™', 'umdNm', 'dong')
    def get_grouped_stats(group_col):
        stats = {}
        if group_col.notna().any():
            summary_raw = df.groupby(group_col).agg(
                Count=('ê±°ë˜ê¸ˆì•¡_num', 'size'),
                Mean_Price=('ê±°ë˜ê¸ˆì•¡_num', 'mean'),
                Median_Price=('ê±°ë˜ê¸ˆì•¡_num', 'median'),
                Mean_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'mean'),
                Median_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'median')
            )
            for name, data in summary_raw.to_dict('index').items():
                stats[name] = {
                    "transactionCount": int(data['Count']),
                    "averagePrice": as_value_unit_m(data['Mean_Price']),
                    "medianPrice": as_value_unit_m(data['Median_Price']),
                    "averagePricePerPyeong": as_value_unit_per_pyeong(data['Mean_PPA']),
                    "medianPricePerPyeong": as_value_unit_per_pyeong(data['Median_PPA']),
                }
        return stats
    building_stats = get_grouped_stats(building_col)
    location_stats = get_grouped_stats(location_col)
    return {
        "overallStatistics": overall_stats,
        "priceLevelStatistics": price_stats,
        "pricePerAreaStatistics": price_per_area_stats,
        "statisticsByBuilding": building_stats,
        "statisticsByDong": location_stats,
        "notes": "PPA (Price Per Pyeong) is calculated based on the 'gross floor area'. ë‹¨ë…/ë‹¤ê°€êµ¬ëŠ” ì—°ë©´ì (totalFloorAr) ê¸°ì¤€ìœ¼ë¡œ ì§‘ê³„í•©ë‹ˆë‹¤."
    }

@mcp.tool(
    name="analyze_single_detached_house_trade",
    description="""ë‹¨ë…/ë‹¤ê°€êµ¬ ë§¤ë§¤ ì‹¤ê±°ë˜ ë°ì´í„° íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ì›”ê°„ ë¦¬í¬íŠ¸ í˜•ì‹ì˜ í•µì‹¬ í†µê³„ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.\nì´ ë„êµ¬ëŠ” `get_single_detached_house_trade_data`ë¥¼ í†µí•´ ì–»ì€ ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì…ë ¥ë°›ì•„ ì‘ë™í•©ë‹ˆë‹¤.\nì¢…í•© í†µê³„, ê°€ê²© ìˆ˜ì¤€, í‰ë‹¹ê°€, ê±´ë¬¼ëª…ë³„, ë™ë³„ ë“± ë‹¤ê°ì ì¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\në¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì£¼ìš” í†µê³„ ì§€í‘œë“¤ì„ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ì°¨íŠ¸ë‚˜ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n\nArguments:\n- file_path (str, required): `get_single_detached_house_trade_data` ë„êµ¬ë¡œ ìƒì„±ëœ `raw.data.json` ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œ.\n\nReturns:\n- í†µê³„ ë¶„ì„ ê²°ê³¼ê°€ ë‹´ê¸´ ìƒì„¸í•œ JSON ë¬¸ìì—´.""",
    tags={"ë‹¨ë…ë‹¤ê°€êµ¬", "í†µê³„", "ë¶„ì„", "ë¦¬í¬íŠ¸", "ë§¤ë§¤", "ì‹¤ê±°ë˜ê°€"}
)
def analyze_single_detached_house_trade(
    file_path: Annotated[str, Field(description="ë¶„ì„í•  raw.data.json íŒŒì¼ ê²½ë¡œ")],
    ctx: Optional[Any] = None
) -> TextContent:
    def call(context):
        try:
            p = Path(file_path)
            if not p.exists():
                return json.dumps({"error": f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}"}, ensure_ascii=False)
            cache_path = get_summary_cache_path(p, property_type="single_detached", trade_type="trade")
            if cache_path.exists():
                if cache_path.stat().st_mtime > p.stat().st_mtime:
                    logger.info(f"âœ… ìœ íš¨í•œ ë‹¨ë…/ë‹¤ê°€êµ¬ ë§¤ë§¤ ìºì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {cache_path}")
                    with open(cache_path, 'r', encoding='utf-8') as f:
                        return f.read()
            logger.info(f"ğŸ”„ ìƒˆë¡œìš´ ë‹¨ë…/ë‹¤ê°€êµ¬ ë§¤ë§¤ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤: {file_path}")
            df = pd.read_json(p, lines=True)
            summary_data = analyze_single_detached_trade_data(df)
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=4, default=default_serializer)
            summary_data["summary_cached_path"] = str(cache_path)
            return json.dumps(summary_data, ensure_ascii=False, indent=4, default=default_serializer)
        except Exception as e:
            logger.error(f"ë‹¨ë…/ë‹¤ê°€êµ¬ ë§¤ë§¤ ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return json.dumps({"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}, ensure_ascii=False)
    result = with_context(ctx, "analyze_single_detached_house_trade", call)
    return TextContent(type="text", text=result)

# --- ë‹¨ë…/ë‹¤ê°€êµ¬ ì „ì›”ì„¸ ë¶„ì„ ---
def analyze_single_detached_rent_data(df: pd.DataFrame) -> Dict[str, Any]:
    """DataFrameì„ ë°›ì•„ ë‹¨ë…/ë‹¤ê°€êµ¬ ì „ì›”ì„¸ í†µê³„ë¥¼ ë¶„ì„í•˜ê³  ì „ì„¸/ì›”ì„¸ë¥¼ êµ¬ë¶„í•˜ì—¬ JSONìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if df.empty:
        return {"error": "No data to analyze."}
    df['ë³´ì¦ê¸ˆ_num'] = to_numeric(get_col_from_df(df, 'ë³´ì¦ê¸ˆì•¡', 'ë³´ì¦ê¸ˆ', 'deposit', 'depositNum'))
    df['ì›”ì„¸_num'] = to_numeric(get_col_from_df(df, 'ì›”ì„¸ì•¡', 'ì›”ì„¸', 'monthlyRent', 'rentFeeNum'))
    df['ê³„ì•½ë©´ì _num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ë©´ì ', 'contractArea', 'areaNum'))
    df['ê±´ì¶•ë…„ë„_num'] = to_numeric(get_col_from_df(df, 'ê±´ì¶•ë…„ë„', 'buildYear', 'buildYearNum'))
    df['ê³„ì•½ë…„ì›”_num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ë…„ì›”'))
    df['ê³„ì•½ì¼_num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ì¼'))
    df.dropna(subset=['ë³´ì¦ê¸ˆ_num', 'ì›”ì„¸_num', 'ê³„ì•½ë©´ì _num'], inplace=True)
    df = df[df['ê³„ì•½ë©´ì _num'] > 0].copy()
    if df.empty:
        return {"error": "No valid transaction data after cleaning."}
    df_jeonse = df[df['ì›”ì„¸_num'] == 0].copy()
    df_wolse = df[df['ì›”ì„¸_num'] > 0].copy()
    def as_value_unit_m(v): return as_value_unit(v, "ë§Œì›")
    def analyze_rent_type(df_rent_type, rent_type_name):
        if df_rent_type.empty:
            return { "totalTransactionCount": 0 }
        stats = { "totalTransactionCount": len(df_rent_type) }
        price_stats_raw = df_rent_type['ë³´ì¦ê¸ˆ_num'].agg(['mean', 'median', 'max', 'min'])
        stats['depositPriceStatistics'] = {
            "averageDeposit": as_value_unit_m(price_stats_raw['mean']),
            "medianDeposit": as_value_unit_m(price_stats_raw['median']),
            "highestDeposit": as_value_unit_m(price_stats_raw['max']),
            "lowestDeposit": as_value_unit_m(price_stats_raw['min']),
            "representativeDeals": {
                "highestDepositDeal": clean_deal_for_display(df_rent_type.loc[df_rent_type['ë³´ì¦ê¸ˆ_num'].idxmax()]),
                "lowestDepositDeal": clean_deal_for_display(df_rent_type.loc[df_rent_type['ë³´ì¦ê¸ˆ_num'].idxmin()]),
            }
        }
        if rent_type_name == 'wolse':
            wolse_stats_raw = df_rent_type['ì›”ì„¸_num'].agg(['mean', 'median', 'max', 'min'])
            stats['monthlyRentStatistics'] = {
                "averageMonthlyRent": as_value_unit_m(wolse_stats_raw['mean']),
                "medianMonthlyRent": as_value_unit_m(wolse_stats_raw['median']),
            }
        building_col = get_col_from_df(df_rent_type, 'ê±´ë¬¼ëª…', 'buildingName')
        if building_col.notna().any():
            stats['statisticsByBuilding'] = df_rent_type.groupby(building_col).agg(
                transactionCount=('ë³´ì¦ê¸ˆ_num', 'size'),
                averageDeposit=('ë³´ì¦ê¸ˆ_num', 'mean')
            ).apply(lambda x: x.astype(int) if x.name == 'transactionCount' else as_value_unit_m(x)).to_dict('index')
        return stats
    jeonse_analysis = analyze_rent_type(df_jeonse, 'jeonse')
    wolse_analysis = analyze_rent_type(df_wolse, 'wolse')
    return {
        "transactionTypeDistribution": {
            "jeonse_count": len(df_jeonse),
            "wolse_count": len(df_wolse),
        },
        "jeonseAnalysis": jeonse_analysis,
        "wolseAnalysis": wolse_analysis,
        "notes": "Statistics are separated by transaction type (Jeonse vs. Wolse). ê³„ì•½ë©´ì  ê¸°ì¤€ìœ¼ë¡œ ì§‘ê³„í•©ë‹ˆë‹¤."
    }

@mcp.tool(
    name="analyze_single_detached_house_rent",
    description="""ë‹¨ë…/ë‹¤ê°€êµ¬ ì „ì›”ì„¸ ì‹¤ê±°ë˜ ë°ì´í„° íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ì›”ê°„ ë¦¬í¬íŠ¸ í˜•ì‹ì˜ í•µì‹¬ í†µê³„ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.\nì´ ë„êµ¬ëŠ” `get_sh_rent_data`ë¥¼ í†µí•´ ì–»ì€ ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì…ë ¥ë°›ì•„ ì‘ë™í•˜ë©°, 'ì „ì„¸'ì™€ 'ì›”ì„¸'ë¥¼ ìë™ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ê°ê°ì— ëŒ€í•œ ìƒì„¸ í†µê³„ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\në¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì£¼ìš” í†µê³„ ì§€í‘œë“¤ì„ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ì°¨íŠ¸ë‚˜ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n\n- **ê±°ë˜ ìœ í˜• ë¶„í¬**: ì „ì²´ ê±°ë˜ ì¤‘ ì „ì„¸ì™€ ì›”ì„¸ì˜ ë¹„ì¤‘ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n- **ì „ì„¸ ë¶„ì„**: ë³´ì¦ê¸ˆì— ëŒ€í•œ í‰ê· /ì¤‘ìœ„/ìµœê³ /ìµœì € ê°€ê²© ë° ì£¼ìš” ê±°ë˜ ì‚¬ë¡€ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ê±´ë¬¼ëª…ë³„ í†µê³„ë„ í¬í•¨ë©ë‹ˆë‹¤.\n- **ì›”ì„¸ ë¶„ì„**: ë³´ì¦ê¸ˆ ë° ì›”ì„¸ ê°ê°ì— ëŒ€í•œ í‰ê· /ì¤‘ìœ„ ê°€ê²© í†µê³„ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ê±´ë¬¼ëª…ë³„ í†µê³„ë„ í¬í•¨ë©ë‹ˆë‹¤.\n\nArguments:\n- file_path (str, required): `get_sh_rent_data` ë„êµ¬ë¡œ ìƒì„±ëœ `raw.data.json` ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œ.\n\nReturns:\n- ì „ì„¸ì™€ ì›”ì„¸ë¡œ êµ¬ë¶„ëœ í†µê³„ ë¶„ì„ ê²°ê³¼ê°€ ë‹´ê¸´ ìƒì„¸í•œ JSON ë¬¸ìì—´.""",
    tags={"ë‹¨ë…ë‹¤ê°€êµ¬", "í†µê³„", "ë¶„ì„", "ë¦¬í¬íŠ¸", "ì „ì„¸", "ì›”ì„¸", "ì „ì›”ì„¸", "ì‹¤ê±°ë˜ê°€"}
)
def analyze_single_detached_house_rent(
    file_path: Annotated[str, Field(description="ë¶„ì„í•  raw.data.json íŒŒì¼ ê²½ë¡œ")],
    ctx: Optional[Any] = None
) -> TextContent:
    def call(context):
        try:
            p = Path(file_path)
            if not p.exists():
                return json.dumps({"error": f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}"}, ensure_ascii=False)
            cache_path = get_summary_cache_path(p, property_type="single_detached", trade_type="rent")
            if cache_path.exists():
                if cache_path.stat().st_mtime > p.stat().st_mtime:
                    logger.info(f"âœ… ìœ íš¨í•œ ë‹¨ë…/ë‹¤ê°€êµ¬ ì „ì›”ì„¸ ìºì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {cache_path}")
                    with open(cache_path, 'r', encoding='utf-8') as f:
                        return f.read()
            logger.info(f"ğŸ”„ ìƒˆë¡œìš´ ë‹¨ë…/ë‹¤ê°€êµ¬ ì „ì›”ì„¸ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤: {file_path}")
            df = pd.read_json(p, lines=True)
            summary_data = analyze_single_detached_rent_data(df)
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=4, default=default_serializer)
            summary_data["summary_cached_path"] = str(cache_path)
            return json.dumps(summary_data, ensure_ascii=False, indent=4, default=default_serializer)
        except Exception as e:
            logger.error(f"ë‹¨ë…/ë‹¤ê°€êµ¬ ì „ì›”ì„¸ ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return json.dumps({"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}, ensure_ascii=False)
    result = with_context(ctx, "analyze_single_detached_house_rent", call)
    return TextContent(type="text", text=result)

def analyze_row_house_trade_data(df: pd.DataFrame) -> Dict[str, Any]:
    """DataFrameì„ ë°›ì•„ ì—°ë¦½ë‹¤ì„¸ëŒ€ ë§¤ë§¤ í†µê³„ë¥¼ ë¶„ì„í•˜ê³  ì˜ë¬¸ keyì™€ ë‹¨ìœ„ê°€ í¬í•¨ëœ ê°’ìœ¼ë¡œ JSONì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if df.empty:
        return {"error": "No data to analyze."}
    # --- ë°ì´í„° ì „ì²˜ë¦¬ ---
    df['ê±°ë˜ê¸ˆì•¡_num'] = to_numeric(get_col_from_df(df, 'ê±°ë˜ê¸ˆì•¡', 'dealAmount', 'dealAmountNum'))
    df['ì „ìš©ë©´ì _num'] = to_numeric(get_col_from_df(df, 'ì „ìš©ë©´ì ', 'area', 'excluUseAr', 'areaNum'))
    df['ê±´ì¶•ë…„ë„_num'] = to_numeric(get_col_from_df(df, 'ê±´ì¶•ë…„ë„', 'buildYear', 'buildYearNum'))
    df['ê³„ì•½ë…„ì›”_num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ë…„ì›”'))
    df['ê³„ì•½ì¼_num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ì¼'))
    df.dropna(subset=['ê±°ë˜ê¸ˆì•¡_num', 'ì „ìš©ë©´ì _num'], inplace=True)
    df = df[df['ì „ìš©ë©´ì _num'] > 0].copy()
    if df.empty:
        return {"error": "No valid transaction data after cleaning."}
    df['í‰ë‹¹ê°€_ë§Œì›'] = (df['ê±°ë˜ê¸ˆì•¡_num'] / df['ì „ìš©ë©´ì _num']) * 3.305785
    current_year = datetime.now().year
    df['ê±´ë¬¼ì—°ë ¹'] = current_year - df['ê±´ì¶•ë…„ë„_num']
    def as_value_unit_m(v): return as_value_unit(v, "ë§Œì›")
    def as_value_unit_per_pyeong(v): return as_value_unit(v, "ë§Œì›/í‰")
    # --- 1. ì¢…í•© í†µê³„ ---
    total_count = len(df)
    total_value = df['ê±°ë˜ê¸ˆì•¡_num'].sum()
    overall_stats = {
        "totalTransactionCount": total_count,
        "totalTransactionValue": as_value_unit_m(total_value),
    }
    # --- 2. ê°€ê²© ìˆ˜ì¤€ í†µê³„ ---
    price_stats_raw = df['ê±°ë˜ê¸ˆì•¡_num'].agg(['mean', 'median', 'max', 'min'])
    price_stats = {
        "overallAveragePrice": as_value_unit_m(price_stats_raw['mean']),
        "overallMedianPrice": as_value_unit_m(price_stats_raw['median']),
        "overallHighestPrice": as_value_unit_m(price_stats_raw['max']),
        "overallLowestPrice": as_value_unit_m(price_stats_raw['min']),
        "representativeDeals": {
            "highestPriceDeal": clean_deal_for_display(df.loc[df['ê±°ë˜ê¸ˆì•¡_num'].idxmax()]),
            "lowestPriceDeal": clean_deal_for_display(df.loc[df['ê±°ë˜ê¸ˆì•¡_num'].idxmin()]),
            "dealClosestToAverage": clean_deal_for_display(df.loc[(df['ê±°ë˜ê¸ˆì•¡_num'] - price_stats_raw['mean']).abs().idxmin()]),
            "dealClosestToMedian": clean_deal_for_display(df.loc[(df['ê±°ë˜ê¸ˆì•¡_num'] - price_stats_raw['median']).abs().idxmin()])
        }
    }
    # --- 3. ë‹¨ìœ„ ë©´ì ë‹¹ ê°€ê²© í†µê³„ ---
    price_per_area_stats = {
        "overallAveragePricePerPyeong": as_value_unit_per_pyeong(df['í‰ë‹¹ê°€_ë§Œì›'].mean()),
        "overallMedianPricePerPyeong": as_value_unit_per_pyeong(df['í‰ë‹¹ê°€_ë§Œì›'].median()),
    }
    # --- 4. ì—°ë¦½ë‹¤ì„¸ëŒ€ëª…/ë™ë³„ í†µê³„ ---
    building_col = get_col_from_df(df, 'ì—°ë¦½ë‹¤ì„¸ëŒ€ëª…', 'rowHouseName')
    location_col = get_col_from_df(df, 'ë²•ì •ë™', 'umdNm', 'dong')
    def get_grouped_stats(group_col):
        stats = {}
        if group_col.notna().any():
            summary_raw = df.groupby(group_col).agg(
                Count=('ê±°ë˜ê¸ˆì•¡_num', 'size'),
                Mean_Price=('ê±°ë˜ê¸ˆì•¡_num', 'mean'),
                Median_Price=('ê±°ë˜ê¸ˆì•¡_num', 'median'),
                Mean_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'mean'),
                Median_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'median')
            )
            for name, data in summary_raw.to_dict('index').items():
                stats[name] = {
                    "transactionCount": int(data['Count']),
                    "averagePrice": as_value_unit_m(data['Mean_Price']),
                    "medianPrice": as_value_unit_m(data['Median_Price']),
                    "averagePricePerPyeong": as_value_unit_per_pyeong(data['Mean_PPA']),
                    "medianPricePerPyeong": as_value_unit_per_pyeong(data['Median_PPA']),
                }
        return stats
    building_stats = get_grouped_stats(building_col)
    location_stats = get_grouped_stats(location_col)
    return {
        "overallStatistics": overall_stats,
        "priceLevelStatistics": price_stats,
        "pricePerAreaStatistics": price_per_area_stats,
        "statisticsByRowHouseComplex": building_stats,
        "statisticsByDong": location_stats,
        "notes": "PPA (Price Per Pyeong) is calculated based on the 'exclusive use area'. ì—°ë¦½ë‹¤ì„¸ëŒ€ëŠ” ë‹¨ì§€ëª…(ì—°ë¦½ë‹¤ì„¸ëŒ€ëª…) ê¸°ì¤€ ì§‘ê³„ê°€ ê°€ëŠ¥í•˜ë©°, ë‹¨ì§€ëª…ì´ ì—†ìœ¼ë©´ ë™ë³„ë¡œ ì§‘ê³„í•©ë‹ˆë‹¤."
    }

@mcp.tool(
    name="analyze_row_house_trade",
    description="""ì—°ë¦½ë‹¤ì„¸ëŒ€ ë§¤ë§¤ ì‹¤ê±°ë˜ ë°ì´í„° íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ì›”ê°„ ë¦¬í¬íŠ¸ í˜•ì‹ì˜ í•µì‹¬ í†µê³„ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.\nì´ ë„êµ¬ëŠ” `get_row_house_trade_data`ë¥¼ í†µí•´ ì–»ì€ ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì…ë ¥ë°›ì•„ ì‘ë™í•©ë‹ˆë‹¤.\nì¢…í•© í†µê³„, ê°€ê²© ìˆ˜ì¤€, í‰ë‹¹ê°€, ë‹¨ì§€ë³„, ë™ë³„ ë“± ë‹¤ê°ì ì¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\në¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì£¼ìš” í†µê³„ ì§€í‘œë“¤ì„ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ì°¨íŠ¸ë‚˜ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n\nArguments:\n- file_path (str, required): `get_row_house_trade_data` ë„êµ¬ë¡œ ìƒì„±ëœ `raw.data.json` ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œ.\n\nReturns:\n- í†µê³„ ë¶„ì„ ê²°ê³¼ê°€ ë‹´ê¸´ ìƒì„¸í•œ JSON ë¬¸ìì—´.""",
    tags={"ì—°ë¦½ë‹¤ì„¸ëŒ€", "í†µê³„", "ë¶„ì„", "ë¦¬í¬íŠ¸", "ë§¤ë§¤", "ì‹¤ê±°ë˜ê°€"}
)
def analyze_row_house_trade(
    file_path: Annotated[str, Field(description="ë¶„ì„í•  raw.data.json íŒŒì¼ ê²½ë¡œ")],
    ctx: Optional[Any] = None
) -> TextContent:
    def call(context):
        try:
            p = Path(file_path)
            if not p.exists():
                return json.dumps({"error": f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}"}, ensure_ascii=False)
            cache_path = get_summary_cache_path(p, property_type="row_house", trade_type="trade")
            if cache_path.exists():
                if cache_path.stat().st_mtime > p.stat().st_mtime:
                    logger.info(f"âœ… ìœ íš¨í•œ ì—°ë¦½ë‹¤ì„¸ëŒ€ ë§¤ë§¤ ìºì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {cache_path}")
                    with open(cache_path, 'r', encoding='utf-8') as f:
                        return f.read()
            logger.info(f"ğŸ”„ ìƒˆë¡œìš´ ì—°ë¦½ë‹¤ì„¸ëŒ€ ë§¤ë§¤ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤: {file_path}")
            df = pd.read_json(p, lines=True)
            summary_data = analyze_row_house_trade_data(df)
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=4, default=default_serializer)
            summary_data["summary_cached_path"] = str(cache_path)
            return json.dumps(summary_data, ensure_ascii=False, indent=4, default=default_serializer)
        except Exception as e:
            logger.error(f"ì—°ë¦½ë‹¤ì„¸ëŒ€ ë§¤ë§¤ ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return json.dumps({"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}, ensure_ascii=False)
    result = with_context(ctx, "analyze_row_house_trade", call)
    return TextContent(type="text", text=result)

def analyze_row_house_rent_data(df: pd.DataFrame) -> Dict[str, Any]:
    """DataFrameì„ ë°›ì•„ ì—°ë¦½ë‹¤ì„¸ëŒ€ ì „ì›”ì„¸ í†µê³„ë¥¼ ë¶„ì„í•˜ê³  ì „ì„¸/ì›”ì„¸ë¥¼ êµ¬ë¶„í•˜ì—¬ JSONìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if df.empty:
        return {"error": "No data to analyze."}
    df['ë³´ì¦ê¸ˆ_num'] = to_numeric(get_col_from_df(df, 'ë³´ì¦ê¸ˆì•¡', 'ë³´ì¦ê¸ˆ', 'deposit', 'depositNum'))
    df['ì›”ì„¸_num'] = to_numeric(get_col_from_df(df, 'ì›”ì„¸ì•¡', 'ì›”ì„¸', 'monthlyRent', 'rentFeeNum'))
    # Fix: Use excluUseAr as fallback for area, before areaNum
    df['ê³„ì•½ë©´ì _num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ë©´ì ', 'contractArea', 'excluUseAr', 'areaNum'))
    df['ê±´ì¶•ë…„ë„_num'] = to_numeric(get_col_from_df(df, 'ê±´ì¶•ë…„ë„', 'buildYear', 'buildYearNum'))
    df['ê³„ì•½ë…„ì›”_num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ë…„ì›”'))
    df['ê³„ì•½ì¼_num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ì¼'))
    df.dropna(subset=['ë³´ì¦ê¸ˆ_num', 'ì›”ì„¸_num', 'ê³„ì•½ë©´ì _num'], inplace=True)
    df = df[df['ê³„ì•½ë©´ì _num'] > 0].copy()
    if df.empty:
        return {"error": "No valid transaction data after cleaning."}
    df_jeonse = df[df['ì›”ì„¸_num'] == 0].copy()
    df_wolse = df[df['ì›”ì„¸_num'] > 0].copy()
    def as_value_unit_m(v): return as_value_unit(v, "ë§Œì›")
    def analyze_rent_type(df_rent_type, rent_type_name):
        if df_rent_type.empty:
            return { "totalTransactionCount": 0 }
        stats = { "totalTransactionCount": len(df_rent_type) }
        price_stats_raw = df_rent_type['ë³´ì¦ê¸ˆ_num'].agg(['mean', 'median', 'max', 'min'])
        stats['depositPriceStatistics'] = {
            "averageDeposit": as_value_unit_m(price_stats_raw['mean']),
            "medianDeposit": as_value_unit_m(price_stats_raw['median']),
            "highestDeposit": as_value_unit_m(price_stats_raw['max']),
            "lowestDeposit": as_value_unit_m(price_stats_raw['min']),
            "representativeDeals": {
                "highestDepositDeal": clean_deal_for_display(df_rent_type.loc[df_rent_type['ë³´ì¦ê¸ˆ_num'].idxmax()]),
                "lowestDepositDeal": clean_deal_for_display(df_rent_type.loc[df_rent_type['ë³´ì¦ê¸ˆ_num'].idxmin()]),
            }
        }
        if rent_type_name == 'wolse':
            wolse_stats_raw = df_rent_type['ì›”ì„¸_num'].agg(['mean', 'median', 'max', 'min'])
            stats['monthlyRentStatistics'] = {
                "averageMonthlyRent": as_value_unit_m(wolse_stats_raw['mean']),
                "medianMonthlyRent": as_value_unit_m(wolse_stats_raw['median']),
            }
        building_col = get_col_from_df(df_rent_type, 'ì—°ë¦½ë‹¤ì„¸ëŒ€ëª…', 'rowHouseName')
        if building_col.notna().any():
            stats['statisticsByRowHouseComplex'] = df_rent_type.groupby(building_col).agg(
                transactionCount=('ë³´ì¦ê¸ˆ_num', 'size'),
                averageDeposit=('ë³´ì¦ê¸ˆ_num', 'mean')
            ).apply(lambda x: x.astype(int) if x.name == 'transactionCount' else as_value_unit_m(x)).to_dict('index')
        return stats
    jeonse_analysis = analyze_rent_type(df_jeonse, 'jeonse')
    wolse_analysis = analyze_rent_type(df_wolse, 'wolse')
    return {
        "transactionTypeDistribution": {
            "jeonse_count": len(df_jeonse),
            "wolse_count": len(df_wolse),
        },
        "jeonseAnalysis": jeonse_analysis,
        "wolseAnalysis": wolse_analysis,
        "notes": "Statistics are separated by transaction type (Jeonse vs. Wolse). ê³„ì•½ë©´ì  ê¸°ì¤€ìœ¼ë¡œ ì§‘ê³„í•©ë‹ˆë‹¤. (excluUseArë„ ìë™ í™œìš©)"
    }

@mcp.tool(
    name="analyze_row_house_rent",
    description="""ì—°ë¦½ë‹¤ì„¸ëŒ€ ì „ì›”ì„¸ ì‹¤ê±°ë˜ ë°ì´í„° íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ì›”ê°„ ë¦¬í¬íŠ¸ í˜•ì‹ì˜ í•µì‹¬ í†µê³„ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.\nì´ ë„êµ¬ëŠ” `get_row_house_rent_data`ë¥¼ í†µí•´ ì–»ì€ ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì…ë ¥ë°›ì•„ ì‘ë™í•˜ë©°, 'ì „ì„¸'ì™€ 'ì›”ì„¸'ë¥¼ ìë™ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ê°ê°ì— ëŒ€í•œ ìƒì„¸ í†µê³„ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\në¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì£¼ìš” í†µê³„ ì§€í‘œë“¤ì„ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ì°¨íŠ¸ë‚˜ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n\n- **ê±°ë˜ ìœ í˜• ë¶„í¬**: ì „ì²´ ê±°ë˜ ì¤‘ ì „ì„¸ì™€ ì›”ì„¸ì˜ ë¹„ì¤‘ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n- **ì „ì„¸ ë¶„ì„**: ë³´ì¦ê¸ˆì— ëŒ€í•œ í‰ê· /ì¤‘ìœ„/ìµœê³ /ìµœì € ê°€ê²© ë° ì£¼ìš” ê±°ë˜ ì‚¬ë¡€ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ê±´ë¬¼ëª…ë³„ í†µê³„ë„ í¬í•¨ë©ë‹ˆë‹¤.\n- **ì›”ì„¸ ë¶„ì„**: ë³´ì¦ê¸ˆ ë° ì›”ì„¸ ê°ê°ì— ëŒ€í•œ í‰ê· /ì¤‘ìœ„ ê°€ê²© í†µê³„ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ê±´ë¬¼ëª…ë³„ í†µê³„ë„ í¬í•¨ë©ë‹ˆë‹¤.\n\nArguments:\n- file_path (str, required): `get_row_house_rent_data` ë„êµ¬ë¡œ ìƒì„±ëœ `raw.data.json` ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œ.\n\nReturns:\n- ì „ì„¸ì™€ ì›”ì„¸ë¡œ êµ¬ë¶„ëœ í†µê³„ ë¶„ì„ ê²°ê³¼ê°€ ë‹´ê¸´ ìƒì„¸í•œ JSON ë¬¸ìì—´.""",
    tags={"ì—°ë¦½ë‹¤ì„¸ëŒ€", "í†µê³„", "ë¶„ì„", "ë¦¬í¬íŠ¸", "ì „ì„¸", "ì›”ì„¸", "ì „ì›”ì„¸", "ì‹¤ê±°ë˜ê°€"}
)
def analyze_row_house_rent(
    file_path: Annotated[str, Field(description="ë¶„ì„í•  raw.data.json íŒŒì¼ ê²½ë¡œ")],
    ctx: Optional[Any] = None
) -> TextContent:
    def call(context):
        try:
            p = Path(file_path)
            if not p.exists():
                return json.dumps({"error": f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}"}, ensure_ascii=False)
            cache_path = get_summary_cache_path(p, property_type="row_house", trade_type="rent")
            if cache_path.exists():
                if cache_path.stat().st_mtime > p.stat().st_mtime:
                    logger.info(f"âœ… ìœ íš¨í•œ ì—°ë¦½ë‹¤ì„¸ëŒ€ ì „ì›”ì„¸ ìºì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {cache_path}")
                    with open(cache_path, 'r', encoding='utf-8') as f:
                        return f.read()
            logger.info(f"ğŸ”„ ìƒˆë¡œìš´ ì—°ë¦½ë‹¤ì„¸ëŒ€ ì „ì›”ì„¸ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤: {file_path}")
            df = pd.read_json(p, lines=True)
            summary_data = analyze_row_house_rent_data(df)
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=4, default=default_serializer)
            summary_data["summary_cached_path"] = str(cache_path)
            return json.dumps(summary_data, ensure_ascii=False, indent=4, default=default_serializer)
        except Exception as e:
            logger.error(f"ì—°ë¦½ë‹¤ì„¸ëŒ€ ì „ì›”ì„¸ ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return json.dumps({"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}, ensure_ascii=False)
    result = with_context(ctx, "analyze_row_house_rent", call)
    return TextContent(type="text", text=result)

def analyze_industrial_property_data(df: pd.DataFrame) -> Dict[str, Any]:
    """DataFrameì„ ë°›ì•„ ê³µì¥/ì°½ê³  ë“± ì‚°ì—…ìš© ë¶€ë™ì‚° í†µê³„ë¥¼ ë¶„ì„í•˜ê³  ì˜ë¬¸ keyì™€ ë‹¨ìœ„ê°€ í¬í•¨ëœ ê°’ìœ¼ë¡œ JSONì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if df.empty:
        return {"error": "No data to analyze."}
    # --- ë°ì´í„° ì „ì²˜ë¦¬ ---
    df['ê±°ë˜ê¸ˆì•¡_num'] = to_numeric(get_col_from_df(df, 'ê±°ë˜ê¸ˆì•¡', 'dealAmount', 'dealAmountNum'))
    # Fix: include 'buildingAr' as a fallback for area
    df['ì „ìš©ë©´ì _num'] = to_numeric(get_col_from_df(df, 'ì „ìš©ë©´ì ', 'area', 'excluUseAr', 'buildingAr', 'areaNum'))
    df['ê±´ì¶•ë…„ë„_num'] = to_numeric(get_col_from_df(df, 'ê±´ì¶•ë…„ë„', 'buildYear', 'buildYearNum'))
    df.dropna(subset=['ê±°ë˜ê¸ˆì•¡_num', 'ì „ìš©ë©´ì _num'], inplace=True)
    df = df[df['ì „ìš©ë©´ì _num'] > 0].copy()
    if df.empty:
        return {"error": "No valid transaction data after cleaning."}
    df['í‰ë‹¹ê°€_ë§Œì›'] = (df['ê±°ë˜ê¸ˆì•¡_num'] / df['ì „ìš©ë©´ì _num']) * 3.305785
    current_year = datetime.now().year
    df['ê±´ë¬¼ì—°ë ¹'] = current_year - df['ê±´ì¶•ë…„ë„_num']
    def as_value_unit_m(v): return as_value_unit(v, "ë§Œì›")
    def as_value_unit_per_pyeong(v): return as_value_unit(v, "ë§Œì›/í‰")
    # --- 1. ì¢…í•© í†µê³„ ---
    total_count = len(df)
    total_value = df['ê±°ë˜ê¸ˆì•¡_num'].sum()
    use_col = get_col_from_df(df, 'ìš©ë„', 'ìœ í˜•', 'buildingUse')
    use_distribution = use_col.value_counts().to_dict() if use_col.notna().any() else {}
    overall_stats = {
        "totalTransactionCount": total_count,
        "totalTransactionValue": as_value_unit_m(total_value),
        "transactionDistributionByUseType": use_distribution
    }
    # --- 2. ê°€ê²© ìˆ˜ì¤€ í†µê³„ ---
    price_stats_raw = df['ê±°ë˜ê¸ˆì•¡_num'].agg(['mean', 'median', 'max', 'min'])
    price_stats = {
        "overallAveragePrice": as_value_unit_m(price_stats_raw['mean']),
        "overallMedianPrice": as_value_unit_m(price_stats_raw['median']),
        "overallHighestPrice": as_value_unit_m(price_stats_raw['max']),
        "overallLowestPrice": as_value_unit_m(price_stats_raw['min']),
        "representativeDeals": {
            "highestPriceDeal": clean_deal_for_display(df.loc[df['ê±°ë˜ê¸ˆì•¡_num'].idxmax()]),
            "lowestPriceDeal": clean_deal_for_display(df.loc[df['ê±°ë˜ê¸ˆì•¡_num'].idxmin()]),
            "dealClosestToAverage": clean_deal_for_display(df.loc[(df['ê±°ë˜ê¸ˆì•¡_num'] - price_stats_raw['mean']).abs().idxmin()]),
            "dealClosestToMedian": clean_deal_for_display(df.loc[(df['ê±°ë˜ê¸ˆì•¡_num'] - price_stats_raw['median']).abs().idxmin()])
        }
    }
    # --- 3. ë‹¨ìœ„ ë©´ì ë‹¹ ê°€ê²© í†µê³„ ---
    price_per_area_stats = {
        "overallAveragePricePerPyeong": as_value_unit_per_pyeong(df['í‰ë‹¹ê°€_ë§Œì›'].mean()),
        "overallMedianPricePerPyeong": as_value_unit_per_pyeong(df['í‰ë‹¹ê°€_ë§Œì›'].median()),
    }
    if use_col.notna().any():
        price_by_use_raw = df.groupby(use_col)['ê±°ë˜ê¸ˆì•¡_num'].agg(['mean', 'median', 'max', 'min'])
        price_stats["priceStatisticsByUseType"] = {
            use: {
                "averagePrice": as_value_unit_m(stats['mean']),
                "medianPrice": as_value_unit_m(stats['median']),
                "highestPrice": as_value_unit_m(stats['max']),
                "lowestPrice": as_value_unit_m(stats['min']),
            } for use, stats in price_by_use_raw.to_dict('index').items()
        }
        price_per_area_by_use_raw = df.groupby(use_col)['í‰ë‹¹ê°€_ë§Œì›'].agg(['mean', 'median'])
        price_per_area_stats["pricePerPyeongStatisticsByUseType"] = {
            use: {
                "averagePricePerPyeong": as_value_unit_per_pyeong(stats['mean']),
                "medianPricePerPyeong": as_value_unit_per_pyeong(stats['median']),
            } for use, stats in price_per_area_by_use_raw.to_dict('index').items()
        }
    # --- 4. ì…ì§€ë³„ í†µê³„ (ë™ë³„) ---
    location_col = get_col_from_df(df, 'ë²•ì •ë™', 'umdNm', 'dong')
    location_stats = {}
    if location_col.notna().any():
        location_summary_raw = df.groupby(location_col).agg(
            Count=('ê±°ë˜ê¸ˆì•¡_num', 'size'),
            Mean_Price=('ê±°ë˜ê¸ˆì•¡_num', 'mean'),
            Max_Price=('ê±°ë˜ê¸ˆì•¡_num', 'max'),
            Min_Price=('ê±°ë˜ê¸ˆì•¡_num', 'min'),
            Mean_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'mean'),
            Median_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'median')
        )
        for dong, stats in location_summary_raw.to_dict('index').items():
            location_stats[dong] = {
                "transactionCount": int(stats['Count']),
                "averagePrice": as_value_unit_m(stats['Mean_Price']),
                "highestPrice": as_value_unit_m(stats['Max_Price']),
                "lowestPrice": as_value_unit_m(stats['Min_Price']),
                "averagePricePerPyeong": as_value_unit_per_pyeong(stats['Mean_PPA']),
                "medianPricePerPyeong": as_value_unit_per_pyeong(stats['Median_PPA']),
            }
    # --- 5. ê±´ë¬¼ íŠ¹ì„±ë³„ í†µê³„ (ì—°ë ¹/ê·œëª¨) ---
    age_bins = [0, 6, 11, 21, np.inf]
    age_labels = ['5 years or newer', '6-10 years', '11-20 years', 'over 20 years']
    df['ê±´ë¬¼ì—°ë ¹ëŒ€'] = pd.cut(df['ê±´ë¬¼ì—°ë ¹'], bins=age_bins, labels=age_labels, right=False)
    age_stats = {}
    if not df['ê±´ë¬¼ì—°ë ¹ëŒ€'].isnull().all():
        age_summary_raw = df.groupby('ê±´ë¬¼ì—°ë ¹ëŒ€', observed=True).agg(
            Count=('ê±°ë˜ê¸ˆì•¡_num', 'size'),
            Mean_Price=('ê±°ë˜ê¸ˆì•¡_num', 'mean'),
            Mean_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'mean')
        )
        age_stats = {
            age_group: {
                "transactionCount": int(stats['Count']),
                "averagePrice": as_value_unit_m(stats['Mean_Price']),
                "averagePricePerPyeong": as_value_unit_per_pyeong(stats['Mean_PPA']),
            } for age_group, stats in age_summary_raw.to_dict('index').items()
        }
    area_bins = [0, 100, 300, 1000, np.inf]
    area_labels = ['small (<100mÂ²)', 'medium (100-300mÂ²)', 'large (300-1000mÂ²)', 'extra_large (>1000mÂ²)']
    df['ê±´ë¬¼ê·œëª¨'] = pd.cut(df['ì „ìš©ë©´ì _num'], bins=area_bins, labels=area_labels, right=False)
    scale_stats = {}
    if not df['ê±´ë¬¼ê·œëª¨'].isnull().all():
        scale_summary_raw = df.groupby('ê±´ë¬¼ê·œëª¨', observed=True).agg(
            Count=('ê±°ë˜ê¸ˆì•¡_num', 'size'),
            Mean_Price=('ê±°ë˜ê¸ˆì•¡_num', 'mean'),
            Mean_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'mean')
        )
        scale_stats = {
            scale_group: {
                "transactionCount": int(stats['Count']),
                "averagePrice": as_value_unit_m(stats['Mean_Price']),
                "averagePricePerPyeong": as_value_unit_per_pyeong(stats['Mean_PPA']),
            } for scale_group, stats in scale_summary_raw.to_dict('index').items()
        }
    building_stats = {
        "statisticsByBuildingAge": age_stats,
        "statisticsByBuildingScale_exclusiveArea": scale_stats
    }
    return {
        "overallStatistics": overall_stats,
        "priceLevelStatistics": price_stats,
        "pricePerAreaStatistics": price_per_area_stats,
        "statisticsByUseType": price_stats.get("priceStatisticsByUseType", {}),
        "locationStatistics_byDong": location_stats,
        "buildingCharacteristicsStatistics": building_stats,
        "notes": "ê³µì¥/ì°½ê³  ë“± ì‚°ì—…ìš© ë¶€ë™ì‚°ì€ ìš©ë„(ê³µì¥/ì°½ê³ /ê¸°íƒ€), ì „ìš©ë©´ì , ê±´ë¬¼ì—°ë ¹, ì¸µê³ , ëŒ€ì§€/ê±´ë¬¼ë©´ì  ë“± íŠ¹ì„±ì´ ì¤‘ìš”í•©ë‹ˆë‹¤. í‰ë‹¹ê°€ëŠ” ì „ìš©ë©´ì  ê¸°ì¤€ì…ë‹ˆë‹¤."
    }

@mcp.tool(
    name="analyze_industrial_property_trade",
    description="""ê³µì¥/ì°½ê³  ë“± ì‚°ì—…ìš© ë¶€ë™ì‚° ë§¤ë§¤ ì‹¤ê±°ë˜ ë°ì´í„° íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ì›”ê°„ ë¦¬í¬íŠ¸ í˜•ì‹ì˜ í•µì‹¬ í†µê³„ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.\nì´ ë„êµ¬ëŠ” `get_indu_trade_data`ë¥¼ í†µí•´ ì–»ì€ ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì…ë ¥ë°›ì•„ ì‘ë™í•©ë‹ˆë‹¤.\nì¢…í•© í†µê³„, ê°€ê²© ìˆ˜ì¤€, í‰ë‹¹ê°€, ìš©ë„ë³„, ë™ë³„, ê±´ë¬¼ íŠ¹ì„±ë³„ ë“± ë‹¤ê°ì ì¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\në¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì£¼ìš” í†µê³„ ì§€í‘œë“¤ì„ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ì°¨íŠ¸ë‚˜ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n\nArguments:\n- file_path (str, required): `get_indu_trade_data` ë„êµ¬ë¡œ ìƒì„±ëœ `raw.data.json` ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œ.\n\nReturns:\n- í†µê³„ ë¶„ì„ ê²°ê³¼ê°€ ë‹´ê¸´ ìƒì„¸í•œ JSON ë¬¸ìì—´.""",
    tags={"ê³µì¥", "ì°½ê³ ", "ì‚°ì—…ìš©", "í†µê³„", "ë¶„ì„", "ë¦¬í¬íŠ¸", "ë§¤ë§¤", "ì‹¤ê±°ë˜ê°€"}
)
def analyze_industrial_property_trade(
    file_path: Annotated[str, Field(description="ë¶„ì„í•  raw.data.json íŒŒì¼ ê²½ë¡œ")],
    ctx: Optional[Any] = None
) -> TextContent:
    def call(context):
        try:
            p = Path(file_path)
            if not p.exists():
                return json.dumps({"error": f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}"}, ensure_ascii=False)
            cache_path = get_summary_cache_path(p, property_type="industrial", trade_type=None)
            if cache_path.exists():
                if cache_path.stat().st_mtime > p.stat().st_mtime:
                    logger.info(f"âœ… ìœ íš¨í•œ ì‚°ì—…ìš© ë¶€ë™ì‚° ë§¤ë§¤ ìºì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {cache_path}")
                    with open(cache_path, 'r', encoding='utf-8') as f:
                        return f.read()
            logger.info(f"ğŸ”„ ìƒˆë¡œìš´ ì‚°ì—…ìš© ë¶€ë™ì‚° ë§¤ë§¤ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤: {file_path}")
            df = pd.read_json(p, lines=True)
            summary_data = analyze_industrial_property_data(df)
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=4, default=default_serializer)
            summary_data["summary_cached_path"] = str(cache_path)
            return json.dumps(summary_data, ensure_ascii=False, indent=4, default=default_serializer)
        except Exception as e:
            logger.error(f"ì‚°ì—…ìš© ë¶€ë™ì‚° ë§¤ë§¤ ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return json.dumps({"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}, ensure_ascii=False)
    result = with_context(ctx, "analyze_industrial_property_trade", call)
    return TextContent(type="text", text=result) 

def analyze_land_property_data(df: pd.DataFrame) -> Dict[str, Any]:
    """DataFrameì„ ë°›ì•„ í† ì§€ ë§¤ë§¤ í†µê³„ë¥¼ ë¶„ì„í•˜ê³  ì˜ë¬¸ keyì™€ ë‹¨ìœ„ê°€ í¬í•¨ëœ ê°’ìœ¼ë¡œ JSONì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if df.empty:
        return {"error": "No data to analyze."}
    # --- ë°ì´í„° ì „ì²˜ë¦¬ ---
    df['ê±°ë˜ê¸ˆì•¡_num'] = to_numeric(get_col_from_df(df, 'ê±°ë˜ê¸ˆì•¡', 'dealAmount', 'dealAmountNum'))
    # Fix: include 'dealArea' as a fallback for area
    df['í† ì§€ë©´ì _num'] = to_numeric(get_col_from_df(df, 'ë©´ì ', 'landAr', 'dealArea', 'area', 'areaNum'))
    df.dropna(subset=['ê±°ë˜ê¸ˆì•¡_num', 'í† ì§€ë©´ì _num'], inplace=True)
    df = df[df['í† ì§€ë©´ì _num'] > 0].copy()
    if df.empty:
        return {"error": "No valid transaction data after cleaning."}
    df['í‰ë‹¹ê°€_ë§Œì›'] = (df['ê±°ë˜ê¸ˆì•¡_num'] / df['í† ì§€ë©´ì _num']) * 3.305785
    def as_value_unit_m(v): return as_value_unit(v, "ë§Œì›")
    def as_value_unit_per_pyeong(v): return as_value_unit(v, "ë§Œì›/í‰")
    # --- 1. ì¢…í•© í†µê³„ ---
    total_count = len(df)
    total_value = df['ê±°ë˜ê¸ˆì•¡_num'].sum()
    land_type_col = get_col_from_df(df, 'ì§€ëª©', 'landType')
    type_distribution = land_type_col.value_counts().to_dict() if land_type_col.notna().any() else {}
    overall_stats = {
        "totalTransactionCount": total_count,
        "totalTransactionValue": as_value_unit_m(total_value),
        "transactionDistributionByLandType": type_distribution
    }
    # --- 2. ê°€ê²© ìˆ˜ì¤€ í†µê³„ ---
    price_stats_raw = df['ê±°ë˜ê¸ˆì•¡_num'].agg(['mean', 'median', 'max', 'min'])
    price_stats = {
        "overallAveragePrice": as_value_unit_m(price_stats_raw['mean']),
        "overallMedianPrice": as_value_unit_m(price_stats_raw['median']),
        "overallHighestPrice": as_value_unit_m(price_stats_raw['max']),
        "overallLowestPrice": as_value_unit_m(price_stats_raw['min']),
        "representativeDeals": {
            "highestPriceDeal": clean_deal_for_display(df.loc[df['ê±°ë˜ê¸ˆì•¡_num'].idxmax()]),
            "lowestPriceDeal": clean_deal_for_display(df.loc[df['ê±°ë˜ê¸ˆì•¡_num'].idxmin()]),
            "dealClosestToAverage": clean_deal_for_display(df.loc[(df['ê±°ë˜ê¸ˆì•¡_num'] - price_stats_raw['mean']).abs().idxmin()]),
            "dealClosestToMedian": clean_deal_for_display(df.loc[(df['ê±°ë˜ê¸ˆì•¡_num'] - price_stats_raw['median']).abs().idxmin()])
        }
    }
    # --- 3. ë‹¨ìœ„ ë©´ì ë‹¹ ê°€ê²© í†µê³„ ---
    price_per_area_stats = {
        "overallAveragePricePerPyeong": as_value_unit_per_pyeong(df['í‰ë‹¹ê°€_ë§Œì›'].mean()),
        "overallMedianPricePerPyeong": as_value_unit_per_pyeong(df['í‰ë‹¹ê°€_ë§Œì›'].median()),
    }
    if land_type_col.notna().any():
        price_by_type_raw = df.groupby(land_type_col)['ê±°ë˜ê¸ˆì•¡_num'].agg(['mean', 'median', 'max', 'min'])
        price_stats["priceStatisticsByLandType"] = {
            land_type: {
                "averagePrice": as_value_unit_m(stats['mean']),
                "medianPrice": as_value_unit_m(stats['median']),
                "highestPrice": as_value_unit_m(stats['max']),
                "lowestPrice": as_value_unit_m(stats['min']),
            } for land_type, stats in price_by_type_raw.to_dict('index').items()
        }
        price_per_area_by_type_raw = df.groupby(land_type_col)['í‰ë‹¹ê°€_ë§Œì›'].agg(['mean', 'median'])
        price_per_area_stats["pricePerPyeongStatisticsByLandType"] = {
            land_type: {
                "averagePricePerPyeong": as_value_unit_per_pyeong(stats['mean']),
                "medianPricePerPyeong": as_value_unit_per_pyeong(stats['median']),
            } for land_type, stats in price_per_area_by_type_raw.to_dict('index').items()
        }
    # --- 4. ì…ì§€ë³„ í†µê³„ (ë™ë³„) ---
    location_col = get_col_from_df(df, 'ë²•ì •ë™', 'umdNm', 'dong')
    location_stats = {}
    if location_col.notna().any():
        location_summary_raw = df.groupby(location_col).agg(
            Count=('ê±°ë˜ê¸ˆì•¡_num', 'size'),
            Mean_Price=('ê±°ë˜ê¸ˆì•¡_num', 'mean'),
            Max_Price=('ê±°ë˜ê¸ˆì•¡_num', 'max'),
            Min_Price=('ê±°ë˜ê¸ˆì•¡_num', 'min'),
            Mean_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'mean'),
            Median_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'median')
        )
        for dong, stats in location_summary_raw.to_dict('index').items():
            location_stats[dong] = {
                "transactionCount": int(stats['Count']),
                "averagePrice": as_value_unit_m(stats['Mean_Price']),
                "highestPrice": as_value_unit_m(stats['Max_Price']),
                "lowestPrice": as_value_unit_m(stats['Min_Price']),
                "averagePricePerPyeong": as_value_unit_per_pyeong(stats['Mean_PPA']),
                "medianPricePerPyeong": as_value_unit_per_pyeong(stats['Median_PPA']),
            }
    return {
        "overallStatistics": overall_stats,
        "priceLevelStatistics": price_stats,
        "pricePerAreaStatistics": price_per_area_stats,
        "statisticsByLandType": price_stats.get("priceStatisticsByLandType", {}),
        "locationStatistics_byDong": location_stats,
        "notes": "í† ì§€ ê±°ë˜ëŠ” ì§€ëª©(ìš©ë„), ë©´ì , ë„ë¡œì ‘ë©´, í˜•ìƒ, ë°©ìœ„ ë“± íŠ¹ì„±ì´ ì¤‘ìš”í•©ë‹ˆë‹¤. í‰ë‹¹ê°€ëŠ” í† ì§€ë©´ì  ê¸°ì¤€ì…ë‹ˆë‹¤."
    }

@mcp.tool(
    name="analyze_land_trade",
    description="""í† ì§€ ë§¤ë§¤ ì‹¤ê±°ë˜ ë°ì´í„° íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ì›”ê°„ ë¦¬í¬íŠ¸ í˜•ì‹ì˜ í•µì‹¬ í†µê³„ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.\nì´ ë„êµ¬ëŠ” `get_land_trade_data`ë¥¼ í†µí•´ ì–»ì€ ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì…ë ¥ë°›ì•„ ì‘ë™í•©ë‹ˆë‹¤.\nì¢…í•© í†µê³„, ê°€ê²© ìˆ˜ì¤€, í‰ë‹¹ê°€, ì§€ëª©ë³„, ë™ë³„ ë“± ë‹¤ê°ì ì¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\në¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì£¼ìš” í†µê³„ ì§€í‘œë“¤ì„ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ì°¨íŠ¸ë‚˜ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n\nArguments:\n- file_path (str, required): `get_land_trade_data` ë„êµ¬ë¡œ ìƒì„±ëœ `raw.data.json` ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œ.\n\nReturns:\n- í†µê³„ ë¶„ì„ ê²°ê³¼ê°€ ë‹´ê¸´ ìƒì„¸í•œ JSON ë¬¸ìì—´.""",
    tags={"í† ì§€", "í†µê³„", "ë¶„ì„", "ë¦¬í¬íŠ¸", "ë§¤ë§¤", "ì‹¤ê±°ë˜ê°€"}
)
def analyze_land_trade(
    file_path: Annotated[str, Field(description="ë¶„ì„í•  raw.data.json íŒŒì¼ ê²½ë¡œ")],
    ctx: Optional[Any] = None
) -> TextContent:
    def call(context):
        try:
            p = Path(file_path)
            if not p.exists():
                return json.dumps({"error": f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}"}, ensure_ascii=False)
            cache_path = get_summary_cache_path(p, property_type="land", trade_type=None)
            if cache_path.exists():
                if cache_path.stat().st_mtime > p.stat().st_mtime:
                    logger.info(f"âœ… ìœ íš¨í•œ í† ì§€ ë§¤ë§¤ ìºì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {cache_path}")
                    with open(cache_path, 'r', encoding='utf-8') as f:
                        return f.read()
            logger.info(f"ğŸ”„ ìƒˆë¡œìš´ í† ì§€ ë§¤ë§¤ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤: {file_path}")
            df = pd.read_json(p, lines=True)
            summary_data = analyze_land_property_data(df)
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=4, default=default_serializer)
            summary_data["summary_cached_path"] = str(cache_path)
            return json.dumps(summary_data, ensure_ascii=False, indent=4, default=default_serializer)
        except Exception as e:
            logger.error(f"í† ì§€ ë§¤ë§¤ ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return json.dumps({"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}, ensure_ascii=False)
    result = with_context(ctx, "analyze_land_trade", call)
    return TextContent(type="text", text=result)

@mcp.tool(
    name="get_ecos_statistic_table_list",
    description="""
í•œêµ­ì€í–‰ ECOS ì˜¤í”ˆAPIì˜ í†µê³„í‘œ ëª©ë¡ì„ ì¡°íšŒí•˜ê³  ìºì‹±í•©ë‹ˆë‹¤. (StatisticTableList)

ì‚¬ìš©ë²• ì˜ˆì‹œ:
get_ecos_statistic_table_list({"start": 1, "end": 100, "stat_code": null})
- start, end: ì¡°íšŒ ì‹œì‘/ì¢…ë£Œ ì¸ë±ìŠ¤(ê¸°ë³¸ 1~100)
- stat_code: íŠ¹ì • í†µê³„í‘œì½”ë“œë¡œ í•„í„°ë§(ì„ íƒ)

ê²°ê³¼ëŠ” jsonìœ¼ë¡œ ìºì‹±ë˜ë©°, ìºì‹œ íŒŒì¼ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
""",
    tags={"ECOS", "í†µê³„", "ëª©ë¡", "í•œêµ­ì€í–‰"}
)
def get_ecos_statistic_table_list(
    params: Annotated[dict, Field(description="ì¡°íšŒ íŒŒë¼ë¯¸í„° (start, end, stat_code)")]
) -> TextContent:
    path = get_statistic_table_list(params)
    return TextContent(type="text", text=str(path))

@mcp.tool(
    name="get_ecos_statistic_word",
    description="""
í•œêµ­ì€í–‰ ECOS ì˜¤í”ˆAPIì˜ í†µê³„ìš©ì–´ì‚¬ì „ì„ ì¡°íšŒí•˜ê³  ìºì‹±í•©ë‹ˆë‹¤. (StatisticWord)

ì‚¬ìš©ë²• ì˜ˆì‹œ:
get_ecos_statistic_word({"word": null, "start": 1, "end": 100})
- word: ê²€ìƒ‰í•  ìš©ì–´(ì„ íƒ)
- start, end: ì¡°íšŒ ì‹œì‘/ì¢…ë£Œ ì¸ë±ìŠ¤(ê¸°ë³¸ 1~100)

ê²°ê³¼ëŠ” jsonìœ¼ë¡œ ìºì‹±ë˜ë©°, ìºì‹œ íŒŒì¼ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
""",
    tags={"ECOS", "í†µê³„", "ìš©ì–´", "ì‚¬ì „", "í•œêµ­ì€í–‰"}
)
def get_ecos_statistic_word(
    params: Annotated[dict, Field(description="ì¡°íšŒ íŒŒë¼ë¯¸í„° (word, start, end)")]
) -> TextContent:
    path = get_statistic_word(params)
    return TextContent(type="text", text=str(path))

@mcp.tool(
    name="get_ecos_statistic_item_list",
    description="""
í•œêµ­ì€í–‰ ECOS ì˜¤í”ˆAPIì˜ í†µê³„ ì„¸ë¶€í•­ëª© ëª©ë¡ì„ ì¡°íšŒí•˜ê³  ìºì‹±í•©ë‹ˆë‹¤. (StatisticItemList)

ì‚¬ìš©ë²• ì˜ˆì‹œ:
get_ecos_statistic_item_list({"stat_code": "601Y002", "start": 1, "end": 100})
- stat_code: í†µê³„í‘œì½”ë“œ(í•„ìˆ˜)
- start, end: ì¡°íšŒ ì‹œì‘/ì¢…ë£Œ ì¸ë±ìŠ¤(ê¸°ë³¸ 1~100)

ê²°ê³¼ëŠ” jsonìœ¼ë¡œ ìºì‹±ë˜ë©°, ìºì‹œ íŒŒì¼ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
""",
    tags={"ECOS", "í†µê³„", "í•­ëª©", "ì„¸ë¶€í•­ëª©", "í•œêµ­ì€í–‰"}
)
def get_ecos_statistic_item_list(
    params: Annotated[dict, Field(description="ì¡°íšŒ íŒŒë¼ë¯¸í„° (stat_code, start, end)")]
) -> TextContent:
    path = get_statistic_item_list(params)
    return TextContent(type="text", text=str(path))

@mcp.tool(
    name="get_ecos_statistic_search",
    description="""
í•œêµ­ì€í–‰ ECOS ì˜¤í”ˆAPIì˜ í†µê³„ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ê³  ìºì‹±í•©ë‹ˆë‹¤. (StatisticSearch)

ì‚¬ìš©ë²• ì˜ˆì‹œ:
get_ecos_statistic_search({"stat_code": "200Y101", "cycle": "A", "start_time": "2020", "end_time": "2023", "item_code1": null, "item_code2": null, "item_code3": null, "item_code4": null, "start": 1, "end": 100})
- stat_code: í†µê³„í‘œì½”ë“œ(í•„ìˆ˜)
- cycle: ì£¼ê¸°(A, S, Q, M, SM, D)
- start_time, end_time: ê²€ìƒ‰ ì‹œì‘/ì¢…ë£Œì¼ì(ì£¼ê¸°ì— ë§ëŠ” í˜•ì‹)
- item_code1~4: í†µê³„í•­ëª©ì½”ë“œ(ì„ íƒ)
- start, end: ì¡°íšŒ ì‹œì‘/ì¢…ë£Œ ì¸ë±ìŠ¤(ê¸°ë³¸ 1~100)

ê²°ê³¼ëŠ” jsonìœ¼ë¡œ ìºì‹±ë˜ë©°, ìºì‹œ íŒŒì¼ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
""",
    tags={"ECOS", "í†µê³„", "ì¡°íšŒ", "ë°ì´í„°", "í•œêµ­ì€í–‰"}
)
def get_ecos_statistic_search(
    params: Annotated[dict, Field(description="ì¡°íšŒ íŒŒë¼ë¯¸í„° (stat_code, cycle, start_time, end_time, item_codes)")]
) -> TextContent:
    from pathlib import Path
    import json
    path = get_statistic_search(params)
    cache_path = Path(path)
    if not cache_path.exists():
        return TextContent(type="text", text=json.dumps({"error": f"Cache file not found. Expected at: {str(cache_path)}"}, ensure_ascii=False, indent=2))
    return TextContent(type="text", text=str(cache_path))

@mcp.tool(
    name="get_ecos_key_statistic_list",
    description="""
í•œêµ­ì€í–‰ ECOS ì˜¤í”ˆAPIì˜ 100ëŒ€ í†µê³„ì§€í‘œë¥¼ ì¡°íšŒí•˜ê³  ìºì‹±í•©ë‹ˆë‹¤. (KeyStatisticList)

ì‚¬ìš©ë²• ì˜ˆì‹œ:
get_ecos_key_statistic_list({"start": 1, "end": 100})
- start, end: ì¡°íšŒ ì‹œì‘/ì¢…ë£Œ ì¸ë±ìŠ¤(ê¸°ë³¸ 1~100)

ê²°ê³¼ëŠ” jsonìœ¼ë¡œ ìºì‹±ë˜ë©°, ìºì‹œ íŒŒì¼ ê²½ë¡œì™€ pandas ìš”ì•½(preview, ìƒìœ„ 5ê°œ ì£¼ìš” ì»¬ëŸ¼)ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
""",
    tags={"ECOS", "í†µê³„", "100ëŒ€ì§€í‘œ", "ì£¼ìš”ì§€í‘œ", "í•œêµ­ì€í–‰"}
)
def get_ecos_key_statistic_list(
    params: Annotated[dict, Field(description="ì¡°íšŒ íŒŒë¼ë¯¸í„° (start, end)")]
) -> TextContent:
    path = get_key_statistic_list(params)
    if path is None:
        return TextContent(type="text", text=json.dumps({"error": "Cache path is None. Check your parameters."}, ensure_ascii=False, indent=2))
    try:
        with open(str(path), "r", encoding="utf-8") as f:
            data = json.load(f)
        # ì‹¤ì œ ECOS ì‘ë‹µ êµ¬ì¡°ì— ë§ê²Œ row ì¶”ì¶œ
        if "KeyStatisticList" in data and "row" in data["KeyStatisticList"]:
            rows = data["KeyStatisticList"]["row"]
            df = pd.DataFrame(rows)
            preview = df.head(5)[[c for c in df.columns if c in ["CLASS_NAME", "KEYSTAT_NAME", "DATA_VALUE", "CYCLE", "UNIT_NAME"] or c.lower().startswith("stat")]].to_dict(orient="records")
            return TextContent(type="text", text=json.dumps({"cache_path": str(path), "preview": preview}, ensure_ascii=False, indent=2))
        return TextContent(type="text", text=json.dumps({"cache_path": str(path), "preview": "No preview available"}, ensure_ascii=False, indent=2))
    except Exception as e:
        return TextContent(type="text", text=json.dumps({"cache_path": str(path), "error": str(e)}, ensure_ascii=False, indent=2))

def _normalize_korean(text):
    # í•œê¸€ ìëª¨ ë¶„ë¦¬ ë° ì†Œë¬¸ì ë³€í™˜
    if not isinstance(text, str):
        return ""
    text = unicodedata.normalize('NFKD', text)
    return ''.join([c for c in text if not unicodedata.combining(c)]).lower().replace(" ", "")

def _ecos_timerange_to_dates(cycle, timerange):
    """
    ECOS CYCLE: 'A'(ì—°), 'M'(ì›”), 'Q'(ë¶„ê¸°), 'D'(ì¼)
    timerange: '2018-2024', '2020', '202301-202312', ...
    Returns: (start_time, end_time) in ECOS API format
    """
    import re
    import pandas as pd
    if not timerange:
        now = pd.Timestamp.now()
        if cycle == 'A':
            return str(now.year - 3), str(now.year)
        elif cycle == 'M':
            return (now - pd.DateOffset(years=3)).strftime('%Y%m'), now.strftime('%Y%m')
        elif cycle == 'Q':
            return f"{now.year-3}Q1", f"{now.year}Q4"
        elif cycle == 'D':
            return (now - pd.DateOffset(years=1)).strftime('%Y%m%d'), now.strftime('%Y%m%d')
        else:
            return str(now.year - 3), str(now.year)
    # timerange íŒŒì‹±
    parts = re.split(r'[-~]', timerange)
    if len(parts) == 2:
        start, end = parts
    else:
        start = end = parts[0]
    # ê° ì£¼ê¸°ë³„ë¡œ ë³€í™˜
    def pad(val, n):
        return val + '0'*(n-len(val)) if len(val) < n else val
    if cycle == 'A':
        return start[:4], end[:4]
    elif cycle == 'M':
        # YYYYMM í˜•ì‹ìœ¼ë¡œ ë³´ì •
        s = pad(start, 6) if len(start) <= 6 else start[:6]
        e = pad(end, 6) if len(end) <= 6 else end[:6]
        return s, e
    elif cycle == 'Q':
        # YYYYQn í˜•ì‹
        def to_q(val):
            if 'Q' in val: return val
            if len(val) == 6 and val[4] in '1234':
                return val[:4] + 'Q' + val[5]
            if len(val) == 5 and val[4] in '1234':
                return val[:4] + 'Q' + val[4]
            return val[:4] + 'Q1'
        return to_q(start), to_q(end)
    elif cycle == 'D':
        s = pad(start, 8) if len(start) <= 8 else start[:8]
        e = pad(end, 8) if len(end) <= 8 else end[:8]
        return s, e
    else:
        return start, end

def _load_all_stat_table_rows():
    """ëª¨ë“  StatisticTableList ìºì‹œ íŒŒì¼ì„ ë³‘í•©í•˜ì—¬ row ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
    import json
    import os
    cache_dir = os.path.join(os.path.dirname(__file__), "../utils/cache")
    files = glob.glob(os.path.join(cache_dir, "StatisticTableList_end-*_start-*.json"))
    all_rows = []
    for f in files:
        try:
            with open(f, "r", encoding="utf-8") as fp:
                data = json.load(fp)
            rows = data.get("StatisticTableList", {}).get("row", [])
            if isinstance(rows, dict):
                rows = [rows]
            all_rows.extend(rows)
        except Exception:
            continue
    return all_rows

def ensure_latest_keystatlist_cache():
    cache_path = os.path.join(os.path.dirname(__file__), "../utils/cache/ecos/KeyStatisticList.json")
    # 24ì‹œê°„ ì´ë‚´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    if os.path.exists(cache_path):
        mtime = os.path.getmtime(cache_path)
        if time.time() - mtime < 86400:
            return
    # ì•„ë‹ˆë©´ ìµœì‹ í™”
    get_key_statistic_list({"start": 1, "end": 100})

@mcp.tool(
    name="search_realestate_indicators",
    description="""
í‚¤ì›Œë“œë¡œ í•œêµ­ì€í–‰ ECOSì˜ 100ëŒ€ ì£¼ìš” í†µê³„ì§€í‘œ(KeyStatisticList)ì—ì„œ ë¶€ë™ì‚°/ê¸ˆë¦¬/ê°€ê³„/íˆ¬ì/ê±°ì‹œ/ì‹¬ë¦¬ì§€í‘œë¥¼ ê²€ìƒ‰í•˜ê³ , ê´€ë ¨ ì§€í‘œì˜ ìµœì‹ ê°’ë§Œ ìš”ì•½í•©ë‹ˆë‹¤.

- ì´ ë„êµ¬ëŠ” ë¬´ì¡°ê±´ 100ëŒ€ ì£¼ìš”ì§€í‘œ(KeyStatisticList) ê¸°ì¤€ìœ¼ë¡œë§Œ ì‘ë‹µí•©ë‹ˆë‹¤.
- keyword: ì§€í‘œëª…(ì˜ˆ: 'ê¸°ì¤€ê¸ˆë¦¬', 'ì£¼íƒë§¤ë§¤ê°€ê²©ì§€ìˆ˜', 'ê°€ê³„ì‹ ìš©', 'ê±´ì„¤íˆ¬ìì¦ê°ë¥ ', 'ì†Œë¹„ìì‹¬ë¦¬ì§€ìˆ˜' ë“±)
- ê²°ê³¼ëŠ” ìµœì‹ ê°’, ë‹¨ìœ„, ê¸°ì¤€ì¼ ë“±ë§Œ ìš”ì•½í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
- data_preview ë“± ì‹œê³„ì—´/í•­ëª© ì •ë³´ëŠ” ì œê³µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- ì˜ˆì‹œ: search_realestate_indicators({"keyword": "ê¸°ì¤€ê¸ˆë¦¬"})
""",
    tags={"ECOS", "100ëŒ€ì§€í‘œ", "ë¶€ë™ì‚°", "í†µê³„", "ìë™ìˆ˜ì§‘", "ì¶”ì²œì§€í‘œ"}
)
def search_realestate_indicators(
    params: Annotated[dict, Field(description="ê²€ìƒ‰ íŒŒë¼ë¯¸í„° (keyword)")]
) -> TextContent:
    """
    Always respond based on the 100 KeyStatisticList indicators, returning only filtered/summarized results from that list.
    """
    keyword = params.get("keyword")
    if not keyword:
        return TextContent(type="text", text=json.dumps({"error": "keyword íŒŒë¼ë¯¸í„°ëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤."}, ensure_ascii=False))
    keystat_name_to_row = load_keystat_name_to_row()
    # ìœ ì‚¬ë„/í¬í•¨ ê²€ìƒ‰ (í•œê¸€ ì •ê·œí™” í¬í•¨)
    def norm(s):
        import unicodedata
        return unicodedata.normalize("NFKC", str(s or "")).replace(" ", "")
    norm_kw = norm(keyword)
    matches = []
    for name, row in keystat_name_to_row.items():
        if norm_kw in norm(name) or norm_kw in norm(row.get("KEYSTAT_NAME", "")):
            matches.append(row)
    # ìœ ì‚¬ë„ ë†’ì€ ìˆœ ì •ë ¬ (ê°„ë‹¨íˆ ê¸¸ì´ ì°¨ì´ ê¸°ì¤€)
    matches = sorted(matches, key=lambda r: abs(len(norm(r.get("KEYSTAT_NAME", ""))) - len(norm_kw)))
    # ê²°ê³¼ ìš”ì•½
    result = [
        {
            "stat_name": r.get("KEYSTAT_NAME"),
            "class_name": r.get("CLASS_NAME"),
            "current_value": r.get("DATA_VALUE"),
            "cycle": r.get("CYCLE"),
            "unit": r.get("UNIT_NAME")
        }
        for r in matches
    ]
    if not result:
        return TextContent(type="text", text=json.dumps({"error": "No matching indicator found in the 100 KeyStatisticList.", "keyword": keyword}, ensure_ascii=False, indent=2))
    return TextContent(type="text", text=json.dumps({"keyword": keyword, "results": result}, ensure_ascii=False, indent=2))

# === [í•µì‹¬ ë¶€ë™ì‚°/ê¸ˆë¦¬/ê°€ê³„/íˆ¬ì/ê±°ì‹œ/ì‹¬ë¦¬ì§€í‘œëª… â†’ ECOS KeyStatisticList ë§¤í•‘] ===
# ì‹¤ì œ stat_codeëŠ” ECOS StatisticTableList/KeyStatisticListì—ì„œ ë§¤í•‘ í•„ìš”. ì•„ë˜ëŠ” ì˜ˆì‹œ(ì‹¤ì œ ì½”ë“œì—ì„œëŠ” ìë™ ë§¤í•‘/ê²€ìƒ‰ë„ ì§€ì›)
REALESTATE_KEY_INDICATORS = {
    "ì£¼íƒë§¤ë§¤ê°€ê²©ì§€ìˆ˜": "ì£¼íƒë§¤ë§¤ê°€ê²©ì§€ìˆ˜",
    "ì£¼íƒì „ì„¸ê°€ê²©ì§€ìˆ˜": "ì£¼íƒì „ì„¸ê°€ê²©ì§€ìˆ˜",
    "ì§€ê°€ë³€ë™ë¥ ": "ì§€ê°€ë³€ë™ë¥ (ì „ê¸°ëŒ€ë¹„)",
    "í•œêµ­ì€í–‰ ê¸°ì¤€ê¸ˆë¦¬": "í•œêµ­ì€í–‰ ê¸°ì¤€ê¸ˆë¦¬",
    "ì˜ˆê¸ˆì€í–‰ ëŒ€ì¶œê¸ˆë¦¬": "ì˜ˆê¸ˆì€í–‰ ëŒ€ì¶œê¸ˆë¦¬",
    "ì½œê¸ˆë¦¬": "ì½œê¸ˆë¦¬(ìµì¼ë¬¼)",
    "KORIBOR": "KORIBOR(3ê°œì›”)",
    "ê°€ê³„ì‹ ìš©": "ê°€ê³„ì‹ ìš©",
    "ê°€ê³„ëŒ€ì¶œì—°ì²´ìœ¨": "ê°€ê³„ëŒ€ì¶œì—°ì²´ìœ¨",
    "ê±´ì„¤íˆ¬ìì¦ê°ë¥ ": "ê±´ì„¤íˆ¬ìì¦ê°ë¥ (ì‹¤ì§ˆ, ê³„ì ˆì¡°ì • ì „ê¸°ëŒ€ë¹„)",
    "ê±´ì¶•í—ˆê°€ë©´ì ": "ê±´ì¶•í—ˆê°€ë©´ì ",
    "ê±´ì„¤ê¸°ì„±ì•¡": "ê±´ì„¤ê¸°ì„±ì•¡",
    "ê±´ì¶•ì°©ê³µë©´ì ": "ê±´ì¶•ì°©ê³µë©´ì ",
    "ê²½ì œì„±ì¥ë¥ ": "ê²½ì œì„±ì¥ë¥ (ì‹¤ì§ˆ, ê³„ì ˆì¡°ì • ì „ê¸°ëŒ€ë¹„)",
    "M2í†µí™”ëŸ‰": "M2(ê´‘ì˜í†µí™”, í‰ì”)",
    "ì†Œë¹„ìì‹¬ë¦¬ì§€ìˆ˜": "ì†Œë¹„ìì‹¬ë¦¬ì§€ìˆ˜",
    "ê°€êµ¬ë‹¹ì†Œë“": "ê°€êµ¬ë‹¹ì›”í‰ê· ì†Œë“"
}

def load_keystat_name_to_row():
    """Load the latest KeyStatisticList.json and return a dict mapping KEYSTAT_NAME to row, handling field typos."""
    cache_path = os.path.join(os.path.dirname(__file__), "../utils/cache/ecos/KeyStatisticList.json")
    if not os.path.exists(cache_path):
        return {}
    with open(cache_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    rows = data.get("KeyStatisticList", {}).get("row", [])
    # Fix for possible typo in DATA_VALUE field
    for row in rows:
        if "DATA_VALUE" not in row:
            # Use a copy of the keys to avoid changing dict size during iteration
            for k in list(row.keys()):
                if "DATA_VAL" in k:
                    row["DATA_VALUE"] = row[k]
    return {row["KEYSTAT_NAME"].replace("(ì „ê¸°ëŒ€ë¹„)", ""): row for row in rows if "KEYSTAT_NAME" in row}