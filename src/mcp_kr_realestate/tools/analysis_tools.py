"""
ë¶€ë™ì‚° ë°ì´í„° ë¶„ì„ ë° í†µê³„ ë¦¬í¬íŒ… ë„êµ¬
"""

import logging
import pandas as pd
import numpy as np
import xml.etree.ElementTree as ET
from pathlib import Path
import json
from typing import Any, Optional, Dict
from datetime import datetime
import os
from dateutil.parser import parse as date_parse
import re

from mcp_kr_realestate.server import mcp
from mcp_kr_realestate.utils.ctx_helper import with_context
from mcp.types import TextContent
from mcp_kr_realestate.apis.reb_api import get_reb_stat_list, get_reb_stat_items, get_reb_stat_data, get_reb_stat_list_all, get_reb_stat_items_all, get_reb_stat_data_all, cache_stat_list_full, cache_stat_list

logger = logging.getLogger("mcp-kr-realestate")

# --- Helper Functions ---
def default_serializer(o):
    """JSON ì§ë ¬í™”ë¥¼ ìœ„í•œ ê¸°ë³¸ serializer. numpy íƒ€ì…ì„ python íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    if isinstance(o, (np.integer, np.floating)):
        return o.item()
    if isinstance(o, np.ndarray):
        return o.tolist()
    if isinstance(o, pd.Timestamp):
        return o.isoformat()
    raise TypeError(f"Object of type {o.__class__.__name__} is not JSON serializable")

def to_numeric(series: pd.Series) -> pd.Series:
    """Helper function to convert series to numeric, handling commas."""
    return pd.to_numeric(series.astype(str).str.replace(',', ''), errors='coerce')

def as_value_unit(value, unit_str, precision=0):
    if pd.isna(value):
        return None
    if unit_str == "ë§Œì›":
        v = float(value) * 10000
        unit_str = "ì›"
    elif unit_str == "ë§Œì›/í‰":
        v = float(value) * 10000
        unit_str = "ì›/í‰"
    else:
        if isinstance(value, (np.integer, int)):
            v = int(value)
        else:
            v = float(value)
            if precision == 0:
                v = int(round(v))
    return {"value": v, "unit": unit_str}

def clean_deal_for_display(series):
    deal_dict = series.where(pd.notna(series), None).to_dict()
    # Format date
    try:
        deal_dict['dealDate'] = f"{deal_dict.get('dealYear')}-{str(deal_dict.get('dealMonth','')).zfill(2)}-{str(deal_dict.get('dealDay','')).zfill(2)}"
    except (TypeError, ValueError):
        deal_dict['dealDate'] = None
    # Remove intermediate columns
    cols_to_remove = ['ê±°ë˜ê¸ˆì•¡_num', 'ì „ìš©ë©´ì _num', 'ê±´ì¶•ë…„ë„_num', 'í‰ë‹¹ê°€_ë§Œì›', 'ê±´ë¬¼ì—°ë ¹', 'ê±´ë¬¼ì—°ë ¹ëŒ€', 'ê±´ë¬¼ê·œëª¨', 'dealYear', 'dealMonth', 'dealDay']
    for col in cols_to_remove:
        deal_dict.pop(col, None)
    # ìˆ«ì í•„ë“œ value/unit êµ¬ì¡°ë¡œ ë³€í™˜
    for k in list(deal_dict.keys()):
        if k in ['dealAmount', 'dealAmountNum', 'ë³´ì¦ê¸ˆ', 'deposit', 'ë³´ì¦ê¸ˆì•¡', 'ì›”ì„¸', 'monthlyRent', 'ì›”ì„¸ì•¡', 'rentFee', 'rentFeeNum']:
            try:
                v = float(deal_dict[k]) if deal_dict[k] is not None else None
            except Exception:
                continue
            deal_dict[k] = as_value_unit(v, "ë§Œì›")
        elif k in ['í‰ë‹¹ê°€', 'í‰ë‹¹ê°€_ë§Œì›']:
            try:
                v = float(deal_dict[k]) if deal_dict[k] is not None else None
            except Exception:
                continue
            deal_dict[k] = as_value_unit(v, "ë§Œì›/í‰")
        elif k in ['areaNum', 'ì „ìš©ë©´ì ', 'excluUseAr', 'ê³„ì•½ë©´ì ', 'totalFloorAr', 'ì—°ë©´ì ', 'YUA', 'plottageAr', 'landAr', 'dealArea', 'ê³„ì•½ë©´ì _num', 'ì „ìš©ë©´ì _num', 'ì—°ë©´ì _num', 'í† ì§€ë©´ì _num']:
            try:
                v = float(deal_dict[k]) if deal_dict[k] is not None else None
            except Exception:
                continue
            deal_dict[k] = as_value_unit(v, "ã¡")
        elif k in ['ê±´ì¶•ë…„ë„', 'buildYear', 'buildYearNum']:
            try:
                v = float(deal_dict[k]) if deal_dict[k] is not None else None
            except Exception:
                continue
            deal_dict[k] = as_value_unit(v, "ë…„")
        elif k in ['floor', 'floorNum']:
            try:
                v = float(deal_dict[k]) if deal_dict[k] is not None else None
            except Exception:
                continue
            deal_dict[k] = as_value_unit(v, "ì¸µ")
    return deal_dict

def get_col_from_df(df, *col_names):
    """DataFrameê³¼ ì—¬ëŸ¬ ì»¬ëŸ¼ ì´ë¦„ì„ ë°›ì•„, ì¡´ì¬í•˜ëŠ” ì²« ë²ˆì§¸ ì»¬ëŸ¼ì„ Seriesë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    for col in col_names:
        if col in df.columns:
            return df[col]
    # ì–´ë–¤ ì»¬ëŸ¼ë„ ì°¾ì§€ ëª»í•œ ê²½ìš°, Noneìœ¼ë¡œ ì±„ì›Œì§„ Seriesë¥¼ ë°˜í™˜í•˜ì—¬ ì´í›„ ì—°ì‚°ì—ì„œ ì˜¤ë¥˜ê°€ ë‚˜ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤.
    return pd.Series([None] * len(df), index=df.index, dtype=object)

def get_summary_cache_path(p: Path, property_type: Optional[str] = None, trade_type: Optional[str] = None) -> Path:
    """
    property_type: 'commercial', 'land', 'industrial', 'apartment', 'officetel', 'row_house', 'single_detached', ...
    trade_type: 'trade', 'rent', None
    """
    cache_dir = p.parent.parent / "cache"
    cache_dir.mkdir(parents=True, exist_ok=True)
    # ìƒì—…/í† ì§€/ì°½ê³ (ì‚°ì—…ìš©)ëŠ” ë§¤ë§¤/ì „ì›”ì„¸ êµ¬ë¶„ ì—†ì´ í•˜ë‚˜ì˜ summary
    if property_type in {"commercial", "land", "industrial"}:
        return cache_dir / f"{p.stem}_summary.json"
    # ê·¸ ì™¸ëŠ” ë§¤ë§¤/ì „ì›”ì„¸ë³„ë¡œ ë¶„ë¦¬
    if trade_type:
        return cache_dir / f"{p.stem}_{trade_type}_summary.json"
    return cache_dir / f"{p.stem}_summary.json"

def analyze_commercial_property_data(df: pd.DataFrame) -> Dict[str, Any]:
    """DataFrameì„ ë°›ì•„ ìƒì—…ì—…ë¬´ìš© ë¶€ë™ì‚° í†µê³„ë¥¼ ë¶„ì„í•˜ê³  ì˜ë¬¸ keyì™€ ë‹¨ìœ„ê°€ í¬í•¨ëœ ê°’ìœ¼ë¡œ JSONì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    
    if df.empty:
        return {"error": "No data to analyze."}

    # --- ë°ì´í„° ì „ì²˜ë¦¬ ---
    df['ê±°ë˜ê¸ˆì•¡_num'] = to_numeric(get_col_from_df(df, 'ê±°ë˜ê¸ˆì•¡', 'dealAmount'))
    df['ì „ìš©ë©´ì _num'] = to_numeric(get_col_from_df(df, 'ì „ìš©ë©´ì ', 'area', 'excluUseAr', 'buildingAr'))
    df['ê±´ì¶•ë…„ë„_num'] = to_numeric(get_col_from_df(df, 'ê±´ì¶•ë…„ë„', 'buildYear'))
    
    df.dropna(subset=['ê±°ë˜ê¸ˆì•¡_num', 'ì „ìš©ë©´ì _num'], inplace=True)
    df = df[df['ì „ìš©ë©´ì _num'] > 0].copy()
    if df.empty:
        return {"error": "No valid transaction data after cleaning."}

    df['í‰ë‹¹ê°€_ë§Œì›'] = (df['ê±°ë˜ê¸ˆì•¡_num'] / df['ì „ìš©ë©´ì _num']) * 3.305785
    
    current_year = datetime.now().year
    df['ê±´ë¬¼ì—°ë ¹'] = current_year - df['ê±´ì¶•ë…„ë„_num']

    # --- Formatting helpers ---
    def as_value_unit_m(v): return as_value_unit(v, "ë§Œì›")
    def as_value_unit_per_pyeong(v): return as_value_unit(v, "ë§Œì›/í‰")

    # --- 1. ì¢…í•© í†µê³„ (Overall Statistics) ---
    total_count = len(df)
    total_value = df['ê±°ë˜ê¸ˆì•¡_num'].sum()
    property_type_col = get_col_from_df(df, 'ì£¼ìš©ë„', 'ìœ í˜•', 'buildingUse')
    type_distribution = property_type_col.value_counts().to_dict() if property_type_col.notna().any() else {}

    overall_stats = {
        "totalTransactionCount": total_count,
        "totalTransactionValue": as_value_unit_m(total_value),
        "transactionDistributionByPropertyType": type_distribution
    }

    # --- 2. ê°€ê²© ìˆ˜ì¤€ í†µê³„ (Price Level Statistics) ---
    price_stats_raw = df['ê±°ë˜ê¸ˆì•¡_num'].agg(['mean', 'median', 'max', 'min'])
    price_stats = {
        "overallAveragePrice": as_value_unit_m(price_stats_raw['mean']),
        "overallMedianPrice": as_value_unit_m(price_stats_raw['median']),
        "overallHighestPrice": as_value_unit_m(price_stats_raw['max']),
        "overallLowestPrice": as_value_unit_m(price_stats_raw['min']),
        "representativeDeals": {
            "highestPriceDeal": clean_deal_for_display(df.loc[df['ê±°ë˜ê¸ˆì•¡_num'].idxmax()]),
            "lowestPriceDeal": clean_deal_for_display(df.loc[df['ê±°ë˜ê¸ˆì•¡_num'].idxmin()]),
            "dealClosestToAverage": clean_deal_for_display(df.loc[(df['ê±°ë˜ê¸ˆì•¡_num'] - price_stats_raw['mean']).abs().idxmin()]),
            "dealClosestToMedian": clean_deal_for_display(df.loc[(df['ê±°ë˜ê¸ˆì•¡_num'] - price_stats_raw['median']).abs().idxmin()])
        }
    }

    if property_type_col.notna().any():
        price_by_type_raw = df.groupby(property_type_col)['ê±°ë˜ê¸ˆì•¡_num'].agg(['mean', 'median', 'max', 'min'])
        price_stats["priceStatisticsByPropertyType"] = {
            prop_type: {
                "averagePrice": as_value_unit_m(stats['mean']),
                "medianPrice": as_value_unit_m(stats['median']),
                "highestPrice": as_value_unit_m(stats['max']),
                "lowestPrice": as_value_unit_m(stats['min']),
            } for prop_type, stats in price_by_type_raw.to_dict('index').items()
        }

    # --- 3. ë‹¨ìœ„ ë©´ì ë‹¹ ê°€ê²© í†µê³„ (Price per Area Statistics) ---
    price_per_area_stats_raw = df['í‰ë‹¹ê°€_ë§Œì›'].agg(['mean', 'median'])
    price_per_area_stats = {
        "overallAveragePricePerPyeong": as_value_unit_per_pyeong(price_per_area_stats_raw['mean']),
        "overallMedianPricePerPyeong": as_value_unit_per_pyeong(price_per_area_stats_raw['median']),
    }
    if property_type_col.notna().any():
        price_per_area_by_type_raw = df.groupby(property_type_col)['í‰ë‹¹ê°€_ë§Œì›'].agg(['mean', 'median'])
        price_per_area_stats["pricePerPyeongStatisticsByPropertyType"] = {
            prop_type: {
                "averagePricePerPyeong": as_value_unit_per_pyeong(stats['mean']),
                "medianPricePerPyeong": as_value_unit_per_pyeong(stats['median']),
            } for prop_type, stats in price_per_area_by_type_raw.to_dict('index').items()
        }

    # --- 4. ì…ì§€ë³„ í†µê³„ (Location-based Statistics) ---
    location_col = get_col_from_df(df, 'ë²•ì •ë™', 'umdNm', 'dong')
    location_stats = {}
    if location_col.notna().any():
        location_summary_raw = df.groupby(location_col).agg(
            Count=('ê±°ë˜ê¸ˆì•¡_num', 'size'),
            Mean_Price=('ê±°ë˜ê¸ˆì•¡_num', 'mean'),
            Max_Price=('ê±°ë˜ê¸ˆì•¡_num', 'max'),
            Min_Price=('ê±°ë˜ê¸ˆì•¡_num', 'min'),
            Mean_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'mean'),
            Median_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'median')
        )
        
        for dong, stats in location_summary_raw.to_dict('index').items():
            location_stats[dong] = {
                "transactionCount": int(stats['Count']),
                "averagePrice": as_value_unit_m(stats['Mean_Price']),
                "highestPrice": as_value_unit_m(stats['Max_Price']),
                "lowestPrice": as_value_unit_m(stats['Min_Price']),
                "averagePricePerPyeong": as_value_unit_per_pyeong(stats['Mean_PPA']),
                "medianPricePerPyeong": as_value_unit_per_pyeong(stats['Median_PPA']),
            }

    # --- 5. ê±´ë¬¼ íŠ¹ì„±ë³„ í†µê³„ (Building Characteristics Statistics) ---
    age_bins = [0, 6, 11, 21, np.inf]
    age_labels = ['5 years or newer', '6-10 years', '11-20 years', 'over 20 years']
    df['ê±´ë¬¼ì—°ë ¹ëŒ€'] = pd.cut(df['ê±´ë¬¼ì—°ë ¹'], bins=age_bins, labels=age_labels, right=False)
    age_stats = {}
    if not df['ê±´ë¬¼ì—°ë ¹ëŒ€'].isnull().all():
        age_summary_raw = df.groupby('ê±´ë¬¼ì—°ë ¹ëŒ€', observed=True).agg(
            Count=('ê±°ë˜ê¸ˆì•¡_num', 'size'),
            Mean_Price=('ê±°ë˜ê¸ˆì•¡_num', 'mean'),
            Mean_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'mean')
        )
        age_stats = {
            age_group: {
                "transactionCount": int(stats['Count']),
                "averagePrice": as_value_unit_m(stats['Mean_Price']),
                "averagePricePerPyeong": as_value_unit_per_pyeong(stats['Mean_PPA']),
            } for age_group, stats in age_summary_raw.to_dict('index').items()
        }

    area_bins = [0, 100, 300, 1000, np.inf]
    area_labels = ['small (<100mÂ²)', 'medium (100-300mÂ²)', 'large (300-1000mÂ²)', 'extra_large (>1000mÂ²)']
    df['ê±´ë¬¼ê·œëª¨'] = pd.cut(df['ì „ìš©ë©´ì _num'], bins=area_bins, labels=area_labels, right=False)
    scale_stats = {}
    if not df['ê±´ë¬¼ê·œëª¨'].isnull().all():
        scale_summary_raw = df.groupby('ê±´ë¬¼ê·œëª¨', observed=True).agg(
            Count=('ê±°ë˜ê¸ˆì•¡_num', 'size'),
            Mean_Price=('ê±°ë˜ê¸ˆì•¡_num', 'mean'),
            Mean_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'mean')
        )
        scale_stats = {
            scale_group: {
                "transactionCount": int(stats['Count']),
                "averagePrice": as_value_unit_m(stats['Mean_Price']),
                "averagePricePerPyeong": as_value_unit_per_pyeong(stats['Mean_PPA']),
            } for scale_group, stats in scale_summary_raw.to_dict('index').items()
        }
    
    building_stats = {
        "statisticsByBuildingAge": age_stats, 
        "statisticsByBuildingScale_exclusiveArea": scale_stats
    }

    return {
        "overallStatistics": overall_stats,
        "priceLevelStatistics": price_stats,
        "pricePerAreaStatistics": price_per_area_stats,
        "locationStatistics_byDong": location_stats,
        "buildingCharacteristicsStatistics": building_stats,
        "notes": "Price per pyeong and building scale are calculated based on the 'exclusive use area'. Data for 'land share' or 'road access conditions' is not provided by the API and is not included in this analysis."
    }

@mcp.tool(
    name="analyze_commercial_property_trade",
    description="""ìƒì—…ì—…ë¬´ìš©(ì˜¤í”¼ìŠ¤, ìƒê°€) ë¶€ë™ì‚° ë§¤ë§¤ ì‹¤ê±°ë˜ ë°ì´í„° íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ì›”ê°„ ë¦¬í¬íŠ¸ í˜•ì‹ì˜ í•µì‹¬ í†µê³„ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.
ì´ ë„êµ¬ëŠ” `get_commercial_property_trade_data`ë¥¼ í†µí•´ ì–»ì€ ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì…ë ¥ë°›ì•„ ì‘ë™í•©ë‹ˆë‹¤.
ì¢…í•© í†µê³„, ê°€ê²© ìˆ˜ì¤€, í‰ë‹¹ê°€, ì…ì§€, ê±´ë¬¼ íŠ¹ì„±ë³„ ë“± ë‹¤ê°ì ì¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì£¼ìš” í†µê³„ ì§€í‘œë“¤ì„ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ì°¨íŠ¸ë‚˜ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.

Arguments:
- file_path (str, required): `get_commercial_property_trade_data` ë„êµ¬ë¡œ ìƒì„±ëœ `raw.data.json` ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œ.

Returns:
- í†µê³„ ë¶„ì„ ê²°ê³¼ê°€ ë‹´ê¸´ ìƒì„¸í•œ JSON ë¬¸ìì—´.
""",
    tags={"í†µê³„", "ë¶„ì„", "ë¦¬í¬íŠ¸", "ìƒì—…ì—…ë¬´ìš©", "ì‹¤ê±°ë˜ê°€"}
)
def analyze_commercial_property_trade(file_path: str, ctx: Optional[Any] = None) -> TextContent:
    """
    ìƒì—…ì—…ë¬´ìš© ë¶€ë™ì‚° ê±°ë˜ ë°ì´í„° XML íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ í†µê³„ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    def call(context):
        try:
            p = Path(file_path)
            if not p.exists():
                return json.dumps({"error": f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}"}, ensure_ascii=False)

            cache_path = get_summary_cache_path(p, property_type="commercial")

            # ìºì‹œ í™•ì¸ ë° ì¬ì‚¬ìš© ë¡œì§
            if cache_path.exists():
                source_mtime = p.stat().st_mtime
                cache_mtime = cache_path.stat().st_mtime
                if cache_mtime > source_mtime:
                    logger.info(f"âœ… ìœ íš¨í•œ ìºì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {cache_path}")
                    with open(cache_path, 'r', encoding='utf-8') as f:
                        return f.read()
            
            logger.info(f"ğŸ”„ ìºì‹œê°€ ì—†ê±°ë‚˜ ì˜¤ë˜ë˜ì–´ ìƒˆë¡œìš´ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤: {file_path}")
            df = pd.read_json(p, lines=True)
            
            summary_data = analyze_commercial_property_data(df)
            
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=4, default=default_serializer)

            summary_data["summary_cached_path"] = str(cache_path)
            
            return json.dumps(summary_data, ensure_ascii=False, indent=4, default=default_serializer)

        except Exception as e:
            logger.error(f"ìƒì—…ì—…ë¬´ìš© ë¶€ë™ì‚° ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return json.dumps({"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}, ensure_ascii=False)
            
    result = with_context(ctx, "analyze_commercial_property_trade", call)
    return TextContent(type="text", text=result)

# --- ì•„íŒŒíŠ¸ ë§¤ë§¤ ë¶„ì„ ---

def analyze_apartment_trade_data(df: pd.DataFrame) -> Dict[str, Any]:
    """DataFrameì„ ë°›ì•„ ì•„íŒŒíŠ¸ ë§¤ë§¤ í†µê³„ë¥¼ ë¶„ì„í•˜ê³  ì˜ë¬¸ keyì™€ ë‹¨ìœ„ê°€ í¬í•¨ëœ ê°’ìœ¼ë¡œ JSONì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if df.empty:
        return {"error": "No data to analyze."}

    # --- ë°ì´í„° ì „ì²˜ë¦¬ ---
    df['ê±°ë˜ê¸ˆì•¡_num'] = to_numeric(get_col_from_df(df, 'ê±°ë˜ê¸ˆì•¡', 'dealAmount'))
    df['ì „ìš©ë©´ì _num'] = to_numeric(get_col_from_df(df, 'ì „ìš©ë©´ì ', 'area', 'excluUseAr'))
    df['ê±´ì¶•ë…„ë„_num'] = to_numeric(get_col_from_df(df, 'ê±´ì¶•ë…„ë„', 'buildYear'))
    df['ê³„ì•½ë…„ì›”_num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ë…„ì›”'))
    df['ê³„ì•½ì¼_num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ì¼'))

    df.dropna(subset=['ê±°ë˜ê¸ˆì•¡_num', 'ì „ìš©ë©´ì _num'], inplace=True)
    df = df[df['ì „ìš©ë©´ì _num'] > 0].copy()
    if df.empty:
        return {"error": "No valid transaction data after cleaning."}

    df['í‰ë‹¹ê°€_ë§Œì›'] = (df['ê±°ë˜ê¸ˆì•¡_num'] / df['ì „ìš©ë©´ì _num']) * 3.305785
    current_year = datetime.now().year
    df['ê±´ë¬¼ì—°ë ¹'] = current_year - df['ê±´ì¶•ë…„ë„_num']

    def as_value_unit_m(v): return as_value_unit(v, "ë§Œì›")
    def as_value_unit_per_pyeong(v): return as_value_unit(v, "ë§Œì›/í‰")

    # --- 1. ì¢…í•© í†µê³„ ---
    total_count = len(df)
    total_value = df['ê±°ë˜ê¸ˆì•¡_num'].sum()
    overall_stats = {
        "totalTransactionCount": total_count,
        "totalTransactionValue": as_value_unit_m(total_value),
    }

    # --- 2. ê°€ê²© ìˆ˜ì¤€ í†µê³„ ---
    price_stats_raw = df['ê±°ë˜ê¸ˆì•¡_num'].agg(['mean', 'median', 'max', 'min'])
    price_stats = {
        "overallAveragePrice": as_value_unit_m(price_stats_raw['mean']),
        "overallMedianPrice": as_value_unit_m(price_stats_raw['median']),
        "overallHighestPrice": as_value_unit_m(price_stats_raw['max']),
        "overallLowestPrice": as_value_unit_m(price_stats_raw['min']),
        "representativeDeals": {
            "highestPriceDeal": clean_deal_for_display(df.loc[df['ê±°ë˜ê¸ˆì•¡_num'].idxmax()]),
            "lowestPriceDeal": clean_deal_for_display(df.loc[df['ê±°ë˜ê¸ˆì•¡_num'].idxmin()]),
            "dealClosestToAverage": clean_deal_for_display(df.loc[(df['ê±°ë˜ê¸ˆì•¡_num'] - price_stats_raw['mean']).abs().idxmin()]),
            "dealClosestToMedian": clean_deal_for_display(df.loc[(df['ê±°ë˜ê¸ˆì•¡_num'] - price_stats_raw['median']).abs().idxmin()])
        }
    }

    # --- 3. ë‹¨ìœ„ ë©´ì ë‹¹ ê°€ê²© í†µê³„ ---
    price_per_area_stats = {
        "overallAveragePricePerPyeong": as_value_unit_per_pyeong(df['í‰ë‹¹ê°€_ë§Œì›'].mean()),
        "overallMedianPricePerPyeong": as_value_unit_per_pyeong(df['í‰ë‹¹ê°€_ë§Œì›'].median()),
    }

    # --- 4. ë‹¨ì§€ë³„/ì…ì§€ë³„ í†µê³„ ---
    complex_col = get_col_from_df(df, 'ì•„íŒŒíŠ¸', 'ë‹¨ì§€ëª…', 'aptName')
    location_col = get_col_from_df(df, 'ë²•ì •ë™', 'umdNm', 'dong')
    
    def get_grouped_stats(group_col):
        stats = {}
        if group_col.notna().any():
            summary_raw = df.groupby(group_col).agg(
                Count=('ê±°ë˜ê¸ˆì•¡_num', 'size'),
                Mean_Price=('ê±°ë˜ê¸ˆì•¡_num', 'mean'),
                Median_Price=('ê±°ë˜ê¸ˆì•¡_num', 'median'),
                Mean_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'mean'),
                Median_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'median')
            )
            for name, data in summary_raw.to_dict('index').items():
                stats[name] = {
                    "transactionCount": int(data['Count']),
                    "averagePrice": as_value_unit_m(data['Mean_Price']),
                    "medianPrice": as_value_unit_m(data['Median_Price']),
                    "averagePricePerPyeong": as_value_unit_per_pyeong(data['Mean_PPA']),
                    "medianPricePerPyeong": as_value_unit_per_pyeong(data['Median_PPA']),
                }
        return stats

    complex_stats = get_grouped_stats(complex_col)
    location_stats = get_grouped_stats(location_col)

    return {
        "overallStatistics": overall_stats,
        "priceLevelStatistics": price_stats,
        "pricePerAreaStatistics": price_per_area_stats,
        "statisticsByApartmentComplex": complex_stats,
        "statisticsByDong": location_stats,
        "notes": "PPA (Price Per Pyeong) is calculated based on the 'exclusive use area'."
    }

@mcp.tool(
    name="analyze_apartment_trade",
    description="""ì•„íŒŒíŠ¸ ë§¤ë§¤ ì‹¤ê±°ë˜ ë°ì´í„° íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ì›”ê°„ ë¦¬í¬íŠ¸ í˜•ì‹ì˜ í•µì‹¬ í†µê³„ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.
ì´ ë„êµ¬ëŠ” `get_apt_trade_data`ë¥¼ í†µí•´ ì–»ì€ ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì…ë ¥ë°›ì•„ ì‘ë™í•©ë‹ˆë‹¤.
ì¢…í•© í†µê³„, ê°€ê²© ìˆ˜ì¤€, í‰ë‹¹ê°€, ë‹¨ì§€ë³„, ë™ë³„ ë“± ë‹¤ê°ì ì¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì£¼ìš” í†µê³„ ì§€í‘œë“¤ì„ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ì°¨íŠ¸ë‚˜ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.

Arguments:
- file_path (str, required): `get_apt_trade_data` ë„êµ¬ë¡œ ìƒì„±ëœ `raw.data.json` ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œ.

Returns:
- í†µê³„ ë¶„ì„ ê²°ê³¼ê°€ ë‹´ê¸´ ìƒì„¸í•œ JSON ë¬¸ìì—´.
""",
    tags={"ì•„íŒŒíŠ¸", "í†µê³„", "ë¶„ì„", "ë¦¬í¬íŠ¸", "ë§¤ë§¤", "ì‹¤ê±°ë˜ê°€"}
)
def analyze_apartment_trade(file_path: str, ctx: Optional[Any] = None) -> TextContent:
    """
    ì•„íŒŒíŠ¸ ë§¤ë§¤ ê±°ë˜ ë°ì´í„° XML íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ í†µê³„ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    def call(context):
        try:
            p = Path(file_path)
            if not p.exists():
                return json.dumps({"error": f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}"}, ensure_ascii=False)

            cache_path = get_summary_cache_path(p, property_type="apartment", trade_type="trade")

            if cache_path.exists():
                if cache_path.stat().st_mtime > p.stat().st_mtime:
                    logger.info(f"âœ… ìœ íš¨í•œ ì•„íŒŒíŠ¸ ë§¤ë§¤ ìºì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {cache_path}")
                    with open(cache_path, 'r', encoding='utf-8') as f:
                        return f.read()
            
            logger.info(f"ğŸ”„ ìƒˆë¡œìš´ ì•„íŒŒíŠ¸ ë§¤ë§¤ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤: {file_path}")
            df = pd.read_json(p, lines=True)
            
            summary_data = analyze_apartment_trade_data(df)
            
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=4, default=default_serializer)

            summary_data["summary_cached_path"] = str(cache_path)
            return json.dumps(summary_data, ensure_ascii=False, indent=4, default=default_serializer)

        except Exception as e:
            logger.error(f"ì•„íŒŒíŠ¸ ë§¤ë§¤ ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return json.dumps({"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}, ensure_ascii=False)
            
    result = with_context(ctx, "analyze_apartment_trade", call)
    return TextContent(type="text", text=result)

# --- ì•„íŒŒíŠ¸ ì „ì›”ì„¸ ë¶„ì„ ---

def analyze_apartment_rent_data(df: pd.DataFrame) -> Dict[str, Any]:
    """DataFrameì„ ë°›ì•„ ì•„íŒŒíŠ¸ ì „ì›”ì„¸ í†µê³„ë¥¼ ë¶„ì„í•˜ê³  ì „ì„¸/ì›”ì„¸ë¥¼ êµ¬ë¶„í•˜ì—¬ JSONìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if df.empty:
        return {"error": "No data to analyze."}

    # --- ë°ì´í„° ì „ì²˜ë¦¬ ---
    df['ë³´ì¦ê¸ˆ_num'] = to_numeric(get_col_from_df(df, 'ë³´ì¦ê¸ˆì•¡', 'ë³´ì¦ê¸ˆ', 'deposit', 'depositNum'))
    df['ì›”ì„¸_num'] = to_numeric(get_col_from_df(df, 'ì›”ì„¸ì•¡', 'ì›”ì„¸', 'monthlyRent', 'rentFeeNum'))
    df['ì „ìš©ë©´ì _num'] = to_numeric(get_col_from_df(df, 'ì „ìš©ë©´ì ', 'area', 'excluUseAr', 'areaNum'))
    df['ê±´ì¶•ë…„ë„_num'] = to_numeric(get_col_from_df(df, 'ê±´ì¶•ë…„ë„', 'buildYear'))
    df['ê³„ì•½ë…„ì›”_num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ë…„ì›”'))
    df['ê³„ì•½ì¼_num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ì¼'))

    df.dropna(subset=['ë³´ì¦ê¸ˆ_num', 'ì›”ì„¸_num', 'ì „ìš©ë©´ì _num'], inplace=True)
    df = df[df['ì „ìš©ë©´ì _num'] > 0].copy()
    if df.empty:
        return {"error": "No valid transaction data after cleaning."}

    df_jeonse = df[df['ì›”ì„¸_num'] == 0].copy()
    df_wolse = df[df['ì›”ì„¸_num'] > 0].copy()

    def as_value_unit_m(v): return as_value_unit(v, "ë§Œì›")
    
    def analyze_rent_type(df_rent_type, rent_type_name):
        if df_rent_type.empty:
            return { "totalTransactionCount": 0 }

        stats = { "totalTransactionCount": len(df_rent_type) }
        
        price_stats_raw = df_rent_type['ë³´ì¦ê¸ˆ_num'].agg(['mean', 'median', 'max', 'min'])
        stats['depositPriceStatistics'] = {
            "averageDeposit": as_value_unit_m(price_stats_raw['mean']),
            "medianDeposit": as_value_unit_m(price_stats_raw['median']),
            "highestDeposit": as_value_unit_m(price_stats_raw['max']),
            "lowestDeposit": as_value_unit_m(price_stats_raw['min']),
            "representativeDeals": {
                "highestDepositDeal": clean_deal_for_display(df_rent_type.loc[df_rent_type['ë³´ì¦ê¸ˆ_num'].idxmax()]),
                "lowestDepositDeal": clean_deal_for_display(df_rent_type.loc[df_rent_type['ë³´ì¦ê¸ˆ_num'].idxmin()]),
            }
        }
        if rent_type_name == 'wolse': # ì›”ì„¸ í†µê³„ ì¶”ê°€
            wolse_stats_raw = df_rent_type['ì›”ì„¸_num'].agg(['mean', 'median', 'max', 'min'])
            stats['monthlyRentStatistics'] = {
                "averageMonthlyRent": as_value_unit_m(wolse_stats_raw['mean']),
                "medianMonthlyRent": as_value_unit_m(wolse_stats_raw['median']),
            }

        complex_col = get_col_from_df(df_rent_type, 'ì•„íŒŒíŠ¸', 'ë‹¨ì§€ëª…', 'aptName')
        if complex_col.notna().any():
            stats['statisticsByApartmentComplex'] = df_rent_type.groupby(complex_col).agg(
                transactionCount=('ë³´ì¦ê¸ˆ_num', 'size'),
                averageDeposit=('ë³´ì¦ê¸ˆ_num', 'mean')
            ).apply(lambda x: x.astype(int) if x.name == 'transactionCount' else as_value_unit_m(x)).to_dict('index')

        return stats

    jeonse_analysis = analyze_rent_type(df_jeonse, 'jeonse')
    wolse_analysis = analyze_rent_type(df_wolse, 'wolse')
    
    return {
        "transactionTypeDistribution": {
            "jeonse_count": len(df_jeonse),
            "wolse_count": len(df_wolse),
        },
        "jeonseAnalysis": jeonse_analysis,
        "wolseAnalysis": wolse_analysis,
        "notes": "Statistics are separated by transaction type (Jeonse vs. Wolse)."
    }

@mcp.tool(
    name="analyze_apartment_rent",
    description="""ì•„íŒŒíŠ¸ ì „ì›”ì„¸ ì‹¤ê±°ë˜ ë°ì´í„° íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ì›”ê°„ ë¦¬í¬íŠ¸ í˜•ì‹ì˜ í•µì‹¬ í†µê³„ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.\nì´ ë„êµ¬ëŠ” `get_apt_rent_data`ë¥¼ í†µí•´ ì–»ì€ ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì…ë ¥ë°›ì•„ ì‘ë™í•˜ë©°, 'ì „ì„¸'ì™€ 'ì›”ì„¸'ë¥¼ ìë™ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ê°ê°ì— ëŒ€í•œ ìƒì„¸ í†µê³„ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\në¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì£¼ìš” í†µê³„ ì§€í‘œë“¤ì„ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ì°¨íŠ¸ë‚˜ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n\n- **ê±°ë˜ ìœ í˜• ë¶„í¬**: ì „ì²´ ê±°ë˜ ì¤‘ ì „ì„¸ì™€ ì›”ì„¸ì˜ ë¹„ì¤‘ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n- **ì „ì„¸ ë¶„ì„**: ë³´ì¦ê¸ˆì— ëŒ€í•œ í‰ê· /ì¤‘ìœ„/ìµœê³ /ìµœì € ê°€ê²© ë° ì£¼ìš” ê±°ë˜ ì‚¬ë¡€ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ë‹¨ì§€ë³„ í†µê³„ë„ í¬í•¨ë©ë‹ˆë‹¤.\n- **ì›”ì„¸ ë¶„ì„**: ë³´ì¦ê¸ˆ ë° ì›”ì„¸ ê°ê°ì— ëŒ€í•œ í‰ê· /ì¤‘ìœ„ ê°€ê²© í†µê³„ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ë‹¨ì§€ë³„ í†µê³„ë„ í¬í•¨ë©ë‹ˆë‹¤.\n\nArguments:\n- file_path (str, required): `get_apt_rent_data` ë„êµ¬ë¡œ ìƒì„±ëœ `raw.data.json` ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œ.\n\nReturns:\n- ì „ì„¸ì™€ ì›”ì„¸ë¡œ êµ¬ë¶„ëœ í†µê³„ ë¶„ì„ ê²°ê³¼ê°€ ë‹´ê¸´ ìƒì„¸í•œ JSON ë¬¸ìì—´.""",
    tags={"ì•„íŒŒíŠ¸", "í†µê³„", "ë¶„ì„", "ë¦¬í¬íŠ¸", "ì „ì„¸", "ì›”ì„¸", "ì „ì›”ì„¸", "ì‹¤ê±°ë˜ê°€"}
)
def analyze_apartment_rent(file_path: str, ctx: Optional[Any] = None) -> TextContent:
    def call(context):
        try:
            p = Path(file_path)
            if not p.exists():
                return json.dumps({"error": f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}"}, ensure_ascii=False)
            cache_path = get_summary_cache_path(p, property_type="apartment", trade_type="rent")
            if cache_path.exists():
                if cache_path.stat().st_mtime > p.stat().st_mtime:
                    logger.info(f"âœ… ìœ íš¨í•œ ì•„íŒŒíŠ¸ ì „ì›”ì„¸ ìºì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {cache_path}")
                    with open(cache_path, 'r', encoding='utf-8') as f:
                        return f.read()
            logger.info(f"ğŸ”„ ìƒˆë¡œìš´ ì•„íŒŒíŠ¸ ì „ì›”ì„¸ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤: {file_path}")
            df = pd.read_json(p, lines=True)
            summary_data = analyze_apartment_rent_data(df)
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=4, default=default_serializer)
            summary_data["summary_cached_path"] = str(cache_path)
            return json.dumps(summary_data, ensure_ascii=False, indent=4, default=default_serializer)
        except Exception as e:
            logger.error(f"ì•„íŒŒíŠ¸ ì „ì›”ì„¸ ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return json.dumps({"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}, ensure_ascii=False)
    result = with_context(ctx, "analyze_apartment_rent", call)
    return TextContent(type="text", text=result)

# --- ì˜¤í”¼ìŠ¤í…” ë§¤ë§¤ ë¶„ì„ ---

def analyze_officetel_trade_data(df: pd.DataFrame) -> Dict[str, Any]:
    """DataFrameì„ ë°›ì•„ ì˜¤í”¼ìŠ¤í…” ë§¤ë§¤ í†µê³„ë¥¼ ë¶„ì„í•˜ê³  ì˜ë¬¸ keyì™€ ë‹¨ìœ„ê°€ í¬í•¨ëœ ê°’ìœ¼ë¡œ JSONì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if df.empty:
        return {"error": "No data to analyze."}

    # --- ë°ì´í„° ì „ì²˜ë¦¬ ---
    df['ê±°ë˜ê¸ˆì•¡_num'] = to_numeric(get_col_from_df(df, 'ê±°ë˜ê¸ˆì•¡', 'dealAmount', 'dealAmountNum'))
    df['ì „ìš©ë©´ì _num'] = to_numeric(get_col_from_df(df, 'ì „ìš©ë©´ì ', 'area', 'excluUseAr', 'areaNum'))
    df['ê±´ì¶•ë…„ë„_num'] = to_numeric(get_col_from_df(df, 'ê±´ì¶•ë…„ë„', 'buildYear', 'buildYearNum'))
    df['ê³„ì•½ë…„ì›”_num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ë…„ì›”'))
    df['ê³„ì•½ì¼_num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ì¼'))

    df.dropna(subset=['ê±°ë˜ê¸ˆì•¡_num', 'ì „ìš©ë©´ì _num'], inplace=True)
    df = df[df['ì „ìš©ë©´ì _num'] > 0].copy()
    if df.empty:
        return {"error": "No valid transaction data after cleaning."}

    df['í‰ë‹¹ê°€_ë§Œì›'] = (df['ê±°ë˜ê¸ˆì•¡_num'] / df['ì „ìš©ë©´ì _num']) * 3.305785
    current_year = datetime.now().year
    df['ê±´ë¬¼ì—°ë ¹'] = current_year - df['ê±´ì¶•ë…„ë„_num']

    def as_value_unit_m(v): return as_value_unit(v, "ë§Œì›")
    def as_value_unit_per_pyeong(v): return as_value_unit(v, "ë§Œì›/í‰")

    # --- 1. ì¢…í•© í†µê³„ ---
    total_count = len(df)
    total_value = df['ê±°ë˜ê¸ˆì•¡_num'].sum()
    overall_stats = {
        "totalTransactionCount": total_count,
        "totalTransactionValue": as_value_unit_m(total_value),
    }

    # --- 2. ê°€ê²© ìˆ˜ì¤€ í†µê³„ ---
    price_stats_raw = df['ê±°ë˜ê¸ˆì•¡_num'].agg(['mean', 'median', 'max', 'min'])
    price_stats = {
        "overallAveragePrice": as_value_unit_m(price_stats_raw['mean']),
        "overallMedianPrice": as_value_unit_m(price_stats_raw['median']),
        "overallHighestPrice": as_value_unit_m(price_stats_raw['max']),
        "overallLowestPrice": as_value_unit_m(price_stats_raw['min']),
        "representativeDeals": {
            "highestPriceDeal": clean_deal_for_display(df.loc[df['ê±°ë˜ê¸ˆì•¡_num'].idxmax()]),
            "lowestPriceDeal": clean_deal_for_display(df.loc[df['ê±°ë˜ê¸ˆì•¡_num'].idxmin()]),
            "dealClosestToAverage": clean_deal_for_display(df.loc[(df['ê±°ë˜ê¸ˆì•¡_num'] - price_stats_raw['mean']).abs().idxmin()]),
            "dealClosestToMedian": clean_deal_for_display(df.loc[(df['ê±°ë˜ê¸ˆì•¡_num'] - price_stats_raw['median']).abs().idxmin()])
        }
    }

    # --- 3. ë‹¨ìœ„ ë©´ì ë‹¹ ê°€ê²© í†µê³„ ---
    price_per_area_stats = {
        "overallAveragePricePerPyeong": as_value_unit_per_pyeong(df['í‰ë‹¹ê°€_ë§Œì›'].mean()),
        "overallMedianPricePerPyeong": as_value_unit_per_pyeong(df['í‰ë‹¹ê°€_ë§Œì›'].median()),
    }

    # --- 4. ë‹¨ì§€ë³„/ì…ì§€ë³„ í†µê³„ ---
    complex_col = get_col_from_df(df, 'ì˜¤í”¼ìŠ¤í…”', 'ì˜¤í”¼ìŠ¤í…”ëª…', 'officetelName')
    location_col = get_col_from_df(df, 'ë²•ì •ë™', 'umdNm', 'dong')
    def get_grouped_stats(group_col):
        stats = {}
        if group_col.notna().any():
            summary_raw = df.groupby(group_col).agg(
                Count=('ê±°ë˜ê¸ˆì•¡_num', 'size'),
                Mean_Price=('ê±°ë˜ê¸ˆì•¡_num', 'mean'),
                Median_Price=('ê±°ë˜ê¸ˆì•¡_num', 'median'),
                Mean_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'mean'),
                Median_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'median')
            )
            for name, data in summary_raw.to_dict('index').items():
                stats[name] = {
                    "transactionCount": int(data['Count']),
                    "averagePrice": as_value_unit_m(data['Mean_Price']),
                    "medianPrice": as_value_unit_m(data['Median_Price']),
                    "averagePricePerPyeong": as_value_unit_per_pyeong(data['Mean_PPA']),
                    "medianPricePerPyeong": as_value_unit_per_pyeong(data['Median_PPA']),
                }
        return stats

    complex_stats = get_grouped_stats(complex_col)
    location_stats = get_grouped_stats(location_col)

    return {
        "overallStatistics": overall_stats,
        "priceLevelStatistics": price_stats,
        "pricePerAreaStatistics": price_per_area_stats,
        "statisticsByOfficetelComplex": complex_stats,
        "statisticsByDong": location_stats,
        "notes": "PPA (Price Per Pyeong) is calculated based on the 'exclusive use area'."
    }

@mcp.tool(
    name="analyze_officetel_trade",
    description="""ì˜¤í”¼ìŠ¤í…” ë§¤ë§¤ ì‹¤ê±°ë˜ ë°ì´í„° íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ì›”ê°„ ë¦¬í¬íŠ¸ í˜•ì‹ì˜ í•µì‹¬ í†µê³„ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.\nì´ ë„êµ¬ëŠ” `get_officetel_trade_data`ë¥¼ í†µí•´ ì–»ì€ ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì…ë ¥ë°›ì•„ ì‘ë™í•©ë‹ˆë‹¤.\nì¢…í•© í†µê³„, ê°€ê²© ìˆ˜ì¤€, í‰ë‹¹ê°€, ë‹¨ì§€ë³„, ë™ë³„ ë“± ë‹¤ê°ì ì¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\në¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì£¼ìš” í†µê³„ ì§€í‘œë“¤ì„ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ì°¨íŠ¸ë‚˜ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n\nArguments:\n- file_path (str, required): `get_officetel_trade_data` ë„êµ¬ë¡œ ìƒì„±ëœ `raw.data.json` ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œ.\n\nReturns:\n- í†µê³„ ë¶„ì„ ê²°ê³¼ê°€ ë‹´ê¸´ ìƒì„¸í•œ JSON ë¬¸ìì—´.""",
    tags={"ì˜¤í”¼ìŠ¤í…”", "í†µê³„", "ë¶„ì„", "ë¦¬í¬íŠ¸", "ë§¤ë§¤", "ì‹¤ê±°ë˜ê°€"}
)
def analyze_officetel_trade(file_path: str, ctx: Optional[Any] = None) -> TextContent:
    def call(context):
        try:
            p = Path(file_path)
            if not p.exists():
                return json.dumps({"error": f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}"}, ensure_ascii=False)
            cache_path = get_summary_cache_path(p, property_type="officetel", trade_type="trade")
            if cache_path.exists():
                if cache_path.stat().st_mtime > p.stat().st_mtime:
                    logger.info(f"âœ… ìœ íš¨í•œ ì˜¤í”¼ìŠ¤í…” ë§¤ë§¤ ìºì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {cache_path}")
                    with open(cache_path, 'r', encoding='utf-8') as f:
                        return f.read()
            logger.info(f"ğŸ”„ ìƒˆë¡œìš´ ì˜¤í”¼ìŠ¤í…” ë§¤ë§¤ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤: {file_path}")
            df = pd.read_json(p, lines=True)
            summary_data = analyze_officetel_trade_data(df)
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=4, default=default_serializer)
            summary_data["summary_cached_path"] = str(cache_path)
            return json.dumps(summary_data, ensure_ascii=False, indent=4, default=default_serializer)
        except Exception as e:
            logger.error(f"ì˜¤í”¼ìŠ¤í…” ë§¤ë§¤ ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return json.dumps({"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}, ensure_ascii=False)
    result = with_context(ctx, "analyze_officetel_trade", call)
    return TextContent(type="text", text=result)

# --- ì˜¤í”¼ìŠ¤í…” ì „ì›”ì„¸ ë¶„ì„ ---
def analyze_officetel_rent_data(df: pd.DataFrame) -> Dict[str, Any]:
    """DataFrameì„ ë°›ì•„ ì˜¤í”¼ìŠ¤í…” ì „ì›”ì„¸ í†µê³„ë¥¼ ë¶„ì„í•˜ê³  ì „ì„¸/ì›”ì„¸ë¥¼ êµ¬ë¶„í•˜ì—¬ JSONìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if df.empty:
        return {"error": "No data to analyze."}
    df['ë³´ì¦ê¸ˆ_num'] = to_numeric(get_col_from_df(df, 'ë³´ì¦ê¸ˆì•¡', 'ë³´ì¦ê¸ˆ', 'deposit', 'depositNum'))
    df['ì›”ì„¸_num'] = to_numeric(get_col_from_df(df, 'ì›”ì„¸ì•¡', 'ì›”ì„¸', 'monthlyRent', 'rentFeeNum'))
    df['ì „ìš©ë©´ì _num'] = to_numeric(get_col_from_df(df, 'ì „ìš©ë©´ì ', 'area', 'excluUseAr', 'areaNum'))
    df['ê±´ì¶•ë…„ë„_num'] = to_numeric(get_col_from_df(df, 'ê±´ì¶•ë…„ë„', 'buildYear', 'buildYearNum'))
    df['ê³„ì•½ë…„ì›”_num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ë…„ì›”'))
    df['ê³„ì•½ì¼_num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ì¼'))
    df.dropna(subset=['ë³´ì¦ê¸ˆ_num', 'ì›”ì„¸_num', 'ì „ìš©ë©´ì _num'], inplace=True)
    df = df[df['ì „ìš©ë©´ì _num'] > 0].copy()
    if df.empty:
        return {"error": "No valid transaction data after cleaning."}
    df_jeonse = df[df['ì›”ì„¸_num'] == 0].copy()
    df_wolse = df[df['ì›”ì„¸_num'] > 0].copy()
    def as_value_unit_m(v): return as_value_unit(v, "ë§Œì›")
    def analyze_rent_type(df_rent_type, rent_type_name):
        if df_rent_type.empty:
            return { "totalTransactionCount": 0 }
        stats = { "totalTransactionCount": len(df_rent_type) }
        price_stats_raw = df_rent_type['ë³´ì¦ê¸ˆ_num'].agg(['mean', 'median', 'max', 'min'])
        stats['depositPriceStatistics'] = {
            "averageDeposit": as_value_unit_m(price_stats_raw['mean']),
            "medianDeposit": as_value_unit_m(price_stats_raw['median']),
            "highestDeposit": as_value_unit_m(price_stats_raw['max']),
            "lowestDeposit": as_value_unit_m(price_stats_raw['min']),
            "representativeDeals": {
                "highestDepositDeal": clean_deal_for_display(df_rent_type.loc[df_rent_type['ë³´ì¦ê¸ˆ_num'].idxmax()]),
                "lowestDepositDeal": clean_deal_for_display(df_rent_type.loc[df_rent_type['ë³´ì¦ê¸ˆ_num'].idxmin()]),
            }
        }
        if rent_type_name == 'wolse':
            wolse_stats_raw = df_rent_type['ì›”ì„¸_num'].agg(['mean', 'median', 'max', 'min'])
            stats['monthlyRentStatistics'] = {
                "averageMonthlyRent": as_value_unit_m(wolse_stats_raw['mean']),
                "medianMonthlyRent": as_value_unit_m(wolse_stats_raw['median']),
            }
        building_col = get_col_from_df(df_rent_type, 'ì˜¤í”¼ìŠ¤í…”', 'ì˜¤í”¼ìŠ¤í…”ëª…', 'officetelName')
        if building_col.notna().any():
            stats['statisticsByOfficetelComplex'] = df_rent_type.groupby(building_col).agg(
                transactionCount=('ë³´ì¦ê¸ˆ_num', 'size'),
                averageDeposit=('ë³´ì¦ê¸ˆ_num', 'mean')
            ).apply(lambda x: x.astype(int) if x.name == 'transactionCount' else as_value_unit_m(x)).to_dict('index')
        return stats
    jeonse_analysis = analyze_rent_type(df_jeonse, 'jeonse')
    wolse_analysis = analyze_rent_type(df_wolse, 'wolse')
    return {
        "transactionTypeDistribution": {
            "jeonse_count": len(df_jeonse),
            "wolse_count": len(df_wolse),
        },
        "jeonseAnalysis": jeonse_analysis,
        "wolseAnalysis": wolse_analysis,
        "notes": "Statistics are separated by transaction type (Jeonse vs. Wolse). ê³„ì•½ë©´ì  ê¸°ì¤€ìœ¼ë¡œ ì§‘ê³„í•©ë‹ˆë‹¤."
    }

@mcp.tool(
    name="analyze_officetel_rent",
    description="""ì˜¤í”¼ìŠ¤í…” ì „ì›”ì„¸ ì‹¤ê±°ë˜ ë°ì´í„° íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ì›”ê°„ ë¦¬í¬íŠ¸ í˜•ì‹ì˜ í•µì‹¬ í†µê³„ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.\nì´ ë„êµ¬ëŠ” `get_officetel_rent_data`ë¥¼ í†µí•´ ì–»ì€ ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì…ë ¥ë°›ì•„ ì‘ë™í•˜ë©°, 'ì „ì„¸'ì™€ 'ì›”ì„¸'ë¥¼ ìë™ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ê°ê°ì— ëŒ€í•œ ìƒì„¸ í†µê³„ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\në¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì£¼ìš” í†µê³„ ì§€í‘œë“¤ì„ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ì°¨íŠ¸ë‚˜ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n\n- **ê±°ë˜ ìœ í˜• ë¶„í¬**: ì „ì²´ ê±°ë˜ ì¤‘ ì „ì„¸ì™€ ì›”ì„¸ì˜ ë¹„ì¤‘ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n- **ì „ì„¸ ë¶„ì„**: ë³´ì¦ê¸ˆì— ëŒ€í•œ í‰ê· /ì¤‘ìœ„/ìµœê³ /ìµœì € ê°€ê²© ë° ì£¼ìš” ê±°ë˜ ì‚¬ë¡€ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ê±´ë¬¼ëª…ë³„ í†µê³„ë„ í¬í•¨ë©ë‹ˆë‹¤.\n- **ì›”ì„¸ ë¶„ì„**: ë³´ì¦ê¸ˆ ë° ì›”ì„¸ ê°ê°ì— ëŒ€í•œ í‰ê· /ì¤‘ìœ„ ê°€ê²© í†µê³„ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ê±´ë¬¼ëª…ë³„ í†µê³„ë„ í¬í•¨ë©ë‹ˆë‹¤.\n\nArguments:\n- file_path (str, required): `get_officetel_rent_data` ë„êµ¬ë¡œ ìƒì„±ëœ `raw.data.json` ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œ.\n\nReturns:\n- ì „ì„¸ì™€ ì›”ì„¸ë¡œ êµ¬ë¶„ëœ í†µê³„ ë¶„ì„ ê²°ê³¼ê°€ ë‹´ê¸´ ìƒì„¸í•œ JSON ë¬¸ìì—´.""",
    tags={"ì˜¤í”¼ìŠ¤í…”", "í†µê³„", "ë¶„ì„", "ë¦¬í¬íŠ¸", "ì „ì„¸", "ì›”ì„¸", "ì „ì›”ì„¸", "ì‹¤ê±°ë˜ê°€"}
)
def analyze_officetel_rent(file_path: str, ctx: Optional[Any] = None) -> TextContent:
    def call(context):
        try:
            p = Path(file_path)
            if not p.exists():
                return json.dumps({"error": f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}"}, ensure_ascii=False)
            cache_path = get_summary_cache_path(p, property_type="officetel", trade_type="rent")
            if cache_path.exists():
                if cache_path.stat().st_mtime > p.stat().st_mtime:
                    logger.info(f"âœ… ìœ íš¨í•œ ì˜¤í”¼ìŠ¤í…” ì „ì›”ì„¸ ìºì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {cache_path}")
                    with open(cache_path, 'r', encoding='utf-8') as f:
                        return f.read()
            logger.info(f"ğŸ”„ ìƒˆë¡œìš´ ì˜¤í”¼ìŠ¤í…” ì „ì›”ì„¸ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤: {file_path}")
            df = pd.read_json(p, lines=True)
            summary_data = analyze_officetel_rent_data(df)
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=4, default=default_serializer)
            summary_data["summary_cached_path"] = str(cache_path)
            return json.dumps(summary_data, ensure_ascii=False, indent=4, default=default_serializer)
        except Exception as e:
            logger.error(f"ì˜¤í”¼ìŠ¤í…” ì „ì›”ì„¸ ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return json.dumps({"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}, ensure_ascii=False)
    result = with_context(ctx, "analyze_officetel_rent", call)
    return TextContent(type="text", text=result)

def analyze_single_detached_trade_data(df: pd.DataFrame) -> Dict[str, Any]:
    """DataFrameì„ ë°›ì•„ ë‹¨ë…/ë‹¤ê°€êµ¬ ë§¤ë§¤ í†µê³„ë¥¼ ë¶„ì„í•˜ê³  ì˜ë¬¸ keyì™€ ë‹¨ìœ„ê°€ í¬í•¨ëœ ê°’ìœ¼ë¡œ JSONì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if df.empty:
        return {"error": "No data to analyze."}
    # --- ë°ì´í„° ì „ì²˜ë¦¬ ---
    df['ê±°ë˜ê¸ˆì•¡_num'] = to_numeric(get_col_from_df(df, 'ê±°ë˜ê¸ˆì•¡', 'dealAmount', 'dealAmountNum'))
    # Fix: Use totalFloorAr if areaNum/YUA are null
    df['ì—°ë©´ì _num'] = to_numeric(get_col_from_df(df, 'ì—°ë©´ì ', 'YUA', 'totalFloorAr', 'areaNum'))
    df['ê±´ì¶•ë…„ë„_num'] = to_numeric(get_col_from_df(df, 'ê±´ì¶•ë…„ë„', 'buildYear', 'buildYearNum'))
    df['ê³„ì•½ë…„ì›”_num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ë…„ì›”'))
    df['ê³„ì•½ì¼_num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ì¼'))
    df.dropna(subset=['ê±°ë˜ê¸ˆì•¡_num', 'ì—°ë©´ì _num'], inplace=True)
    df = df[df['ì—°ë©´ì _num'] > 0].copy()
    if df.empty:
        return {"error": "No valid transaction data after cleaning."}
    df['í‰ë‹¹ê°€_ë§Œì›'] = (df['ê±°ë˜ê¸ˆì•¡_num'] / df['ì—°ë©´ì _num']) * 3.305785
    current_year = datetime.now().year
    df['ê±´ë¬¼ì—°ë ¹'] = current_year - df['ê±´ì¶•ë…„ë„_num']
    def as_value_unit_m(v): return as_value_unit(v, "ë§Œì›")
    def as_value_unit_per_pyeong(v): return as_value_unit(v, "ë§Œì›/í‰")
    # --- 1. ì¢…í•© í†µê³„ ---
    total_count = len(df)
    total_value = df['ê±°ë˜ê¸ˆì•¡_num'].sum()
    overall_stats = {
        "totalTransactionCount": total_count,
        "totalTransactionValue": as_value_unit_m(total_value),
    }
    # --- 2. ê°€ê²© ìˆ˜ì¤€ í†µê³„ ---
    price_stats_raw = df['ê±°ë˜ê¸ˆì•¡_num'].agg(['mean', 'median', 'max', 'min'])
    price_stats = {
        "overallAveragePrice": as_value_unit_m(price_stats_raw['mean']),
        "overallMedianPrice": as_value_unit_m(price_stats_raw['median']),
        "overallHighestPrice": as_value_unit_m(price_stats_raw['max']),
        "overallLowestPrice": as_value_unit_m(price_stats_raw['min']),
        "representativeDeals": {
            "highestPriceDeal": clean_deal_for_display(df.loc[df['ê±°ë˜ê¸ˆì•¡_num'].idxmax()]),
            "lowestPriceDeal": clean_deal_for_display(df.loc[df['ê±°ë˜ê¸ˆì•¡_num'].idxmin()]),
            "dealClosestToAverage": clean_deal_for_display(df.loc[(df['ê±°ë˜ê¸ˆì•¡_num'] - price_stats_raw['mean']).abs().idxmin()]),
            "dealClosestToMedian": clean_deal_for_display(df.loc[(df['ê±°ë˜ê¸ˆì•¡_num'] - price_stats_raw['median']).abs().idxmin()])
        }
    }
    # --- 3. ë‹¨ìœ„ ë©´ì ë‹¹ ê°€ê²© í†µê³„ ---
    price_per_area_stats = {
        "overallAveragePricePerPyeong": as_value_unit_per_pyeong(df['í‰ë‹¹ê°€_ë§Œì›'].mean()),
        "overallMedianPricePerPyeong": as_value_unit_per_pyeong(df['í‰ë‹¹ê°€_ë§Œì›'].median()),
    }
    # --- 4. ê±´ë¬¼ëª…/ë™ë³„ í†µê³„ ---
    building_col = get_col_from_df(df, 'ê±´ë¬¼ëª…', 'buildingName')
    location_col = get_col_from_df(df, 'ë²•ì •ë™', 'umdNm', 'dong')
    def get_grouped_stats(group_col):
        stats = {}
        if group_col.notna().any():
            summary_raw = df.groupby(group_col).agg(
                Count=('ê±°ë˜ê¸ˆì•¡_num', 'size'),
                Mean_Price=('ê±°ë˜ê¸ˆì•¡_num', 'mean'),
                Median_Price=('ê±°ë˜ê¸ˆì•¡_num', 'median'),
                Mean_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'mean'),
                Median_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'median')
            )
            for name, data in summary_raw.to_dict('index').items():
                stats[name] = {
                    "transactionCount": int(data['Count']),
                    "averagePrice": as_value_unit_m(data['Mean_Price']),
                    "medianPrice": as_value_unit_m(data['Median_Price']),
                    "averagePricePerPyeong": as_value_unit_per_pyeong(data['Mean_PPA']),
                    "medianPricePerPyeong": as_value_unit_per_pyeong(data['Median_PPA']),
                }
        return stats
    building_stats = get_grouped_stats(building_col)
    location_stats = get_grouped_stats(location_col)
    return {
        "overallStatistics": overall_stats,
        "priceLevelStatistics": price_stats,
        "pricePerAreaStatistics": price_per_area_stats,
        "statisticsByBuilding": building_stats,
        "statisticsByDong": location_stats,
        "notes": "PPA (Price Per Pyeong) is calculated based on the 'gross floor area'. ë‹¨ë…/ë‹¤ê°€êµ¬ëŠ” ì—°ë©´ì (totalFloorAr) ê¸°ì¤€ìœ¼ë¡œ ì§‘ê³„í•©ë‹ˆë‹¤."
    }

@mcp.tool(
    name="analyze_single_detached_house_trade",
    description="""ë‹¨ë…/ë‹¤ê°€êµ¬ ë§¤ë§¤ ì‹¤ê±°ë˜ ë°ì´í„° íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ì›”ê°„ ë¦¬í¬íŠ¸ í˜•ì‹ì˜ í•µì‹¬ í†µê³„ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.\nì´ ë„êµ¬ëŠ” `get_single_detached_house_trade_data`ë¥¼ í†µí•´ ì–»ì€ ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì…ë ¥ë°›ì•„ ì‘ë™í•©ë‹ˆë‹¤.\nì¢…í•© í†µê³„, ê°€ê²© ìˆ˜ì¤€, í‰ë‹¹ê°€, ê±´ë¬¼ëª…ë³„, ë™ë³„ ë“± ë‹¤ê°ì ì¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\në¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì£¼ìš” í†µê³„ ì§€í‘œë“¤ì„ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ì°¨íŠ¸ë‚˜ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n\nArguments:\n- file_path (str, required): `get_single_detached_house_trade_data` ë„êµ¬ë¡œ ìƒì„±ëœ `raw.data.json` ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œ.\n\nReturns:\n- í†µê³„ ë¶„ì„ ê²°ê³¼ê°€ ë‹´ê¸´ ìƒì„¸í•œ JSON ë¬¸ìì—´.""",
    tags={"ë‹¨ë…ë‹¤ê°€êµ¬", "í†µê³„", "ë¶„ì„", "ë¦¬í¬íŠ¸", "ë§¤ë§¤", "ì‹¤ê±°ë˜ê°€"}
)
def analyze_single_detached_house_trade(file_path: str, ctx: Optional[Any] = None) -> TextContent:
    def call(context):
        try:
            p = Path(file_path)
            if not p.exists():
                return json.dumps({"error": f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}"}, ensure_ascii=False)
            cache_path = get_summary_cache_path(p, property_type="single_detached", trade_type="trade")
            if cache_path.exists():
                if cache_path.stat().st_mtime > p.stat().st_mtime:
                    logger.info(f"âœ… ìœ íš¨í•œ ë‹¨ë…/ë‹¤ê°€êµ¬ ë§¤ë§¤ ìºì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {cache_path}")
                    with open(cache_path, 'r', encoding='utf-8') as f:
                        return f.read()
            logger.info(f"ğŸ”„ ìƒˆë¡œìš´ ë‹¨ë…/ë‹¤ê°€êµ¬ ë§¤ë§¤ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤: {file_path}")
            df = pd.read_json(p, lines=True)
            summary_data = analyze_single_detached_trade_data(df)
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=4, default=default_serializer)
            summary_data["summary_cached_path"] = str(cache_path)
            return json.dumps(summary_data, ensure_ascii=False, indent=4, default=default_serializer)
        except Exception as e:
            logger.error(f"ë‹¨ë…/ë‹¤ê°€êµ¬ ë§¤ë§¤ ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return json.dumps({"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}, ensure_ascii=False)
    result = with_context(ctx, "analyze_single_detached_house_trade", call)
    return TextContent(type="text", text=result)

# --- ë‹¨ë…/ë‹¤ê°€êµ¬ ì „ì›”ì„¸ ë¶„ì„ ---
def analyze_single_detached_rent_data(df: pd.DataFrame) -> Dict[str, Any]:
    """DataFrameì„ ë°›ì•„ ë‹¨ë…/ë‹¤ê°€êµ¬ ì „ì›”ì„¸ í†µê³„ë¥¼ ë¶„ì„í•˜ê³  ì „ì„¸/ì›”ì„¸ë¥¼ êµ¬ë¶„í•˜ì—¬ JSONìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if df.empty:
        return {"error": "No data to analyze."}
    df['ë³´ì¦ê¸ˆ_num'] = to_numeric(get_col_from_df(df, 'ë³´ì¦ê¸ˆì•¡', 'ë³´ì¦ê¸ˆ', 'deposit', 'depositNum'))
    df['ì›”ì„¸_num'] = to_numeric(get_col_from_df(df, 'ì›”ì„¸ì•¡', 'ì›”ì„¸', 'monthlyRent', 'rentFeeNum'))
    df['ê³„ì•½ë©´ì _num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ë©´ì ', 'contractArea', 'areaNum'))
    df['ê±´ì¶•ë…„ë„_num'] = to_numeric(get_col_from_df(df, 'ê±´ì¶•ë…„ë„', 'buildYear', 'buildYearNum'))
    df['ê³„ì•½ë…„ì›”_num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ë…„ì›”'))
    df['ê³„ì•½ì¼_num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ì¼'))
    df.dropna(subset=['ë³´ì¦ê¸ˆ_num', 'ì›”ì„¸_num', 'ê³„ì•½ë©´ì _num'], inplace=True)
    df = df[df['ê³„ì•½ë©´ì _num'] > 0].copy()
    if df.empty:
        return {"error": "No valid transaction data after cleaning."}
    df_jeonse = df[df['ì›”ì„¸_num'] == 0].copy()
    df_wolse = df[df['ì›”ì„¸_num'] > 0].copy()
    def as_value_unit_m(v): return as_value_unit(v, "ë§Œì›")
    def analyze_rent_type(df_rent_type, rent_type_name):
        if df_rent_type.empty:
            return { "totalTransactionCount": 0 }
        stats = { "totalTransactionCount": len(df_rent_type) }
        price_stats_raw = df_rent_type['ë³´ì¦ê¸ˆ_num'].agg(['mean', 'median', 'max', 'min'])
        stats['depositPriceStatistics'] = {
            "averageDeposit": as_value_unit_m(price_stats_raw['mean']),
            "medianDeposit": as_value_unit_m(price_stats_raw['median']),
            "highestDeposit": as_value_unit_m(price_stats_raw['max']),
            "lowestDeposit": as_value_unit_m(price_stats_raw['min']),
            "representativeDeals": {
                "highestDepositDeal": clean_deal_for_display(df_rent_type.loc[df_rent_type['ë³´ì¦ê¸ˆ_num'].idxmax()]),
                "lowestDepositDeal": clean_deal_for_display(df_rent_type.loc[df_rent_type['ë³´ì¦ê¸ˆ_num'].idxmin()]),
            }
        }
        if rent_type_name == 'wolse':
            wolse_stats_raw = df_rent_type['ì›”ì„¸_num'].agg(['mean', 'median', 'max', 'min'])
            stats['monthlyRentStatistics'] = {
                "averageMonthlyRent": as_value_unit_m(wolse_stats_raw['mean']),
                "medianMonthlyRent": as_value_unit_m(wolse_stats_raw['median']),
            }
        building_col = get_col_from_df(df_rent_type, 'ê±´ë¬¼ëª…', 'buildingName')
        if building_col.notna().any():
            stats['statisticsByBuilding'] = df_rent_type.groupby(building_col).agg(
                transactionCount=('ë³´ì¦ê¸ˆ_num', 'size'),
                averageDeposit=('ë³´ì¦ê¸ˆ_num', 'mean')
            ).apply(lambda x: x.astype(int) if x.name == 'transactionCount' else as_value_unit_m(x)).to_dict('index')
        return stats
    jeonse_analysis = analyze_rent_type(df_jeonse, 'jeonse')
    wolse_analysis = analyze_rent_type(df_wolse, 'wolse')
    return {
        "transactionTypeDistribution": {
            "jeonse_count": len(df_jeonse),
            "wolse_count": len(df_wolse),
        },
        "jeonseAnalysis": jeonse_analysis,
        "wolseAnalysis": wolse_analysis,
        "notes": "Statistics are separated by transaction type (Jeonse vs. Wolse). ê³„ì•½ë©´ì  ê¸°ì¤€ìœ¼ë¡œ ì§‘ê³„í•©ë‹ˆë‹¤."
    }

@mcp.tool(
    name="analyze_single_detached_house_rent",
    description="""ë‹¨ë…/ë‹¤ê°€êµ¬ ì „ì›”ì„¸ ì‹¤ê±°ë˜ ë°ì´í„° íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ì›”ê°„ ë¦¬í¬íŠ¸ í˜•ì‹ì˜ í•µì‹¬ í†µê³„ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.\nì´ ë„êµ¬ëŠ” `get_sh_rent_data`ë¥¼ í†µí•´ ì–»ì€ ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì…ë ¥ë°›ì•„ ì‘ë™í•˜ë©°, 'ì „ì„¸'ì™€ 'ì›”ì„¸'ë¥¼ ìë™ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ê°ê°ì— ëŒ€í•œ ìƒì„¸ í†µê³„ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\në¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì£¼ìš” í†µê³„ ì§€í‘œë“¤ì„ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ì°¨íŠ¸ë‚˜ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n\n- **ê±°ë˜ ìœ í˜• ë¶„í¬**: ì „ì²´ ê±°ë˜ ì¤‘ ì „ì„¸ì™€ ì›”ì„¸ì˜ ë¹„ì¤‘ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n- **ì „ì„¸ ë¶„ì„**: ë³´ì¦ê¸ˆì— ëŒ€í•œ í‰ê· /ì¤‘ìœ„/ìµœê³ /ìµœì € ê°€ê²© ë° ì£¼ìš” ê±°ë˜ ì‚¬ë¡€ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ê±´ë¬¼ëª…ë³„ í†µê³„ë„ í¬í•¨ë©ë‹ˆë‹¤.\n- **ì›”ì„¸ ë¶„ì„**: ë³´ì¦ê¸ˆ ë° ì›”ì„¸ ê°ê°ì— ëŒ€í•œ í‰ê· /ì¤‘ìœ„ ê°€ê²© í†µê³„ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ê±´ë¬¼ëª…ë³„ í†µê³„ë„ í¬í•¨ë©ë‹ˆë‹¤.\n\nArguments:\n- file_path (str, required): `get_sh_rent_data` ë„êµ¬ë¡œ ìƒì„±ëœ `raw.data.json` ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œ.\n\nReturns:\n- ì „ì„¸ì™€ ì›”ì„¸ë¡œ êµ¬ë¶„ëœ í†µê³„ ë¶„ì„ ê²°ê³¼ê°€ ë‹´ê¸´ ìƒì„¸í•œ JSON ë¬¸ìì—´.""",
    tags={"ë‹¨ë…ë‹¤ê°€êµ¬", "í†µê³„", "ë¶„ì„", "ë¦¬í¬íŠ¸", "ì „ì„¸", "ì›”ì„¸", "ì „ì›”ì„¸", "ì‹¤ê±°ë˜ê°€"}
)
def analyze_single_detached_house_rent(file_path: str, ctx: Optional[Any] = None) -> TextContent:
    def call(context):
        try:
            p = Path(file_path)
            if not p.exists():
                return json.dumps({"error": f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}"}, ensure_ascii=False)
            cache_path = get_summary_cache_path(p, property_type="single_detached", trade_type="rent")
            if cache_path.exists():
                if cache_path.stat().st_mtime > p.stat().st_mtime:
                    logger.info(f"âœ… ìœ íš¨í•œ ë‹¨ë…/ë‹¤ê°€êµ¬ ì „ì›”ì„¸ ìºì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {cache_path}")
                    with open(cache_path, 'r', encoding='utf-8') as f:
                        return f.read()
            logger.info(f"ğŸ”„ ìƒˆë¡œìš´ ë‹¨ë…/ë‹¤ê°€êµ¬ ì „ì›”ì„¸ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤: {file_path}")
            df = pd.read_json(p, lines=True)
            summary_data = analyze_single_detached_rent_data(df)
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=4, default=default_serializer)
            summary_data["summary_cached_path"] = str(cache_path)
            return json.dumps(summary_data, ensure_ascii=False, indent=4, default=default_serializer)
        except Exception as e:
            logger.error(f"ë‹¨ë…/ë‹¤ê°€êµ¬ ì „ì›”ì„¸ ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return json.dumps({"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}, ensure_ascii=False)
    result = with_context(ctx, "analyze_single_detached_house_rent", call)
    return TextContent(type="text", text=result)

def analyze_row_house_trade_data(df: pd.DataFrame) -> Dict[str, Any]:
    """DataFrameì„ ë°›ì•„ ì—°ë¦½ë‹¤ì„¸ëŒ€ ë§¤ë§¤ í†µê³„ë¥¼ ë¶„ì„í•˜ê³  ì˜ë¬¸ keyì™€ ë‹¨ìœ„ê°€ í¬í•¨ëœ ê°’ìœ¼ë¡œ JSONì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if df.empty:
        return {"error": "No data to analyze."}
    # --- ë°ì´í„° ì „ì²˜ë¦¬ ---
    df['ê±°ë˜ê¸ˆì•¡_num'] = to_numeric(get_col_from_df(df, 'ê±°ë˜ê¸ˆì•¡', 'dealAmount', 'dealAmountNum'))
    df['ì „ìš©ë©´ì _num'] = to_numeric(get_col_from_df(df, 'ì „ìš©ë©´ì ', 'area', 'excluUseAr', 'areaNum'))
    df['ê±´ì¶•ë…„ë„_num'] = to_numeric(get_col_from_df(df, 'ê±´ì¶•ë…„ë„', 'buildYear', 'buildYearNum'))
    df['ê³„ì•½ë…„ì›”_num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ë…„ì›”'))
    df['ê³„ì•½ì¼_num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ì¼'))
    df.dropna(subset=['ê±°ë˜ê¸ˆì•¡_num', 'ì „ìš©ë©´ì _num'], inplace=True)
    df = df[df['ì „ìš©ë©´ì _num'] > 0].copy()
    if df.empty:
        return {"error": "No valid transaction data after cleaning."}
    df['í‰ë‹¹ê°€_ë§Œì›'] = (df['ê±°ë˜ê¸ˆì•¡_num'] / df['ì „ìš©ë©´ì _num']) * 3.305785
    current_year = datetime.now().year
    df['ê±´ë¬¼ì—°ë ¹'] = current_year - df['ê±´ì¶•ë…„ë„_num']
    def as_value_unit_m(v): return as_value_unit(v, "ë§Œì›")
    def as_value_unit_per_pyeong(v): return as_value_unit(v, "ë§Œì›/í‰")
    # --- 1. ì¢…í•© í†µê³„ ---
    total_count = len(df)
    total_value = df['ê±°ë˜ê¸ˆì•¡_num'].sum()
    overall_stats = {
        "totalTransactionCount": total_count,
        "totalTransactionValue": as_value_unit_m(total_value),
    }
    # --- 2. ê°€ê²© ìˆ˜ì¤€ í†µê³„ ---
    price_stats_raw = df['ê±°ë˜ê¸ˆì•¡_num'].agg(['mean', 'median', 'max', 'min'])
    price_stats = {
        "overallAveragePrice": as_value_unit_m(price_stats_raw['mean']),
        "overallMedianPrice": as_value_unit_m(price_stats_raw['median']),
        "overallHighestPrice": as_value_unit_m(price_stats_raw['max']),
        "overallLowestPrice": as_value_unit_m(price_stats_raw['min']),
        "representativeDeals": {
            "highestPriceDeal": clean_deal_for_display(df.loc[df['ê±°ë˜ê¸ˆì•¡_num'].idxmax()]),
            "lowestPriceDeal": clean_deal_for_display(df.loc[df['ê±°ë˜ê¸ˆì•¡_num'].idxmin()]),
            "dealClosestToAverage": clean_deal_for_display(df.loc[(df['ê±°ë˜ê¸ˆì•¡_num'] - price_stats_raw['mean']).abs().idxmin()]),
            "dealClosestToMedian": clean_deal_for_display(df.loc[(df['ê±°ë˜ê¸ˆì•¡_num'] - price_stats_raw['median']).abs().idxmin()])
        }
    }
    # --- 3. ë‹¨ìœ„ ë©´ì ë‹¹ ê°€ê²© í†µê³„ ---
    price_per_area_stats = {
        "overallAveragePricePerPyeong": as_value_unit_per_pyeong(df['í‰ë‹¹ê°€_ë§Œì›'].mean()),
        "overallMedianPricePerPyeong": as_value_unit_per_pyeong(df['í‰ë‹¹ê°€_ë§Œì›'].median()),
    }
    # --- 4. ì—°ë¦½ë‹¤ì„¸ëŒ€ëª…/ë™ë³„ í†µê³„ ---
    building_col = get_col_from_df(df, 'ì—°ë¦½ë‹¤ì„¸ëŒ€ëª…', 'rowHouseName')
    location_col = get_col_from_df(df, 'ë²•ì •ë™', 'umdNm', 'dong')
    def get_grouped_stats(group_col):
        stats = {}
        if group_col.notna().any():
            summary_raw = df.groupby(group_col).agg(
                Count=('ê±°ë˜ê¸ˆì•¡_num', 'size'),
                Mean_Price=('ê±°ë˜ê¸ˆì•¡_num', 'mean'),
                Median_Price=('ê±°ë˜ê¸ˆì•¡_num', 'median'),
                Mean_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'mean'),
                Median_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'median')
            )
            for name, data in summary_raw.to_dict('index').items():
                stats[name] = {
                    "transactionCount": int(data['Count']),
                    "averagePrice": as_value_unit_m(data['Mean_Price']),
                    "medianPrice": as_value_unit_m(data['Median_Price']),
                    "averagePricePerPyeong": as_value_unit_per_pyeong(data['Mean_PPA']),
                    "medianPricePerPyeong": as_value_unit_per_pyeong(data['Median_PPA']),
                }
        return stats
    building_stats = get_grouped_stats(building_col)
    location_stats = get_grouped_stats(location_col)
    return {
        "overallStatistics": overall_stats,
        "priceLevelStatistics": price_stats,
        "pricePerAreaStatistics": price_per_area_stats,
        "statisticsByRowHouseComplex": building_stats,
        "statisticsByDong": location_stats,
        "notes": "PPA (Price Per Pyeong) is calculated based on the 'exclusive use area'. ì—°ë¦½ë‹¤ì„¸ëŒ€ëŠ” ë‹¨ì§€ëª…(ì—°ë¦½ë‹¤ì„¸ëŒ€ëª…) ê¸°ì¤€ ì§‘ê³„ê°€ ê°€ëŠ¥í•˜ë©°, ë‹¨ì§€ëª…ì´ ì—†ìœ¼ë©´ ë™ë³„ë¡œ ì§‘ê³„í•©ë‹ˆë‹¤."
    }

@mcp.tool(
    name="analyze_row_house_trade",
    description="""ì—°ë¦½ë‹¤ì„¸ëŒ€ ë§¤ë§¤ ì‹¤ê±°ë˜ ë°ì´í„° íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ì›”ê°„ ë¦¬í¬íŠ¸ í˜•ì‹ì˜ í•µì‹¬ í†µê³„ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.\nì´ ë„êµ¬ëŠ” `get_row_house_trade_data`ë¥¼ í†µí•´ ì–»ì€ ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì…ë ¥ë°›ì•„ ì‘ë™í•©ë‹ˆë‹¤.\nì¢…í•© í†µê³„, ê°€ê²© ìˆ˜ì¤€, í‰ë‹¹ê°€, ë‹¨ì§€ë³„, ë™ë³„ ë“± ë‹¤ê°ì ì¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\në¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì£¼ìš” í†µê³„ ì§€í‘œë“¤ì„ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ì°¨íŠ¸ë‚˜ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n\nArguments:\n- file_path (str, required): `get_row_house_trade_data` ë„êµ¬ë¡œ ìƒì„±ëœ `raw.data.json` ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œ.\n\nReturns:\n- í†µê³„ ë¶„ì„ ê²°ê³¼ê°€ ë‹´ê¸´ ìƒì„¸í•œ JSON ë¬¸ìì—´.""",
    tags={"ì—°ë¦½ë‹¤ì„¸ëŒ€", "í†µê³„", "ë¶„ì„", "ë¦¬í¬íŠ¸", "ë§¤ë§¤", "ì‹¤ê±°ë˜ê°€"}
)
def analyze_row_house_trade(file_path: str, ctx: Optional[Any] = None) -> TextContent:
    def call(context):
        try:
            p = Path(file_path)
            if not p.exists():
                return json.dumps({"error": f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}"}, ensure_ascii=False)
            cache_path = get_summary_cache_path(p, property_type="row_house", trade_type="trade")
            if cache_path.exists():
                if cache_path.stat().st_mtime > p.stat().st_mtime:
                    logger.info(f"âœ… ìœ íš¨í•œ ì—°ë¦½ë‹¤ì„¸ëŒ€ ë§¤ë§¤ ìºì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {cache_path}")
                    with open(cache_path, 'r', encoding='utf-8') as f:
                        return f.read()
            logger.info(f"ğŸ”„ ìƒˆë¡œìš´ ì—°ë¦½ë‹¤ì„¸ëŒ€ ë§¤ë§¤ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤: {file_path}")
            df = pd.read_json(p, lines=True)
            summary_data = analyze_row_house_trade_data(df)
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=4, default=default_serializer)
            summary_data["summary_cached_path"] = str(cache_path)
            return json.dumps(summary_data, ensure_ascii=False, indent=4, default=default_serializer)
        except Exception as e:
            logger.error(f"ì—°ë¦½ë‹¤ì„¸ëŒ€ ë§¤ë§¤ ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return json.dumps({"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}, ensure_ascii=False)
    result = with_context(ctx, "analyze_row_house_trade", call)
    return TextContent(type="text", text=result)

def analyze_row_house_rent_data(df: pd.DataFrame) -> Dict[str, Any]:
    """DataFrameì„ ë°›ì•„ ì—°ë¦½ë‹¤ì„¸ëŒ€ ì „ì›”ì„¸ í†µê³„ë¥¼ ë¶„ì„í•˜ê³  ì „ì„¸/ì›”ì„¸ë¥¼ êµ¬ë¶„í•˜ì—¬ JSONìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if df.empty:
        return {"error": "No data to analyze."}
    df['ë³´ì¦ê¸ˆ_num'] = to_numeric(get_col_from_df(df, 'ë³´ì¦ê¸ˆì•¡', 'ë³´ì¦ê¸ˆ', 'deposit', 'depositNum'))
    df['ì›”ì„¸_num'] = to_numeric(get_col_from_df(df, 'ì›”ì„¸ì•¡', 'ì›”ì„¸', 'monthlyRent', 'rentFeeNum'))
    # Fix: Use excluUseAr as fallback for area, before areaNum
    df['ê³„ì•½ë©´ì _num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ë©´ì ', 'contractArea', 'excluUseAr', 'areaNum'))
    df['ê±´ì¶•ë…„ë„_num'] = to_numeric(get_col_from_df(df, 'ê±´ì¶•ë…„ë„', 'buildYear', 'buildYearNum'))
    df['ê³„ì•½ë…„ì›”_num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ë…„ì›”'))
    df['ê³„ì•½ì¼_num'] = to_numeric(get_col_from_df(df, 'ê³„ì•½ì¼'))
    df.dropna(subset=['ë³´ì¦ê¸ˆ_num', 'ì›”ì„¸_num', 'ê³„ì•½ë©´ì _num'], inplace=True)
    df = df[df['ê³„ì•½ë©´ì _num'] > 0].copy()
    if df.empty:
        return {"error": "No valid transaction data after cleaning."}
    df_jeonse = df[df['ì›”ì„¸_num'] == 0].copy()
    df_wolse = df[df['ì›”ì„¸_num'] > 0].copy()
    def as_value_unit_m(v): return as_value_unit(v, "ë§Œì›")
    def analyze_rent_type(df_rent_type, rent_type_name):
        if df_rent_type.empty:
            return { "totalTransactionCount": 0 }
        stats = { "totalTransactionCount": len(df_rent_type) }
        price_stats_raw = df_rent_type['ë³´ì¦ê¸ˆ_num'].agg(['mean', 'median', 'max', 'min'])
        stats['depositPriceStatistics'] = {
            "averageDeposit": as_value_unit_m(price_stats_raw['mean']),
            "medianDeposit": as_value_unit_m(price_stats_raw['median']),
            "highestDeposit": as_value_unit_m(price_stats_raw['max']),
            "lowestDeposit": as_value_unit_m(price_stats_raw['min']),
            "representativeDeals": {
                "highestDepositDeal": clean_deal_for_display(df_rent_type.loc[df_rent_type['ë³´ì¦ê¸ˆ_num'].idxmax()]),
                "lowestDepositDeal": clean_deal_for_display(df_rent_type.loc[df_rent_type['ë³´ì¦ê¸ˆ_num'].idxmin()]),
            }
        }
        if rent_type_name == 'wolse':
            wolse_stats_raw = df_rent_type['ì›”ì„¸_num'].agg(['mean', 'median', 'max', 'min'])
            stats['monthlyRentStatistics'] = {
                "averageMonthlyRent": as_value_unit_m(wolse_stats_raw['mean']),
                "medianMonthlyRent": as_value_unit_m(wolse_stats_raw['median']),
            }
        building_col = get_col_from_df(df_rent_type, 'ì—°ë¦½ë‹¤ì„¸ëŒ€ëª…', 'rowHouseName')
        if building_col.notna().any():
            stats['statisticsByRowHouseComplex'] = df_rent_type.groupby(building_col).agg(
                transactionCount=('ë³´ì¦ê¸ˆ_num', 'size'),
                averageDeposit=('ë³´ì¦ê¸ˆ_num', 'mean')
            ).apply(lambda x: x.astype(int) if x.name == 'transactionCount' else as_value_unit_m(x)).to_dict('index')
        return stats
    jeonse_analysis = analyze_rent_type(df_jeonse, 'jeonse')
    wolse_analysis = analyze_rent_type(df_wolse, 'wolse')
    return {
        "transactionTypeDistribution": {
            "jeonse_count": len(df_jeonse),
            "wolse_count": len(df_wolse),
        },
        "jeonseAnalysis": jeonse_analysis,
        "wolseAnalysis": wolse_analysis,
        "notes": "Statistics are separated by transaction type (Jeonse vs. Wolse). ê³„ì•½ë©´ì  ê¸°ì¤€ìœ¼ë¡œ ì§‘ê³„í•©ë‹ˆë‹¤. (excluUseArë„ ìë™ í™œìš©)"
    }

@mcp.tool(
    name="analyze_row_house_rent",
    description="""ì—°ë¦½ë‹¤ì„¸ëŒ€ ì „ì›”ì„¸ ì‹¤ê±°ë˜ ë°ì´í„° íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ì›”ê°„ ë¦¬í¬íŠ¸ í˜•ì‹ì˜ í•µì‹¬ í†µê³„ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.\nì´ ë„êµ¬ëŠ” `get_row_house_rent_data`ë¥¼ í†µí•´ ì–»ì€ ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì…ë ¥ë°›ì•„ ì‘ë™í•˜ë©°, 'ì „ì„¸'ì™€ 'ì›”ì„¸'ë¥¼ ìë™ìœ¼ë¡œ êµ¬ë¶„í•˜ì—¬ ê°ê°ì— ëŒ€í•œ ìƒì„¸ í†µê³„ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\në¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì£¼ìš” í†µê³„ ì§€í‘œë“¤ì„ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ì°¨íŠ¸ë‚˜ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n\n- **ê±°ë˜ ìœ í˜• ë¶„í¬**: ì „ì²´ ê±°ë˜ ì¤‘ ì „ì„¸ì™€ ì›”ì„¸ì˜ ë¹„ì¤‘ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n- **ì „ì„¸ ë¶„ì„**: ë³´ì¦ê¸ˆì— ëŒ€í•œ í‰ê· /ì¤‘ìœ„/ìµœê³ /ìµœì € ê°€ê²© ë° ì£¼ìš” ê±°ë˜ ì‚¬ë¡€ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ê±´ë¬¼ëª…ë³„ í†µê³„ë„ í¬í•¨ë©ë‹ˆë‹¤.\n- **ì›”ì„¸ ë¶„ì„**: ë³´ì¦ê¸ˆ ë° ì›”ì„¸ ê°ê°ì— ëŒ€í•œ í‰ê· /ì¤‘ìœ„ ê°€ê²© í†µê³„ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ê±´ë¬¼ëª…ë³„ í†µê³„ë„ í¬í•¨ë©ë‹ˆë‹¤.\n\nArguments:\n- file_path (str, required): `get_row_house_rent_data` ë„êµ¬ë¡œ ìƒì„±ëœ `raw.data.json` ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œ.\n\nReturns:\n- ì „ì„¸ì™€ ì›”ì„¸ë¡œ êµ¬ë¶„ëœ í†µê³„ ë¶„ì„ ê²°ê³¼ê°€ ë‹´ê¸´ ìƒì„¸í•œ JSON ë¬¸ìì—´.""",
    tags={"ì—°ë¦½ë‹¤ì„¸ëŒ€", "í†µê³„", "ë¶„ì„", "ë¦¬í¬íŠ¸", "ì „ì„¸", "ì›”ì„¸", "ì „ì›”ì„¸", "ì‹¤ê±°ë˜ê°€"}
)
def analyze_row_house_rent(file_path: str, ctx: Optional[Any] = None) -> TextContent:
    def call(context):
        try:
            p = Path(file_path)
            if not p.exists():
                return json.dumps({"error": f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}"}, ensure_ascii=False)
            cache_path = get_summary_cache_path(p, property_type="row_house", trade_type="rent")
            if cache_path.exists():
                if cache_path.stat().st_mtime > p.stat().st_mtime:
                    logger.info(f"âœ… ìœ íš¨í•œ ì—°ë¦½ë‹¤ì„¸ëŒ€ ì „ì›”ì„¸ ìºì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {cache_path}")
                    with open(cache_path, 'r', encoding='utf-8') as f:
                        return f.read()
            logger.info(f"ğŸ”„ ìƒˆë¡œìš´ ì—°ë¦½ë‹¤ì„¸ëŒ€ ì „ì›”ì„¸ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤: {file_path}")
            df = pd.read_json(p, lines=True)
            summary_data = analyze_row_house_rent_data(df)
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=4, default=default_serializer)
            summary_data["summary_cached_path"] = str(cache_path)
            return json.dumps(summary_data, ensure_ascii=False, indent=4, default=default_serializer)
        except Exception as e:
            logger.error(f"ì—°ë¦½ë‹¤ì„¸ëŒ€ ì „ì›”ì„¸ ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return json.dumps({"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}, ensure_ascii=False)
    result = with_context(ctx, "analyze_row_house_rent", call)
    return TextContent(type="text", text=result)

def analyze_industrial_property_data(df: pd.DataFrame) -> Dict[str, Any]:
    """DataFrameì„ ë°›ì•„ ê³µì¥/ì°½ê³  ë“± ì‚°ì—…ìš© ë¶€ë™ì‚° í†µê³„ë¥¼ ë¶„ì„í•˜ê³  ì˜ë¬¸ keyì™€ ë‹¨ìœ„ê°€ í¬í•¨ëœ ê°’ìœ¼ë¡œ JSONì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if df.empty:
        return {"error": "No data to analyze."}
    # --- ë°ì´í„° ì „ì²˜ë¦¬ ---
    df['ê±°ë˜ê¸ˆì•¡_num'] = to_numeric(get_col_from_df(df, 'ê±°ë˜ê¸ˆì•¡', 'dealAmount', 'dealAmountNum'))
    # Fix: include 'buildingAr' as a fallback for area
    df['ì „ìš©ë©´ì _num'] = to_numeric(get_col_from_df(df, 'ì „ìš©ë©´ì ', 'area', 'excluUseAr', 'buildingAr', 'areaNum'))
    df['ê±´ì¶•ë…„ë„_num'] = to_numeric(get_col_from_df(df, 'ê±´ì¶•ë…„ë„', 'buildYear', 'buildYearNum'))
    df.dropna(subset=['ê±°ë˜ê¸ˆì•¡_num', 'ì „ìš©ë©´ì _num'], inplace=True)
    df = df[df['ì „ìš©ë©´ì _num'] > 0].copy()
    if df.empty:
        return {"error": "No valid transaction data after cleaning."}
    df['í‰ë‹¹ê°€_ë§Œì›'] = (df['ê±°ë˜ê¸ˆì•¡_num'] / df['ì „ìš©ë©´ì _num']) * 3.305785
    current_year = datetime.now().year
    df['ê±´ë¬¼ì—°ë ¹'] = current_year - df['ê±´ì¶•ë…„ë„_num']
    def as_value_unit_m(v): return as_value_unit(v, "ë§Œì›")
    def as_value_unit_per_pyeong(v): return as_value_unit(v, "ë§Œì›/í‰")
    # --- 1. ì¢…í•© í†µê³„ ---
    total_count = len(df)
    total_value = df['ê±°ë˜ê¸ˆì•¡_num'].sum()
    use_col = get_col_from_df(df, 'ìš©ë„', 'ìœ í˜•', 'buildingUse')
    use_distribution = use_col.value_counts().to_dict() if use_col.notna().any() else {}
    overall_stats = {
        "totalTransactionCount": total_count,
        "totalTransactionValue": as_value_unit_m(total_value),
        "transactionDistributionByUseType": use_distribution
    }
    # --- 2. ê°€ê²© ìˆ˜ì¤€ í†µê³„ ---
    price_stats_raw = df['ê±°ë˜ê¸ˆì•¡_num'].agg(['mean', 'median', 'max', 'min'])
    price_stats = {
        "overallAveragePrice": as_value_unit_m(price_stats_raw['mean']),
        "overallMedianPrice": as_value_unit_m(price_stats_raw['median']),
        "overallHighestPrice": as_value_unit_m(price_stats_raw['max']),
        "overallLowestPrice": as_value_unit_m(price_stats_raw['min']),
        "representativeDeals": {
            "highestPriceDeal": clean_deal_for_display(df.loc[df['ê±°ë˜ê¸ˆì•¡_num'].idxmax()]),
            "lowestPriceDeal": clean_deal_for_display(df.loc[df['ê±°ë˜ê¸ˆì•¡_num'].idxmin()]),
            "dealClosestToAverage": clean_deal_for_display(df.loc[(df['ê±°ë˜ê¸ˆì•¡_num'] - price_stats_raw['mean']).abs().idxmin()]),
            "dealClosestToMedian": clean_deal_for_display(df.loc[(df['ê±°ë˜ê¸ˆì•¡_num'] - price_stats_raw['median']).abs().idxmin()])
        }
    }
    # --- 3. ë‹¨ìœ„ ë©´ì ë‹¹ ê°€ê²© í†µê³„ ---
    price_per_area_stats = {
        "overallAveragePricePerPyeong": as_value_unit_per_pyeong(df['í‰ë‹¹ê°€_ë§Œì›'].mean()),
        "overallMedianPricePerPyeong": as_value_unit_per_pyeong(df['í‰ë‹¹ê°€_ë§Œì›'].median()),
    }
    if use_col.notna().any():
        price_by_use_raw = df.groupby(use_col)['ê±°ë˜ê¸ˆì•¡_num'].agg(['mean', 'median', 'max', 'min'])
        price_stats["priceStatisticsByUseType"] = {
            use: {
                "averagePrice": as_value_unit_m(stats['mean']),
                "medianPrice": as_value_unit_m(stats['median']),
                "highestPrice": as_value_unit_m(stats['max']),
                "lowestPrice": as_value_unit_m(stats['min']),
            } for use, stats in price_by_use_raw.to_dict('index').items()
        }
        price_per_area_by_use_raw = df.groupby(use_col)['í‰ë‹¹ê°€_ë§Œì›'].agg(['mean', 'median'])
        price_per_area_stats["pricePerPyeongStatisticsByUseType"] = {
            use: {
                "averagePricePerPyeong": as_value_unit_per_pyeong(stats['mean']),
                "medianPricePerPyeong": as_value_unit_per_pyeong(stats['median']),
            } for use, stats in price_per_area_by_use_raw.to_dict('index').items()
        }
    # --- 4. ì…ì§€ë³„ í†µê³„ (ë™ë³„) ---
    location_col = get_col_from_df(df, 'ë²•ì •ë™', 'umdNm', 'dong')
    location_stats = {}
    if location_col.notna().any():
        location_summary_raw = df.groupby(location_col).agg(
            Count=('ê±°ë˜ê¸ˆì•¡_num', 'size'),
            Mean_Price=('ê±°ë˜ê¸ˆì•¡_num', 'mean'),
            Max_Price=('ê±°ë˜ê¸ˆì•¡_num', 'max'),
            Min_Price=('ê±°ë˜ê¸ˆì•¡_num', 'min'),
            Mean_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'mean'),
            Median_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'median')
        )
        for dong, stats in location_summary_raw.to_dict('index').items():
            location_stats[dong] = {
                "transactionCount": int(stats['Count']),
                "averagePrice": as_value_unit_m(stats['Mean_Price']),
                "highestPrice": as_value_unit_m(stats['Max_Price']),
                "lowestPrice": as_value_unit_m(stats['Min_Price']),
                "averagePricePerPyeong": as_value_unit_per_pyeong(stats['Mean_PPA']),
                "medianPricePerPyeong": as_value_unit_per_pyeong(stats['Median_PPA']),
            }
    # --- 5. ê±´ë¬¼ íŠ¹ì„±ë³„ í†µê³„ (ì—°ë ¹/ê·œëª¨) ---
    age_bins = [0, 6, 11, 21, np.inf]
    age_labels = ['5 years or newer', '6-10 years', '11-20 years', 'over 20 years']
    df['ê±´ë¬¼ì—°ë ¹ëŒ€'] = pd.cut(df['ê±´ë¬¼ì—°ë ¹'], bins=age_bins, labels=age_labels, right=False)
    age_stats = {}
    if not df['ê±´ë¬¼ì—°ë ¹ëŒ€'].isnull().all():
        age_summary_raw = df.groupby('ê±´ë¬¼ì—°ë ¹ëŒ€', observed=True).agg(
            Count=('ê±°ë˜ê¸ˆì•¡_num', 'size'),
            Mean_Price=('ê±°ë˜ê¸ˆì•¡_num', 'mean'),
            Mean_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'mean')
        )
        age_stats = {
            age_group: {
                "transactionCount": int(stats['Count']),
                "averagePrice": as_value_unit_m(stats['Mean_Price']),
                "averagePricePerPyeong": as_value_unit_per_pyeong(stats['Mean_PPA']),
            } for age_group, stats in age_summary_raw.to_dict('index').items()
        }
    area_bins = [0, 100, 300, 1000, np.inf]
    area_labels = ['small (<100mÂ²)', 'medium (100-300mÂ²)', 'large (300-1000mÂ²)', 'extra_large (>1000mÂ²)']
    df['ê±´ë¬¼ê·œëª¨'] = pd.cut(df['ì „ìš©ë©´ì _num'], bins=area_bins, labels=area_labels, right=False)
    scale_stats = {}
    if not df['ê±´ë¬¼ê·œëª¨'].isnull().all():
        scale_summary_raw = df.groupby('ê±´ë¬¼ê·œëª¨', observed=True).agg(
            Count=('ê±°ë˜ê¸ˆì•¡_num', 'size'),
            Mean_Price=('ê±°ë˜ê¸ˆì•¡_num', 'mean'),
            Mean_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'mean')
        )
        scale_stats = {
            scale_group: {
                "transactionCount": int(stats['Count']),
                "averagePrice": as_value_unit_m(stats['Mean_Price']),
                "averagePricePerPyeong": as_value_unit_per_pyeong(stats['Mean_PPA']),
            } for scale_group, stats in scale_summary_raw.to_dict('index').items()
        }
    building_stats = {
        "statisticsByBuildingAge": age_stats,
        "statisticsByBuildingScale_exclusiveArea": scale_stats
    }
    return {
        "overallStatistics": overall_stats,
        "priceLevelStatistics": price_stats,
        "pricePerAreaStatistics": price_per_area_stats,
        "statisticsByUseType": price_stats.get("priceStatisticsByUseType", {}),
        "locationStatistics_byDong": location_stats,
        "buildingCharacteristicsStatistics": building_stats,
        "notes": "ê³µì¥/ì°½ê³  ë“± ì‚°ì—…ìš© ë¶€ë™ì‚°ì€ ìš©ë„(ê³µì¥/ì°½ê³ /ê¸°íƒ€), ì „ìš©ë©´ì , ê±´ë¬¼ì—°ë ¹, ì¸µê³ , ëŒ€ì§€/ê±´ë¬¼ë©´ì  ë“± íŠ¹ì„±ì´ ì¤‘ìš”í•©ë‹ˆë‹¤. í‰ë‹¹ê°€ëŠ” ì „ìš©ë©´ì  ê¸°ì¤€ì…ë‹ˆë‹¤."
    }

@mcp.tool(
    name="analyze_industrial_property_trade",
    description="""ê³µì¥/ì°½ê³  ë“± ì‚°ì—…ìš© ë¶€ë™ì‚° ë§¤ë§¤ ì‹¤ê±°ë˜ ë°ì´í„° íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ì›”ê°„ ë¦¬í¬íŠ¸ í˜•ì‹ì˜ í•µì‹¬ í†µê³„ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.\nì´ ë„êµ¬ëŠ” `get_indu_trade_data`ë¥¼ í†µí•´ ì–»ì€ ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì…ë ¥ë°›ì•„ ì‘ë™í•©ë‹ˆë‹¤.\nì¢…í•© í†µê³„, ê°€ê²© ìˆ˜ì¤€, í‰ë‹¹ê°€, ìš©ë„ë³„, ë™ë³„, ê±´ë¬¼ íŠ¹ì„±ë³„ ë“± ë‹¤ê°ì ì¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\në¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì£¼ìš” í†µê³„ ì§€í‘œë“¤ì„ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ì°¨íŠ¸ë‚˜ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n\nArguments:\n- file_path (str, required): `get_indu_trade_data` ë„êµ¬ë¡œ ìƒì„±ëœ `raw.data.json` ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œ.\n\nReturns:\n- í†µê³„ ë¶„ì„ ê²°ê³¼ê°€ ë‹´ê¸´ ìƒì„¸í•œ JSON ë¬¸ìì—´.""",
    tags={"ê³µì¥", "ì°½ê³ ", "ì‚°ì—…ìš©", "í†µê³„", "ë¶„ì„", "ë¦¬í¬íŠ¸", "ë§¤ë§¤", "ì‹¤ê±°ë˜ê°€"}
)
def analyze_industrial_property_trade(file_path: str, ctx: Optional[Any] = None) -> TextContent:
    def call(context):
        try:
            p = Path(file_path)
            if not p.exists():
                return json.dumps({"error": f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}"}, ensure_ascii=False)
            cache_path = get_summary_cache_path(p, property_type="industrial", trade_type=None)
            if cache_path.exists():
                if cache_path.stat().st_mtime > p.stat().st_mtime:
                    logger.info(f"âœ… ìœ íš¨í•œ ì‚°ì—…ìš© ë¶€ë™ì‚° ë§¤ë§¤ ìºì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {cache_path}")
                    with open(cache_path, 'r', encoding='utf-8') as f:
                        return f.read()
            logger.info(f"ğŸ”„ ìƒˆë¡œìš´ ì‚°ì—…ìš© ë¶€ë™ì‚° ë§¤ë§¤ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤: {file_path}")
            df = pd.read_json(p, lines=True)
            summary_data = analyze_industrial_property_data(df)
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=4, default=default_serializer)
            summary_data["summary_cached_path"] = str(cache_path)
            return json.dumps(summary_data, ensure_ascii=False, indent=4, default=default_serializer)
        except Exception as e:
            logger.error(f"ì‚°ì—…ìš© ë¶€ë™ì‚° ë§¤ë§¤ ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return json.dumps({"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}, ensure_ascii=False)
    result = with_context(ctx, "analyze_industrial_property_trade", call)
    return TextContent(type="text", text=result) 

def analyze_land_property_data(df: pd.DataFrame) -> Dict[str, Any]:
    """DataFrameì„ ë°›ì•„ í† ì§€ ë§¤ë§¤ í†µê³„ë¥¼ ë¶„ì„í•˜ê³  ì˜ë¬¸ keyì™€ ë‹¨ìœ„ê°€ í¬í•¨ëœ ê°’ìœ¼ë¡œ JSONì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if df.empty:
        return {"error": "No data to analyze."}
    # --- ë°ì´í„° ì „ì²˜ë¦¬ ---
    df['ê±°ë˜ê¸ˆì•¡_num'] = to_numeric(get_col_from_df(df, 'ê±°ë˜ê¸ˆì•¡', 'dealAmount', 'dealAmountNum'))
    # Fix: include 'dealArea' as a fallback for area
    df['í† ì§€ë©´ì _num'] = to_numeric(get_col_from_df(df, 'ë©´ì ', 'landAr', 'dealArea', 'area', 'areaNum'))
    df.dropna(subset=['ê±°ë˜ê¸ˆì•¡_num', 'í† ì§€ë©´ì _num'], inplace=True)
    df = df[df['í† ì§€ë©´ì _num'] > 0].copy()
    if df.empty:
        return {"error": "No valid transaction data after cleaning."}
    df['í‰ë‹¹ê°€_ë§Œì›'] = (df['ê±°ë˜ê¸ˆì•¡_num'] / df['í† ì§€ë©´ì _num']) * 3.305785
    def as_value_unit_m(v): return as_value_unit(v, "ë§Œì›")
    def as_value_unit_per_pyeong(v): return as_value_unit(v, "ë§Œì›/í‰")
    # --- 1. ì¢…í•© í†µê³„ ---
    total_count = len(df)
    total_value = df['ê±°ë˜ê¸ˆì•¡_num'].sum()
    land_type_col = get_col_from_df(df, 'ì§€ëª©', 'landType')
    type_distribution = land_type_col.value_counts().to_dict() if land_type_col.notna().any() else {}
    overall_stats = {
        "totalTransactionCount": total_count,
        "totalTransactionValue": as_value_unit_m(total_value),
        "transactionDistributionByLandType": type_distribution
    }
    # --- 2. ê°€ê²© ìˆ˜ì¤€ í†µê³„ ---
    price_stats_raw = df['ê±°ë˜ê¸ˆì•¡_num'].agg(['mean', 'median', 'max', 'min'])
    price_stats = {
        "overallAveragePrice": as_value_unit_m(price_stats_raw['mean']),
        "overallMedianPrice": as_value_unit_m(price_stats_raw['median']),
        "overallHighestPrice": as_value_unit_m(price_stats_raw['max']),
        "overallLowestPrice": as_value_unit_m(price_stats_raw['min']),
        "representativeDeals": {
            "highestPriceDeal": clean_deal_for_display(df.loc[df['ê±°ë˜ê¸ˆì•¡_num'].idxmax()]),
            "lowestPriceDeal": clean_deal_for_display(df.loc[df['ê±°ë˜ê¸ˆì•¡_num'].idxmin()]),
            "dealClosestToAverage": clean_deal_for_display(df.loc[(df['ê±°ë˜ê¸ˆì•¡_num'] - price_stats_raw['mean']).abs().idxmin()]),
            "dealClosestToMedian": clean_deal_for_display(df.loc[(df['ê±°ë˜ê¸ˆì•¡_num'] - price_stats_raw['median']).abs().idxmin()])
        }
    }
    # --- 3. ë‹¨ìœ„ ë©´ì ë‹¹ ê°€ê²© í†µê³„ ---
    price_per_area_stats = {
        "overallAveragePricePerPyeong": as_value_unit_per_pyeong(df['í‰ë‹¹ê°€_ë§Œì›'].mean()),
        "overallMedianPricePerPyeong": as_value_unit_per_pyeong(df['í‰ë‹¹ê°€_ë§Œì›'].median()),
    }
    if land_type_col.notna().any():
        price_by_type_raw = df.groupby(land_type_col)['ê±°ë˜ê¸ˆì•¡_num'].agg(['mean', 'median', 'max', 'min'])
        price_stats["priceStatisticsByLandType"] = {
            land_type: {
                "averagePrice": as_value_unit_m(stats['mean']),
                "medianPrice": as_value_unit_m(stats['median']),
                "highestPrice": as_value_unit_m(stats['max']),
                "lowestPrice": as_value_unit_m(stats['min']),
            } for land_type, stats in price_by_type_raw.to_dict('index').items()
        }
        price_per_area_by_type_raw = df.groupby(land_type_col)['í‰ë‹¹ê°€_ë§Œì›'].agg(['mean', 'median'])
        price_per_area_stats["pricePerPyeongStatisticsByLandType"] = {
            land_type: {
                "averagePricePerPyeong": as_value_unit_per_pyeong(stats['mean']),
                "medianPricePerPyeong": as_value_unit_per_pyeong(stats['median']),
            } for land_type, stats in price_per_area_by_type_raw.to_dict('index').items()
        }
    # --- 4. ì…ì§€ë³„ í†µê³„ (ë™ë³„) ---
    location_col = get_col_from_df(df, 'ë²•ì •ë™', 'umdNm', 'dong')
    location_stats = {}
    if location_col.notna().any():
        location_summary_raw = df.groupby(location_col).agg(
            Count=('ê±°ë˜ê¸ˆì•¡_num', 'size'),
            Mean_Price=('ê±°ë˜ê¸ˆì•¡_num', 'mean'),
            Max_Price=('ê±°ë˜ê¸ˆì•¡_num', 'max'),
            Min_Price=('ê±°ë˜ê¸ˆì•¡_num', 'min'),
            Mean_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'mean'),
            Median_PPA=('í‰ë‹¹ê°€_ë§Œì›', 'median')
        )
        for dong, stats in location_summary_raw.to_dict('index').items():
            location_stats[dong] = {
                "transactionCount": int(stats['Count']),
                "averagePrice": as_value_unit_m(stats['Mean_Price']),
                "highestPrice": as_value_unit_m(stats['Max_Price']),
                "lowestPrice": as_value_unit_m(stats['Min_Price']),
                "averagePricePerPyeong": as_value_unit_per_pyeong(stats['Mean_PPA']),
                "medianPricePerPyeong": as_value_unit_per_pyeong(stats['Median_PPA']),
            }
    return {
        "overallStatistics": overall_stats,
        "priceLevelStatistics": price_stats,
        "pricePerAreaStatistics": price_per_area_stats,
        "statisticsByLandType": price_stats.get("priceStatisticsByLandType", {}),
        "locationStatistics_byDong": location_stats,
        "notes": "í† ì§€ ê±°ë˜ëŠ” ì§€ëª©(ìš©ë„), ë©´ì , ë„ë¡œì ‘ë©´, í˜•ìƒ, ë°©ìœ„ ë“± íŠ¹ì„±ì´ ì¤‘ìš”í•©ë‹ˆë‹¤. í‰ë‹¹ê°€ëŠ” í† ì§€ë©´ì  ê¸°ì¤€ì…ë‹ˆë‹¤."
    }

@mcp.tool(
    name="analyze_land_trade",
    description="""í† ì§€ ë§¤ë§¤ ì‹¤ê±°ë˜ ë°ì´í„° íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ ì›”ê°„ ë¦¬í¬íŠ¸ í˜•ì‹ì˜ í•µì‹¬ í†µê³„ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.\nì´ ë„êµ¬ëŠ” `get_land_trade_data`ë¥¼ í†µí•´ ì–»ì€ ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì…ë ¥ë°›ì•„ ì‘ë™í•©ë‹ˆë‹¤.\nì¢…í•© í†µê³„, ê°€ê²© ìˆ˜ì¤€, í‰ë‹¹ê°€, ì§€ëª©ë³„, ë™ë³„ ë“± ë‹¤ê°ì ì¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\në¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì£¼ìš” í†µê³„ ì§€í‘œë“¤ì„ ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ì°¨íŠ¸ë‚˜ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n\nArguments:\n- file_path (str, required): `get_land_trade_data` ë„êµ¬ë¡œ ìƒì„±ëœ `raw.data.json` ë°ì´í„° íŒŒì¼ì˜ ê²½ë¡œ.\n\nReturns:\n- í†µê³„ ë¶„ì„ ê²°ê³¼ê°€ ë‹´ê¸´ ìƒì„¸í•œ JSON ë¬¸ìì—´.""",
    tags={"í† ì§€", "í†µê³„", "ë¶„ì„", "ë¦¬í¬íŠ¸", "ë§¤ë§¤", "ì‹¤ê±°ë˜ê°€"}
)
def analyze_land_trade(file_path: str, ctx: Optional[Any] = None) -> TextContent:
    def call(context):
        try:
            p = Path(file_path)
            if not p.exists():
                return json.dumps({"error": f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}"}, ensure_ascii=False)
            cache_path = get_summary_cache_path(p, property_type="land", trade_type=None)
            if cache_path.exists():
                if cache_path.stat().st_mtime > p.stat().st_mtime:
                    logger.info(f"âœ… ìœ íš¨í•œ í† ì§€ ë§¤ë§¤ ìºì‹œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {cache_path}")
                    with open(cache_path, 'r', encoding='utf-8') as f:
                        return f.read()
            logger.info(f"ğŸ”„ ìƒˆë¡œìš´ í† ì§€ ë§¤ë§¤ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤: {file_path}")
            df = pd.read_json(p, lines=True)
            summary_data = analyze_land_property_data(df)
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, ensure_ascii=False, indent=4, default=default_serializer)
            summary_data["summary_cached_path"] = str(cache_path)
            return json.dumps(summary_data, ensure_ascii=False, indent=4, default=default_serializer)
        except Exception as e:
            logger.error(f"í† ì§€ ë§¤ë§¤ ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return json.dumps({"error": f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"}, ensure_ascii=False)
    result = with_context(ctx, "analyze_land_trade", call)
    return TextContent(type="text", text=result)

@mcp.tool(
    name="get_reb_stat_list",
    description="""ë¶€ë™ì‚° í†µê³„ ì„œë¹„ìŠ¤ì˜ í†µê³„í‘œ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.\n- ë°˜ë“œì‹œ ì´ ë„êµ¬ë¡œ STATBL_ID(í†µê³„í‘œID)ë¥¼ ë¨¼ì € í™•ì¸í•œ ë’¤, ì„¸ë¶€í•­ëª©/ë°ì´í„° ì¡°íšŒ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\n- params ì˜ˆì‹œ: {\"STATBL_ID\": \"...\", \"pIndex\": 1, \"pSize\": 100}\n""",
    tags={"ë¶€ë™ì‚°", "í†µê³„", "REB", "í†µê³„í‘œ"}
)
def get_reb_stat_list_tool(params: dict) -> TextContent:
    try:
        data = get_reb_stat_list(params)
        return TextContent(type="text", text=json.dumps(data, ensure_ascii=False))
    except Exception as e:
        return TextContent(type="text", text=json.dumps({"error": str(e)}, ensure_ascii=False))

@mcp.tool(
    name="get_reb_stat_items",
    description="""íŠ¹ì • í†µê³„í‘œì˜ ì„¸ë¶€í•­ëª© ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.\n- ë°˜ë“œì‹œ 'get_reb_stat_list' ë„êµ¬ë¡œ STATBL_IDë¥¼ ë¨¼ì € í™•ì¸í•˜ì„¸ìš”.\n- params ì˜ˆì‹œ: {\"STATBL_ID\": \"...\", \"pIndex\": 1, \"pSize\": 100}\n""",
    tags={"ë¶€ë™ì‚°", "í†µê³„", "REB", "í•­ëª©"}
)
def get_reb_stat_items_tool(params: dict) -> TextContent:
    try:
        data = get_reb_stat_items(params)
        return TextContent(type="text", text=json.dumps(data, ensure_ascii=False))
    except Exception as e:
        return TextContent(type="text", text=json.dumps({"error": str(e)}, ensure_ascii=False))

@mcp.tool(
    name="get_reb_stat_data",
    description="""íŠ¹ì • í†µê³„í‘œì˜ í†µê³„ ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.\n- ë°˜ë“œì‹œ 'get_reb_stat_list' ë„êµ¬ë¡œ STATBL_ID, DTACYCLE_CD(ì£¼ê¸°ì½”ë“œ)ë¥¼ ë¨¼ì € í™•ì¸í•˜ì„¸ìš”.\n- params ì˜ˆì‹œ: {\"STATBL_ID\": \"...\", \"DTACYCLE_CD\": \"...\", \"pIndex\": 1, \"pSize\": 100, ...}\n""",
    tags={"ë¶€ë™ì‚°", "í†µê³„", "REB", "ë°ì´í„°"}
)
def get_reb_stat_data_tool(params: dict) -> TextContent:
    try:
        data = get_reb_stat_data(params)
        return TextContent(type="text", text=json.dumps(data, ensure_ascii=False))
    except Exception as e:
        return TextContent(type="text", text=json.dumps({"error": str(e)}, ensure_ascii=False))

@mcp.tool(
    name="get_reb_stat_list_full",
    description="""Collects and caches the full list of REB statistical tables, and returns the cache file path. Use this path in analysis/search tools.\nì´ ë„êµ¬ëŠ” ì „ì²´ ë°ì´í„°ë¥¼ ìºì‹±í•˜ê³ , ìºì‹œ íŒŒì¼ ê²½ë¡œë§Œ ë°˜í™˜í•©ë‹ˆë‹¤. ë¶„ì„/ê²€ìƒ‰ ë„êµ¬ì—ì„œ ì´ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.""",
    tags={"realestate", "statistics", "REB", "stat_table"}
)
def get_reb_stat_list_full(params: dict) -> TextContent:
    # ì‹¤ì œ ë°ì´í„° ìºì‹± ë° ê²½ë¡œ ë°˜í™˜
    path = cache_stat_list_full(params)
    return TextContent(type="text", text=path)

@mcp.tool(
    name="get_reb_stat_list",
    description="""Collects and caches a list of REB statistical tables (with optional filters), and returns the cache file path. Use this path in analysis/search tools.\nì´ ë„êµ¬ëŠ” ì „ì²´ ë°ì´í„°ë¥¼ ìºì‹±í•˜ê³ , ìºì‹œ íŒŒì¼ ê²½ë¡œë§Œ ë°˜í™˜í•©ë‹ˆë‹¤. ë¶„ì„/ê²€ìƒ‰ ë„êµ¬ì—ì„œ ì´ ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.""",
    tags={"realestate", "statistics", "REB", "stat_table"}
)
def get_reb_stat_list(params: dict) -> TextContent:
    path = cache_stat_list(params)
    return TextContent(type="text", text=path)

@mcp.tool(
    name="analyze_reb_stat_items",
    description="""Analyzes REB statistical items from a cache file. If the file does not exist, it will be automatically created.\nYou can filter by any column using the 'filter' parameter.\nResults are sorted by the most recent order.\nResult should be visualized using a chart or table.\nfilter example: {\"ITM_NM\": \"ì§€ìˆ˜\", \"UI_NM\": \"ì§€ìˆ˜\"}\nBy default, the preview shows the latest 5 items.""",
    tags={"realestate", "statistics", "REB", "item"}
)
def analyze_reb_stat_items(params: dict) -> TextContent:
    cache_path = params.get("cache_path")
    filter = params.get("filter", {})
    if not cache_path or not os.path.exists(cache_path):
        # ìë™ ìºì‹±
        from mcp_kr_realestate.apis.reb_api import cache_stat_list_full
        cache_path = cache_stat_list_full({})
    import pandas as pd
    import json
    with open(cache_path, "r", encoding="utf-8") as f:
        all_items = json.load(f)
    df = pd.DataFrame(all_items)
    if "V_ORDER" in df.columns:
        df = df.sort_values(by="V_ORDER", ascending=False)
    for k, v in filter.items():
        if k in df.columns:
            df = df[df[k] == v]
    preview = df.head(5).to_dict(orient="records")
    summary = {
        "total_count": len(df),
        "item_names": df["ITM_NM"].unique().tolist() if "ITM_NM" in df.columns else [],
        "unit_names": df["UI_NM"].unique().tolist() if "UI_NM" in df.columns else [],
        "preview": preview
    }
    return TextContent(type="text", text=json.dumps(summary, ensure_ascii=False))

def _get_data_cache_path(statbl_id):
    return f"/tmp/reb_stats_cache/stat_data_{statbl_id}.json"

def _ensure_data_cache(statbl_id):
    import os, json
    from mcp_kr_realestate.apis.reb_api import get_reb_stat_data_all
    cache_path = _get_data_cache_path(statbl_id)
    if not os.path.exists(cache_path):
        all_data = get_reb_stat_data_all({"STATBL_ID": statbl_id})
        with open(cache_path, "w", encoding="utf-8") as f:
            json.dump(all_data, f, ensure_ascii=False)
    # ê²€ì¦: rowì™€ WRTTIME_IDTFR_ID ì»¬ëŸ¼ì´ ìˆëŠ”ì§€
    import pandas as pd
    with open(cache_path, "r", encoding="utf-8") as f:
        items = json.load(f)
    df = pd.DataFrame(items)
    if df.empty or "WRTTIME_IDTFR_ID" not in df.columns:
        # API ì¬í˜¸ì¶œ ë° rowë§Œ ì €ì¥
        all_data = get_reb_stat_data_all({"STATBL_ID": statbl_id})
        with open(cache_path, "w", encoding="utf-8") as f:
            json.dump(all_data, f, ensure_ascii=False)
        df = pd.DataFrame(all_data)
    return cache_path, df

@mcp.tool(
    name="analyze_reb_stat_data",
    description="""Analyzes REB statistical data for a given STATBL_ID.\nYou can simply provide STATBL_ID and the tool will automatically use the latest data.\nIf filter['time'] is not provided or set to 'latest', the tool will analyze the most recent period.\nIf the data cache does not exist, it will be created automatically.\nIf no data is found, the tool will inform you of the latest available period.""",
    tags={"realestate", "statistics", "REB", "data"}
)
def analyze_reb_stat_data(params: dict) -> TextContent:
    import json, pandas as pd
    statbl_id = params.get("STATBL_ID")
    filter = params.get("filter", {})
    cache_path = params.get("cache_path")
    if not cache_path and statbl_id:
        cache_path = _get_data_cache_path(statbl_id)
    # ìºì‹œ íŒŒì¼/row/ì»¬ëŸ¼ ê²€ì¦ ë° ìë™ ìƒì„±
    if statbl_id:
        cache_path, df = _ensure_data_cache(statbl_id)
    else:
        with open(cache_path, "r", encoding="utf-8") as f:
            all_items = json.load(f)
        df = pd.DataFrame(all_items)
    if df.empty or "WRTTIME_IDTFR_ID" not in df.columns:
        latest = None
        if "WRTTIME_IDTFR_ID" in df.columns:
            latest = df["WRTTIME_IDTFR_ID"].max()
        msg = {"error": "No data (row) for this STATBL_ID. APIì—ì„œ ì§ì ‘ ì¡°íšŒë¥¼ ì‹œë„í–ˆìœ¼ë‚˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.", "latest_available_period": latest}
        return TextContent(type="text", text=json.dumps(msg, ensure_ascii=False))
    df = df.sort_values(by="WRTTIME_IDTFR_ID", ascending=False)
    # ë‚ ì§œ í˜•ì‹ ìë™ ë³€í™˜ ë° 'latest' ì§€ì›
    time_val = filter.get("WRTTIME_IDTFR_ID") or filter.get("time")
    if not time_val or str(time_val).lower() == "latest":
        latest = df["WRTTIME_IDTFR_ID"].max()
        df = df[df["WRTTIME_IDTFR_ID"] == latest]
    else:
        norm_time = _normalize_time_str(time_val)
        if norm_time:
            df = df[df["WRTTIME_IDTFR_ID"].astype(str).str.startswith(norm_time)]
    for k, v in filter.items():
        if k in ["WRTTIME_IDTFR_ID", "time"]:
            continue
        if k in df.columns:
            df = df[df[k] == v]
    preview = df.head(5).to_dict(orient="records")
    stats = {}
    if "DTA_VAL" in df.columns:
        stats = df["DTA_VAL"].describe().to_dict()
    if len(df) == 0:
        latest = None
        if "WRTTIME_IDTFR_ID" in df.columns:
            latest = df["WRTTIME_IDTFR_ID"].max()
        msg = {"error": "No data for the given filter.", "latest_available_period": latest}
        return TextContent(type="text", text=json.dumps(msg, ensure_ascii=False))
    summary = {
        "total_count": len(df),
        "time_range": [df["WRTTIME_IDTFR_ID"].min(), df["WRTTIME_IDTFR_ID"].max()] if "WRTTIME_IDTFR_ID" in df.columns else [],
        "item_names": df["ITM_NM"].unique().tolist() if "ITM_NM" in df.columns else [],
        "region_names": df["CLS_NM"].unique().tolist() if "CLS_NM" in df.columns else [],
        "value_stats": stats,
        "preview": preview
    }
    return TextContent(type="text", text=json.dumps(summary, ensure_ascii=False))

@mcp.tool(
    name="search_reb_stat_tables",
    description="""Searches the cached REB statistical table list by keyword (e.g., STATBL_NM, DTACYCLE_NM).\nUse this tool to find the STATBL_ID you want, then use it in the item/data analysis tools.\n\nExample workflow:\n1. search_reb_stat_tables({"filter": {"STATBL_NM": "ê°€ê²©ì§€ìˆ˜"}})\n2. analyze_reb_stat_items({"cache_path": ..., "STATBL_ID": ...})\n3. analyze_reb_stat_data({"cache_path": ..., "STATBL_ID": ..., "filter": {...}})\n\nReturns only summary info (STATBL_ID, STATBL_NM, DTACYCLE_NM) for up to 10 results.""",
    tags={"realestate", "statistics", "REB", "stat_table", "search"}
)
def search_reb_stat_tables(params: dict) -> TextContent:
    import os
    import pandas as pd
    import json
    cache_path = params.get("cache_path", "/tmp/reb_stats_cache/stat_list_full.json")
    filter = params.get("filter", {})
    if not os.path.exists(cache_path):
        from mcp_kr_realestate.apis.reb_api import cache_stat_list_full
        cache_path = cache_stat_list_full({})
    with open(cache_path, "r", encoding="utf-8") as f:
        stats = json.load(f)
    df = pd.DataFrame(stats)
    for k, v in filter.items():
        if k in df.columns:
            df = df[df[k].astype(str).str.contains(str(v), na=False)]
    preview = df[["STATBL_ID", "STATBL_NM", "DTACYCLE_NM"]].head(10).to_dict(orient="records")
    summary = {
        "total_count": len(df),
        "preview": preview
    }
    return TextContent(type="text", text=json.dumps(summary, ensure_ascii=False))

# ... ê¸°ì¡´ analyze_reb_stat_items ...
analyze_reb_stat_items.__doc__ = """Step 2: Analyze the items of a specific REB statistical table.\n\nWorkflow example:\n1. search_reb_stat_tables({\"filter\": {\"STATBL_NM\": \"ê°€ê²©ì§€ìˆ˜\"}})\n2. analyze_reb_stat_items({\"cache_path\": ..., \"STATBL_ID\": ...})\n3. analyze_reb_stat_data({\"cache_path\": ..., \"STATBL_ID\": ..., \"filter\": {...}})\n\nInput the cache_path and STATBL_ID you found in step 1.\nYou can filter by any column using the 'filter' parameter.\nResults are sorted by the most recent order.\nResult should be visualized using a chart or table.\nBy default, the preview shows the latest 5 items."""

# ... ê¸°ì¡´ analyze_reb_stat_data ...
analyze_reb_stat_data.__doc__ = """Step 3: Analyze the data of a specific REB statistical table.\n\nWorkflow example:\n1. search_reb_stat_tables({\"filter\": {\"STATBL_NM\": \"ê°€ê²©ì§€ìˆ˜\"}})\n2. analyze_reb_stat_items({\"cache_path\": ..., \"STATBL_ID\": ...})\n3. analyze_reb_stat_data({\"cache_path\": ..., \"STATBL_ID\": ..., \"filter\": {...}})\n\nInput the cache_path and STATBL_ID you found in step 1.\nYou can filter by any column using the 'filter' parameter.\nResults are sorted by the most recent time.\nResult should be visualized using a chart or table.\nBy default, the preview shows the latest 5 records."""

@mcp.tool(
    name="get_latest_available_period",
    description="""Returns the latest available period (WRTTIME_IDTFR_ID) for the given STATBL_ID.\nIf the cache is missing or empty, the tool will call the API directly.\nIf no data is available, a clear message is returned.""",
    tags={"realestate", "statistics", "REB", "meta"}
)
def get_latest_available_period(params: dict) -> TextContent:
    import os, json, pandas as pd
    from mcp_kr_realestate.apis.reb_api import get_reb_stat_data_all
    statbl_id = params.get("STATBL_ID")
    cache_path = _get_data_cache_path(statbl_id)
    latest = None
    # 1. ìºì‹œ ìš°ì„ 
    if os.path.exists(cache_path):
        with open(cache_path, "r", encoding="utf-8") as f:
            items = json.load(f)
        df = pd.DataFrame(items)
        if not df.empty and "WRTTIME_IDTFR_ID" in df.columns:
            latest = df["WRTTIME_IDTFR_ID"].max()
            return TextContent(type="text", text=json.dumps({"latest_period": latest}))
    # 2. API ì§ì ‘ í˜¸ì¶œ
    all_data = get_reb_stat_data_all({"STATBL_ID": statbl_id})
    df = pd.DataFrame(all_data)
    if not df.empty and "WRTTIME_IDTFR_ID" in df.columns:
        latest = df["WRTTIME_IDTFR_ID"].max()
        return TextContent(type="text", text=json.dumps({"latest_period": latest}))
    return TextContent(type="text", text=json.dumps({"error": "No data for this STATBL_ID. ìµœì‹  ì‚¬ìš© ê°€ëŠ¥ ê¸°ê°„ì´ ì—†ìŠµë‹ˆë‹¤."}))

@mcp.tool(
    name="get_latest_comprehensive_analysis",
    description="""Performs a comprehensive analysis for the given STATBL_ID using the latest available data.\nAutomatically detects the latest period, analyzes trends, and summarizes key statistics.\nIf no data is found, the tool will inform you of the latest available period.""",
    tags={"realestate", "statistics", "REB", "analysis", "auto"}
)
def get_latest_comprehensive_analysis(params: dict) -> TextContent:
    import json, pandas as pd
    from mcp_kr_realestate.apis.reb_api import get_reb_stat_data_all
    statbl_id = params.get("STATBL_ID")
    data_cache_path, df = _ensure_data_cache(statbl_id)
    if df.empty or "WRTTIME_IDTFR_ID" not in df.columns:
        # API ì§ì ‘ í˜¸ì¶œë¡œ fallback
        all_data = get_reb_stat_data_all({"STATBL_ID": statbl_id})
        df = pd.DataFrame(all_data)
    if df.empty or "WRTTIME_IDTFR_ID" not in df.columns:
        return TextContent(type="text", text=json.dumps({"error": "No data for this STATBL_ID. ìµœì‹  ì‚¬ìš© ê°€ëŠ¥ ê¸°ê°„ì´ ì—†ìŠµë‹ˆë‹¤."}))
    latest = df["WRTTIME_IDTFR_ID"].max()
    latest_df = df[df["WRTTIME_IDTFR_ID"] == latest]
    trend_df = df.sort_values(by="WRTTIME_IDTFR_ID", ascending=False).head(6)
    stats = {}
    if "DTA_VAL" in latest_df.columns:
        stats = latest_df["DTA_VAL"].describe().to_dict()
    summary = {
        "STATBL_ID": statbl_id,
        "latest_period": latest,
        "latest_stats": stats,
        "trend": trend_df[["WRTTIME_IDTFR_ID", "DTA_VAL"]].to_dict(orient="records") if "DTA_VAL" in trend_df.columns else [],
        "preview": latest_df.head(5).to_dict(orient="records")
    }
    return TextContent(type="text", text=json.dumps(summary, ensure_ascii=False))

@mcp.tool(
    name="analyze_reb_stat_data",
    description="""Analyzes REB statistical data for a given STATBL_ID.\nIf filter['time'] is not provided or set to 'latest', the tool will automatically detect and use the latest available period.\nIf no data is found, the tool will inform you of the latest available period.""",
    tags={"realestate", "statistics", "REB", "data"}
)
def analyze_reb_stat_data(params: dict) -> TextContent:
    import json, pandas as pd
    from mcp_kr_realestate.apis.reb_api import get_reb_stat_data_all
    statbl_id = params.get("STATBL_ID")
    filter = params.get("filter", {})
    cache_path = params.get("cache_path")
    if not cache_path and statbl_id:
        cache_path = _get_data_cache_path(statbl_id)
    if statbl_id:
        cache_path, df = _ensure_data_cache(statbl_id)
    else:
        with open(cache_path, "r", encoding="utf-8") as f:
            all_items = json.load(f)
        df = pd.DataFrame(all_items)
    if df.empty or "WRTTIME_IDTFR_ID" not in df.columns:
        # API ì§ì ‘ í˜¸ì¶œë¡œ fallback
        all_data = get_reb_stat_data_all({"STATBL_ID": statbl_id})
        df = pd.DataFrame(all_data)
    if df.empty or "WRTTIME_IDTFR_ID" not in df.columns:
        return TextContent(type="text", text=json.dumps({"error": "No data for this STATBL_ID. ìµœì‹  ì‚¬ìš© ê°€ëŠ¥ ê¸°ê°„ì´ ì—†ìŠµë‹ˆë‹¤."}))
    df = df.sort_values(by="WRTTIME_IDTFR_ID", ascending=False)
    # ë‚ ì§œ í˜•ì‹ ìë™ ë³€í™˜ ë° 'latest' ì§€ì›
    time_val = filter.get("WRTTIME_IDTFR_ID") or filter.get("time")
    if not time_val or str(time_val).lower() == "latest":
        latest = df["WRTTIME_IDTFR_ID"].max()
        df = df[df["WRTTIME_IDTFR_ID"] == latest]
    else:
        norm_time = _normalize_time_str(time_val)
        if norm_time:
            df = df[df["WRTTIME_IDTFR_ID"].astype(str).str.startswith(norm_time)]
    for k, v in filter.items():
        if k in ["WRTTIME_IDTFR_ID", "time"]:
            continue
        if k in df.columns:
            df = df[df[k] == v]
    preview = df.head(5).to_dict(orient="records")
    stats = {}
    if "DTA_VAL" in df.columns:
        stats = df["DTA_VAL"].describe().to_dict()
    if len(df) == 0:
        latest = None
        if "WRTTIME_IDTFR_ID" in df.columns:
            latest = df["WRTTIME_IDTFR_ID"].max()
        msg = {"error": "No data for the given filter.", "latest_available_period": latest}
        return TextContent(type="text", text=json.dumps(msg, ensure_ascii=False))
    summary = {
        "total_count": len(df),
        "time_range": [df["WRTTIME_IDTFR_ID"].min(), df["WRTTIME_IDTFR_ID"].max()] if "WRTTIME_IDTFR_ID" in df.columns else [],
        "item_names": df["ITM_NM"].unique().tolist() if "ITM_NM" in df.columns else [],
        "region_names": df["CLS_NM"].unique().tolist() if "CLS_NM" in df.columns else [],
        "value_stats": stats,
        "preview": preview
    }
    return TextContent(type="text", text=json.dumps(summary, ensure_ascii=False))

def _normalize_time_str(time_str):
    # ì§€ì›: '2025-05', '202505', '2025ë…„ 5ì›”', '2025.05', '2025/05' ë“±
    import re
    if not time_str:
        return None
    if isinstance(time_str, int):
        return str(time_str)
    s = str(time_str)
    s = s.replace("ë…„", "-").replace("ì›”", "").replace(".", "-").replace("/", "-").replace(" ", "-")
    s = re.sub(r"-+", "-", s)
    m = re.match(r"(\d{4})-(\d{1,2})$", s)
    if m:
        return f"{m.group(1)}{int(m.group(2)):02d}"
    m = re.match(r"(\d{4})(\d{2})$", s)
    if m:
        return f"{m.group(1)}{m.group(2)}"
    return s

def _find_latest_available_period(statbl_id, months_back=18):
    import pandas as pd
    from mcp_kr_realestate.apis.reb_api import get_reb_stat_data_all
    from datetime import datetime
    today = datetime.today()
    # ìµœê·¼ months_backê°œì›”(í¬í•¨) ì—­ìˆœ
    months = pd.date_range(end=today, periods=months_back, freq='MS').strftime('%Y%m')[::-1]
    for yyyymm in months:
        data = get_reb_stat_data_all({"STATBL_ID": statbl_id, "WRTTIME_IDTFR_ID": yyyymm})
        df = pd.DataFrame(data)
        if not df.empty:
            return yyyymm
    return None

@mcp.tool(
    name="get_latest_available_period",
    description="""Returns the latest available period (WRTTIME_IDTFR_ID) for the given STATBL_ID.\nIf the cache is missing or empty, the tool will call the API directly.\nIf no data is available, a clear message is returned.""",
    tags={"realestate", "statistics", "REB", "meta"}
)
def get_latest_available_period(params: dict) -> TextContent:
    import os, json, pandas as pd
    from mcp_kr_realestate.apis.reb_api import get_reb_stat_data_all
    statbl_id = params.get("STATBL_ID")
    cache_path = _get_data_cache_path(statbl_id)
    latest = None
    # 1. ìºì‹œ ìš°ì„ 
    if os.path.exists(cache_path):
        with open(cache_path, "r", encoding="utf-8") as f:
            items = json.load(f)
        df = pd.DataFrame(items)
        if not df.empty and "WRTTIME_IDTFR_ID" in df.columns:
            latest = df["WRTTIME_IDTFR_ID"].max()
            return TextContent(type="text", text=json.dumps({"latest_period": latest}))
    # 2. API ì§ì ‘ í˜¸ì¶œ (ì „ì²´)
    all_data = get_reb_stat_data_all({"STATBL_ID": statbl_id})
    df = pd.DataFrame(all_data)
    if not df.empty and "WRTTIME_IDTFR_ID" in df.columns:
        latest = df["WRTTIME_IDTFR_ID"].max()
        return TextContent(type="text", text=json.dumps({"latest_period": latest}))
    # 3. ìµœì‹  ì›” ìë™ íƒì§€ (ìµœê·¼ 18ê°œì›” ì—­ìˆœ)
    latest = _find_latest_available_period(statbl_id, months_back=18)
    if latest:
        return TextContent(type="text", text=json.dumps({"latest_period": latest}))
    return TextContent(type="text", text=json.dumps({"error": "No data for this STATBL_ID. ìµœê·¼ 18ê°œì›” ë‚´ ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}))

@mcp.tool(
    name="get_latest_comprehensive_analysis",
    description="""Performs a comprehensive analysis for the given STATBL_ID using the latest available data.\nAutomatically detects the latest period, analyzes trends, and summarizes key statistics.\nIf no data is found, the tool will inform you of the latest available period.""",
    tags={"realestate", "statistics", "REB", "analysis", "auto"}
)
def get_latest_comprehensive_analysis(params: dict) -> TextContent:
    import json, pandas as pd
    from mcp_kr_realestate.apis.reb_api import get_reb_stat_data_all
    statbl_id = params.get("STATBL_ID")
    data_cache_path, df = _ensure_data_cache(statbl_id)
    latest = None
    if df.empty or "WRTTIME_IDTFR_ID" not in df.columns:
        # API ì§ì ‘ í˜¸ì¶œë¡œ fallback
        all_data = get_reb_stat_data_all({"STATBL_ID": statbl_id})
        df = pd.DataFrame(all_data)
    if df.empty or "WRTTIME_IDTFR_ID" not in df.columns:
        # ìµœì‹  ì›” ìë™ íƒì§€
        latest = _find_latest_available_period(statbl_id, months_back=18)
        if latest:
            # í•´ë‹¹ ì›” ë°ì´í„°ë¡œ ì¬ë¶„ì„
            all_data = get_reb_stat_data_all({"STATBL_ID": statbl_id, "WRTTIME_IDTFR_ID": latest})
            df = pd.DataFrame(all_data)
        if df.empty or "WRTTIME_IDTFR_ID" not in df.columns:
            return TextContent(type="text", text=json.dumps({"error": "No data for this STATBL_ID. ìµœê·¼ 18ê°œì›” ë‚´ ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}))
    latest = df["WRTTIME_IDTFR_ID"].max()
    latest_df = df[df["WRTTIME_IDTFR_ID"] == latest]
    trend_df = df.sort_values(by="WRTTIME_IDTFR_ID", ascending=False).head(6)
    stats = {}
    if "DTA_VAL" in latest_df.columns:
        stats = latest_df["DTA_VAL"].describe().to_dict()
    summary = {
        "STATBL_ID": statbl_id,
        "latest_period": latest,
        "latest_stats": stats,
        "trend": trend_df[["WRTTIME_IDTFR_ID", "DTA_VAL"]].to_dict(orient="records") if "DTA_VAL" in trend_df.columns else [],
        "preview": latest_df.head(5).to_dict(orient="records")
    }
    return TextContent(type="text", text=json.dumps(summary, ensure_ascii=False))

@mcp.tool(
    name="analyze_reb_stat_data",
    description="""Analyzes REB statistical data for a given STATBL_ID.\nIf filter['time'] is not provided or set to 'latest', the tool will automatically detect and use the latest available period.\nIf no data is found, the tool will inform you of the latest available period.""",
    tags={"realestate", "statistics", "REB", "data"}
)
def analyze_reb_stat_data(params: dict) -> TextContent:
    import json, pandas as pd
    from mcp_kr_realestate.apis.reb_api import get_reb_stat_data_all
    statbl_id = params.get("STATBL_ID")
    filter = params.get("filter", {})
    cache_path = params.get("cache_path")
    if not cache_path and statbl_id:
        cache_path = _get_data_cache_path(statbl_id)
    if statbl_id:
        cache_path, df = _ensure_data_cache(statbl_id)
    else:
        with open(cache_path, "r", encoding="utf-8") as f:
            all_items = json.load(f)
        df = pd.DataFrame(all_items)
    if df.empty or "WRTTIME_IDTFR_ID" not in df.columns:
        # API ì§ì ‘ í˜¸ì¶œë¡œ fallback
        all_data = get_reb_stat_data_all({"STATBL_ID": statbl_id})
        df = pd.DataFrame(all_data)
    if df.empty or "WRTTIME_IDTFR_ID" not in df.columns:
        # ìµœì‹  ì›” ìë™ íƒì§€
        latest = _find_latest_available_period(statbl_id, months_back=18)
        if latest:
            # í•´ë‹¹ ì›” ë°ì´í„°ë¡œ ì¬ë¶„ì„
            all_data = get_reb_stat_data_all({"STATBL_ID": statbl_id, "WRTTIME_IDTFR_ID": latest})
            df = pd.DataFrame(all_data)
        if df.empty or "WRTTIME_IDTFR_ID" not in df.columns:
            return TextContent(type="text", text=json.dumps({"error": "No data for this STATBL_ID. ìµœê·¼ 18ê°œì›” ë‚´ ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}))
    df = df.sort_values(by="WRTTIME_IDTFR_ID", ascending=False)
    # ë‚ ì§œ í˜•ì‹ ìë™ ë³€í™˜ ë° 'latest' ì§€ì›
    time_val = filter.get("WRTTIME_IDTFR_ID") or filter.get("time")
    if not time_val or str(time_val).lower() == "latest":
        latest = df["WRTTIME_IDTFR_ID"].max()
        df = df[df["WRTTIME_IDTFR_ID"] == latest]
    else:
        norm_time = _normalize_time_str(time_val)
        if norm_time:
            df = df[df["WRTTIME_IDTFR_ID"].astype(str).str.startswith(norm_time)]
    for k, v in filter.items():
        if k in ["WRTTIME_IDTFR_ID", "time"]:
            continue
        if k in df.columns:
            df = df[df[k] == v]
    preview = df.head(5).to_dict(orient="records")
    stats = {}
    if "DTA_VAL" in df.columns:
        stats = df["DTA_VAL"].describe().to_dict()
    if len(df) == 0:
        latest = None
        if "WRTTIME_IDTFR_ID" in df.columns:
            latest = df["WRTTIME_IDTFR_ID"].max()
        msg = {"error": "No data for the given filter.", "latest_available_period": latest}
        return TextContent(type="text", text=json.dumps(msg, ensure_ascii=False))
    summary = {
        "total_count": len(df),
        "time_range": [df["WRTTIME_IDTFR_ID"].min(), df["WRTTIME_IDTFR_ID"].max()] if "WRTTIME_IDTFR_ID" in df.columns else [],
        "item_names": df["ITM_NM"].unique().tolist() if "ITM_NM" in df.columns else [],
        "region_names": df["CLS_NM"].unique().tolist() if "CLS_NM" in df.columns else [],
        "value_stats": stats,
        "preview": preview
    }
    return TextContent(type="text", text=json.dumps(summary, ensure_ascii=False))